{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM7VuR330CfB"
      },
      "source": [
        "# Load Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tBuDmkj0Dep"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel,RobertaTokenizer, RobertaModel,DistilBertModel, DistilBertTokenizer, DistilBertForSequenceClassification, RobertaTokenizer, AutoTokenizer, AutoModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKD2UH57mrBt"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "insample_df = pd.read_csv('insample_df.csv')\n",
        "outsample_df = pd.read_csv('outsample_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wh-HsesTmrBt",
        "outputId": "49ac651f-6cb8-407d-b957-e9332048677e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>companyname</th>\n",
              "      <th>Date From</th>\n",
              "      <th>Date To</th>\n",
              "      <th>date</th>\n",
              "      <th>Weekly Compound Return</th>\n",
              "      <th>Past Return Direction</th>\n",
              "      <th>Future Return Direction</th>\n",
              "      <th>market_cap</th>\n",
              "      <th>headline</th>\n",
              "      <th>eventtype</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Omnicom Group Inc.</td>\n",
              "      <td>2005-01-08</td>\n",
              "      <td>2005-01-14</td>\n",
              "      <td>2005-01-14</td>\n",
              "      <td>0.018572</td>\n",
              "      <td>Up</td>\n",
              "      <td>Down</td>\n",
              "      <td>16145234.55</td>\n",
              "      <td>iVillage Inc. (NASDAQ:IVIL) acquired Healtholo...</td>\n",
              "      <td>M&amp;A Transaction Announcements</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Omnicom Group Inc.</td>\n",
              "      <td>2005-01-08</td>\n",
              "      <td>2005-01-14</td>\n",
              "      <td>2005-01-14</td>\n",
              "      <td>0.018572</td>\n",
              "      <td>Up</td>\n",
              "      <td>Down</td>\n",
              "      <td>16145234.55</td>\n",
              "      <td>[No_Headline]</td>\n",
              "      <td>[No_Event]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          companyname   Date From     Date To        date  \\\n",
              "0  Omnicom Group Inc.  2005-01-08  2005-01-14  2005-01-14   \n",
              "1  Omnicom Group Inc.  2005-01-08  2005-01-14  2005-01-14   \n",
              "\n",
              "   Weekly Compound Return Past Return Direction Future Return Direction  \\\n",
              "0                0.018572                    Up                    Down   \n",
              "1                0.018572                    Up                    Down   \n",
              "\n",
              "    market_cap                                           headline  \\\n",
              "0  16145234.55  iVillage Inc. (NASDAQ:IVIL) acquired Healtholo...   \n",
              "1  16145234.55                                      [No_Headline]   \n",
              "\n",
              "                       eventtype  \n",
              "0  M&A Transaction Announcements  \n",
              "1                     [No_Event]  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "insample_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGUuTYYj0JHP"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYi2uDkl0KKP"
      },
      "outputs": [],
      "source": [
        "#Step 2: Pre-process Text Data\n",
        "\n",
        "# Encode the 'Up' and 'Down' labels\n",
        "label_mapping = {'Up': 1, 'Down': 0}\n",
        "\n",
        "# Filter out 'No_Change' labels and encode target variable\n",
        "insample_df = insample_df[insample_df['Future Return Direction'].isin(['Up', 'Down'])]\n",
        "outsample_df = outsample_df[outsample_df['Future Return Direction'].isin(['Up', 'Down'])]\n",
        "\n",
        "insample_df['Future Return Direction'] = insample_df['Future Return Direction'].map(label_mapping)\n",
        "outsample_df['Future Return Direction'] = outsample_df['Future Return Direction'].map(label_mapping)\n",
        "\n",
        "# Drop rows with NaN values in the target variable\n",
        "insample_df = insample_df.dropna(subset=['Future Return Direction'])\n",
        "outsample_df = outsample_df.dropna(subset=['Future Return Direction'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpnCHm_5znUY"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwxBOhXkzpFf"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to generate embeddings in batches\n",
        "def get_average_embedding_batch(text_list, batch_size=16):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(text_list), batch_size):\n",
        "        batch = text_list[i:i + batch_size]\n",
        "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "        embeddings.extend(batch_embeddings)\n",
        "    return embeddings\n",
        "\n",
        "# Apply the function to generate embeddings for headlines\n",
        "insample_df['embedding'] = get_average_embedding_batch(insample_df['headline'].tolist())\n",
        "outsample_df['embedding'] = get_average_embedding_batch(outsample_df['headline'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrY_itZa0atC",
        "outputId": "c812f019-5eda-49f4-b511-94d314837cce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataframes with embeddings have been saved to 'embedding-BERT-AllCompany-NEW.pkl'\n"
          ]
        }
      ],
      "source": [
        "# Save the dataframes to a pickle file\n",
        "with open('embedding-BERT-AllCompany-NEW.pkl', 'wb') as f:\n",
        "    pd.to_pickle((insample_df, outsample_df), f)\n",
        "\n",
        "print(\"Dataframes with embeddings have been saved to 'embedding-BERT-AllCompany-NEW.pkl'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQJvimHqzpj0"
      },
      "source": [
        "# RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArrgnigRzt5o",
        "outputId": "735670f9-02f2-4cf4-a396-300fd2930af7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load the pre-trained RoBERTa model and tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaModel.from_pretrained('roberta-base')\n",
        "\n",
        "# Function to generate embeddings in batches\n",
        "def get_average_embedding_batch(text_list, batch_size=16):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(text_list), batch_size):\n",
        "        batch = text_list[i:i + batch_size]\n",
        "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "        embeddings.extend(batch_embeddings)\n",
        "    return embeddings\n",
        "\n",
        "# Apply the function to generate embeddings for headlines\n",
        "insample_df['embedding'] = get_average_embedding_batch(insample_df['headline'].tolist())\n",
        "outsample_df['embedding'] = get_average_embedding_batch(outsample_df['headline'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxEvmK-j0szh",
        "outputId": "e0360a3e-d75d-45ae-d1bf-aad004222672"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataframes with embeddings have been saved to 'embedding-RoBERTa-AllCompany-NEW.pkl'\n"
          ]
        }
      ],
      "source": [
        "# Save the dataframes to a pickle file\n",
        "with open('embedding-RoBERTa-AllCompany-NEW.pkl', 'wb') as f:\n",
        "    pd.to_pickle((insample_df, outsample_df), f)\n",
        "\n",
        "print(\"Dataframes with embeddings have been saved to 'embedding-RoBERTa-AllCompany-NEW.pkl'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPqr35QPzxcf"
      },
      "source": [
        "# DistilBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlEfST19zy6F",
        "outputId": "06253160-0aa8-42e6-f36e-fdbf6636aa86",
        "colab": {
          "referenced_widgets": [
            "3899bf4c114f4de1a719d8803fbbc97a"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3899bf4c114f4de1a719d8803fbbc97a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Bagus Pranata\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Bagus Pranata\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        }
      ],
      "source": [
        "# Load the pre-trained DistilBERT model and tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Function to generate embeddings in batches\n",
        "def get_average_embedding_batch(text_list, batch_size=16):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(text_list), batch_size):\n",
        "        batch = text_list[i:i + batch_size]\n",
        "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "        embeddings.extend(batch_embeddings)\n",
        "    return embeddings\n",
        "\n",
        "# Apply the function to generate embeddings for headlines\n",
        "insample_df['embedding'] = get_average_embedding_batch(insample_df['headline'].tolist())\n",
        "outsample_df['embedding'] = get_average_embedding_batch(outsample_df['headline'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggad6XOK095N",
        "outputId": "9144a7d8-bec9-4de9-f24b-5d8a1a862bdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataframes with embeddings have been saved to 'embedding-DistilBERT-AllCompany-NEW.pkl'\n"
          ]
        }
      ],
      "source": [
        "# Save the dataframes to a pickle file\n",
        "with open('embedding-DistilBERT-AllCompany-NEW.pkl', 'wb') as f:\n",
        "    pd.to_pickle((insample_df, outsample_df), f)\n",
        "\n",
        "print(\"Dataframes with embeddings have been saved to 'embedding-DistilBERT-AllCompany-NEW.pkl'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9FhlN-azzMb"
      },
      "source": [
        "# DistilRoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQTTVQnKz0qZ",
        "outputId": "e803acba-f703-44f0-8228-fc4a671a37bc",
        "colab": {
          "referenced_widgets": [
            "37029d9ec9d6443a841904dd50ed0f07"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37029d9ec9d6443a841904dd50ed0f07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Bagus Pranata\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Bagus Pranata\\.cache\\huggingface\\hub\\models--distilroberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        }
      ],
      "source": [
        "# Load the pre-trained DistilRoBERTa model and tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n",
        "model = RobertaModel.from_pretrained('distilroberta-base')\n",
        "\n",
        "# Function to generate embeddings in batches\n",
        "def get_average_embedding_batch(text_list, batch_size=16):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(text_list), batch_size):\n",
        "        batch = text_list[i:i + batch_size]\n",
        "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "        embeddings.extend(batch_embeddings)\n",
        "    return embeddings\n",
        "\n",
        "# Apply the function to generate embeddings for headlines\n",
        "insample_df['embedding'] = get_average_embedding_batch(insample_df['headline'].tolist())\n",
        "outsample_df['embedding'] = get_average_embedding_batch(outsample_df['headline'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mhExMUP1Lxv",
        "outputId": "c2238051-f08d-45c9-b4f7-56319c628997"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataframes with embeddings have been saved to 'embedding-DistilRoBERTa-AllCompany-NEW.pkl'\n"
          ]
        }
      ],
      "source": [
        "# Save the dataframes to a pickle file\n",
        "with open('embedding-DistilRoBERTa-AllCompany-NEW.pkl', 'wb') as f:\n",
        "    pd.to_pickle((insample_df, outsample_df), f)\n",
        "\n",
        "print(\"Dataframes with embeddings have been saved to 'embedding-DistilRoBERTa-AllCompany-NEW.pkl'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGflsXdez03b"
      },
      "source": [
        "# FinBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFPBavV-z1ph",
        "outputId": "cbd30e73-d64e-4146-b300-4836d03878b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Bagus Pranata\\anaconda3\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ],
      "source": [
        "# Load the pre-trained FinBERT model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('yiyanghkust/finbert-tone', use_fast=False)\n",
        "model = AutoModel.from_pretrained('yiyanghkust/finbert-tone')\n",
        "\n",
        "# Function to generate embeddings in batches\n",
        "def get_average_embedding_batch(text_list, batch_size=16):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(text_list), batch_size):\n",
        "        batch = text_list[i:i + batch_size]\n",
        "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, return_dict=True)\n",
        "        # Extract the output embeddings (CLS token representation from the last hidden state)\n",
        "        last_hidden_states = outputs.last_hidden_state\n",
        "        # Average pooling of the last hidden state across tokens\n",
        "        avg_pooled_embeddings = torch.mean(last_hidden_states, dim=1).cpu().numpy()\n",
        "        embeddings.extend(avg_pooled_embeddings)\n",
        "    return embeddings\n",
        "\n",
        "# Assuming insample_df and outsample_df are defined earlier with 'headline' column\n",
        "\n",
        "# Apply the function to generate embeddings for headlines\n",
        "insample_df['embedding'] = get_average_embedding_batch(insample_df['headline'].tolist())\n",
        "outsample_df['embedding'] = get_average_embedding_batch(outsample_df['headline'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_f881SRF1dcW",
        "outputId": "b680ae82-c54c-41ea-fe8a-6123ea4b3ae9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataframes with embeddings have been saved to 'embedding-FinBERT-AllCompany-NEW.pkl'\n"
          ]
        }
      ],
      "source": [
        "# Save the dataframes to a pickle file\n",
        "with open('embedding-FinBERT-AllCompany-NEW.pkl', 'wb') as f:\n",
        "    pd.to_pickle((insample_df, outsample_df), f)\n",
        "\n",
        "print(\"Dataframes with embeddings have been saved to 'embedding-FinBERT-AllCompany-NEW.pkl'\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}