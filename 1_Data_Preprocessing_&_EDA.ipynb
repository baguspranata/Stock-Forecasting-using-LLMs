{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdMQnW_OG_j8"
      },
      "source": [
        "# Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLkAzKbuz-dl"
      },
      "outputs": [],
      "source": [
        "#Import important libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas_market_calendars as mcal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZV8sFYJIPoG"
      },
      "outputs": [],
      "source": [
        "# Load the CRSP and IQ Key Development dataset\n",
        "CRSP = pd.read_csv('CRSP - Communication Services Dataset.csv')\n",
        "IQKD = pd.read_csv('IQ Key Development - Communication Services Dataset.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8PP6En2IPoG"
      },
      "source": [
        "# CRSP Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQt8UFgDIPoH"
      },
      "source": [
        "## Organizing the Company Name and Tickers of CRSP Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLrc9iyqIPoH"
      },
      "outputs": [],
      "source": [
        "# Columns to be dropped as it's unusable\n",
        "columns_to_remove_atCRSP = ['PERMNO']\n",
        "CRSP = CRSP.drop(columns=columns_to_remove_atCRSP)\n",
        "\n",
        "# Display the modified DataFrame\n",
        "CRSP.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9beu8PkIPoH"
      },
      "outputs": [],
      "source": [
        "# Group by 'PERMCO' and select the first 'TICKER' and 'COMNAM' for each group\n",
        "first_values = CRSP.groupby('PERMCO').first()\n",
        "\n",
        "# Map these first values back to the original DataFrame\n",
        "CRSP['TICKER'] = CRSP['PERMCO'].map(first_values['TICKER'])\n",
        "CRSP['COMNAM'] = CRSP['PERMCO'].map(first_values['COMNAM'])\n",
        "\n",
        "# Now CRSP DataFrame should have standardized 'TICKER' and 'COMNAM' for each 'PERMCO'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8ztkZniIPoI"
      },
      "outputs": [],
      "source": [
        "#Summarize of the dataset\n",
        "def summarize_data(df, name):\n",
        "    print(f\"Summary for {name}:\")\n",
        "    print(\"Total columns:\", df.shape[1])\n",
        "    print(\"Total rows:\", df.shape[0])\n",
        "    print(\"Missing values per column:\")\n",
        "    print(df.isnull().sum())\n",
        "    print(\"\\n\")\n",
        "summarize_data(CRSP, \"CRSP Dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT8VCh3PIPoI"
      },
      "outputs": [],
      "source": [
        "# Display all rows\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "# Group by 'PERMCO' and aggregate the 'date' column\n",
        "result = CRSP.groupby('PERMCO')['date'].agg(['min', 'max', 'count'])\n",
        "\n",
        "# Convert 'min' column to datetime to compare with January 2005\n",
        "result['min'] = pd.to_datetime(result['min'])\n",
        "\n",
        "# Filter out rows where 'min' date is earlier than January 2005\n",
        "filtered_result = result[result['min'] <= '2005-01-03']\n",
        "\n",
        "# Show the filtered result\n",
        "print(filtered_result)\n",
        "\n",
        "# Add the total count of unique PERMCO\n",
        "total_unique_permco = filtered_result.shape[0]\n",
        "print(f\"Total count of unique PERMCO: {total_unique_permco}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQhJ6l-iIPoJ"
      },
      "outputs": [],
      "source": [
        "# Ensure 'date' is in the correct datetime format\n",
        "CRSP['date'] = pd.to_datetime(CRSP['date'])\n",
        "\n",
        "# Convert 'PRC' and 'SHROUT' to numeric to avoid any type issues, dropping NaN values\n",
        "CRSP.dropna(subset=['PRC', 'SHROUT'], inplace=True)\n",
        "CRSP['PRC'] = pd.to_numeric(CRSP['PRC'], errors='coerce')\n",
        "CRSP['SHROUT'] = pd.to_numeric(CRSP['SHROUT'], errors='coerce')\n",
        "\n",
        "# Group by 'COMNAM' and calculate maximum market cap observed on the fly, and min/max dates\n",
        "market_cap_result = CRSP.groupby('COMNAM').agg({\n",
        "    'PRC': lambda x: (x * CRSP.loc[x.index, 'SHROUT']).max(),\n",
        "    'date': ['min', 'max']\n",
        "}).reset_index()\n",
        "\n",
        "# Rename columns to make the DataFrame easier to read\n",
        "market_cap_result.columns = ['COMNAM', 'max_market_cap', 'min_date', 'max_date']\n",
        "\n",
        "# Sort the results by market cap in descending order\n",
        "market_cap_result = market_cap_result.sort_values(by='max_market_cap', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display the result\n",
        "market_cap_result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Owc03VlgIPoJ"
      },
      "outputs": [],
      "source": [
        "# Count the unique PERMCO values\n",
        "unique_permco_count = CRSP['PERMCO'].nunique()\n",
        "\n",
        "# Display the count of unique PERMCOs\n",
        "print(f\"Total count of unique PERMCO: {unique_permco_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECq6OAfHIPoJ"
      },
      "outputs": [],
      "source": [
        "# Check for any negative values in the 'PRC' column\n",
        "negative_prc = CRSP[CRSP['PRC'] < 0]\n",
        "\n",
        "# Display the entries with negative 'PRC' values, if any\n",
        "if not negative_prc.empty:\n",
        "    print(\"Negative PRC values found:\")\n",
        "    print(negative_prc)\n",
        "else:\n",
        "    print(\"No negative PRC values found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIhy0n1ZIPoJ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "\n",
        "# Create a DataFrame showing each PERMCO with its associated COMNAM\n",
        "comnam_tickers = CRSP.groupby('PERMCO')['COMNAM'].apply(set).reset_index()\n",
        "\n",
        "# Name the columns as requested\n",
        "comnam_tickers.columns = ['PERMCO', 'Associated Company Name']\n",
        "\n",
        "# Display the DataFrame in a scrollable output window in Jupyter Notebook\n",
        "display(comnam_tickers.style.set_table_attributes('style=\"display:block; height:200px; overflow:auto;\"'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huP_FbtdIPoK"
      },
      "outputs": [],
      "source": [
        "CRSP.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "337QAJi3IPoK"
      },
      "outputs": [],
      "source": [
        "#Summarize of the dataset\n",
        "def summarize_data(df, name):\n",
        "    print(f\"Summary for {name}:\")\n",
        "    print(\"Total columns:\", df.shape[1])\n",
        "    print(\"Total rows:\", df.shape[0])\n",
        "    print(\"Missing values per column:\")\n",
        "    print(df.isnull().sum())\n",
        "    print(\"\\n\")\n",
        "summarize_data(CRSP, \"CRSP Dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCD4uzYiIPoK"
      },
      "outputs": [],
      "source": [
        "# Filter out the rows where 'COMNAM' is 'MILLICOM INTL CELLULAR SA' or 'CHARTER COMMUNICATIONS INC'\n",
        "CRSP = CRSP[~CRSP['COMNAM'].isin(['MILLICOM INTL CELLULAR SA', 'CHARTER COMMUNICATIONS INC'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUztvOt_IPoK"
      },
      "outputs": [],
      "source": [
        "CRSP['date'] = pd.to_datetime(CRSP['date'])\n",
        "\n",
        "# Determine the overall date range in the dataset\n",
        "min_date = CRSP['date'].min()\n",
        "max_date = CRSP['date'].max()\n",
        "full_date_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
        "\n",
        "# Function to calculate missing dates for a given ticker\n",
        "def calculate_missing_dates(ticker_data, full_date_range):\n",
        "    ticker_dates = pd.DatetimeIndex(ticker_data['date'])\n",
        "    missing_dates = full_date_range.difference(ticker_dates)\n",
        "    return missing_dates\n",
        "\n",
        "# Apply the function to each ticker and store the results in a DataFrame\n",
        "results = {}\n",
        "for COMNAM in CRSP['COMNAM'].unique():\n",
        "    ticker_data = CRSP[CRSP['COMNAM'] == COMNAM]\n",
        "    missing_dates = calculate_missing_dates(ticker_data, full_date_range)\n",
        "    results[COMNAM] = len(missing_dates)\n",
        "\n",
        "# Convert results dictionary to DataFrame for better readability and analysis\n",
        "results_df = pd.DataFrame(list(results.items()), columns=['COMNAM', 'Missing Dates Count'])\n",
        "\n",
        "# Display the results\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cevwVIOnIPoK"
      },
      "source": [
        "## Drop the missing value, gap, or uncommon value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R67Y3r-IIPoK"
      },
      "outputs": [],
      "source": [
        "# Drop rows with any missing values\n",
        "CRSP.dropna(inplace=True)\n",
        "\n",
        "# Drop rows where the 'PRC' column contains negative values\n",
        "CRSP = CRSP[CRSP['PRC'] > 0]\n",
        "\n",
        "# Ensure 'RET' contains only numerical values and drop rows with non-numerical values\n",
        "CRSP = CRSP[pd.to_numeric(CRSP['RET'], errors='coerce').notna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLIOuzxpIPoK"
      },
      "outputs": [],
      "source": [
        "# Print unique PERMCO values and their count\n",
        "PERMCO_unique = CRSP['PERMCO'].unique()\n",
        "print(\"Unique PERMCO values in CRSP\")\n",
        "PERMCO_unique\n",
        "print(\"Count of unique COMNAM values:\", len(PERMCO_unique))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MspxP4FfIPoK"
      },
      "outputs": [],
      "source": [
        "#Summarize of the dataset\n",
        "def summarize_data(df, name):\n",
        "    print(f\"Summary for {name}:\")\n",
        "    print(\"Total columns:\", df.shape[1])\n",
        "    print(\"Total rows:\", df.shape[0])\n",
        "    print(\"Missing values per column:\")\n",
        "    print(df.isnull().sum())\n",
        "    print(\"\\n\")\n",
        "summarize_data(CRSP, \"CRSP Dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7Q4B5zrIPoL"
      },
      "outputs": [],
      "source": [
        "# Group by 'PERMCO' and aggregate the 'date' column to find min and max dates\n",
        "final_CRSP_processing_check = CRSP.groupby('PERMCO').agg(\n",
        "    min_date=('date', 'min'),\n",
        "    max_date=('date', 'max')\n",
        ").reset_index()\n",
        "\n",
        "comnam_mapping = CRSP[['PERMCO', 'COMNAM']].drop_duplicates().set_index('PERMCO')\n",
        "final_CRSP_processing_check['COMNAM'] = final_CRSP_processing_check['PERMCO'].map(comnam_mapping['COMNAM'])\n",
        "\n",
        "# Show the result\n",
        "final_CRSP_processing_check\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePNDm-aLIPoL"
      },
      "source": [
        "# IQ Key Development Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sei-eFodIPoL"
      },
      "source": [
        "## Organize Company Name and Ticker of IQKD Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjAn1CITIPoL"
      },
      "outputs": [],
      "source": [
        "#Summarize of the dataset\n",
        "def summarize_data(df, name):\n",
        "    print(f\"Summary for {name}:\")\n",
        "    print(\"Total columns:\", df.shape[1])\n",
        "    print(\"Total rows:\", df.shape[0])\n",
        "    print(\"Missing values per column:\")\n",
        "    print(df.isnull().sum())\n",
        "    print(\"\\n\")\n",
        "summarize_data(IQKD, \"IQ Key Development Dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Whd8d4q6IPoL"
      },
      "outputs": [],
      "source": [
        "# Columns to be removed\n",
        "columns_to_remove = ['companyid', 'sptodate']\n",
        "\n",
        "# Drop the specified columns from the IQKD DataFrame\n",
        "IQKD= IQKD.drop(columns=columns_to_remove)\n",
        "\n",
        "# Display the modified DataFrame\n",
        "IQKD.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MltewkdjIPoL"
      },
      "outputs": [],
      "source": [
        "# Display the unique gvkey and companyname\n",
        "unique_gvkey_companyname = IQKD[['gvkey', 'companyname']].drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# Print the unique gvkey and companyname\n",
        "print(unique_gvkey_companyname)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOy3cmp2IPoL"
      },
      "outputs": [],
      "source": [
        "# Remove rows with specified company names\n",
        "companies_to_remove = ['M&A Rumors and Discussions', 'Millicom International Cellular S.A.', 'Charter Communications, Inc.']\n",
        "IQKD = IQKD[~IQKD['companyname'].isin(companies_to_remove)].reset_index(drop=True)\n",
        "\n",
        "# Fill NaN gvkey based on similar companyname\n",
        "# Create a mapping of companyname to gvkey for non-NaN gvkey\n",
        "company_to_gvkey_map = IQKD.dropna(subset=['gvkey']).set_index('companyname')['gvkey'].to_dict()\n",
        "\n",
        "# Fill NaN gvkey based on the company_to_gvkey_map\n",
        "IQKD['gvkey'] = IQKD.apply(\n",
        "    lambda row: company_to_gvkey_map.get(row['companyname'], row['gvkey']),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Remove duplicate rows based on both gvkey and companyname\n",
        "IQKD = IQKD.drop_duplicates().reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urZwFYm5IPoL"
      },
      "outputs": [],
      "source": [
        "# Group by 'PERMCO' and select the first 'gvkey' and 'companyname' for each group\n",
        "first_values = IQKD.groupby('gvkey').first()\n",
        "\n",
        "# Map these first values back to the original DataFrame\n",
        "IQKD['companyname'] = IQKD['gvkey'].map(first_values['companyname'])\n",
        "\n",
        "# Now CRSP DataFrame should have standardized 'gvkey' and 'companyname' for each 'gvkey'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_wzasAcIPoL"
      },
      "outputs": [],
      "source": [
        "#Summarize of the dataset\n",
        "def summarize_data(df, name):\n",
        "    print(f\"Summary for {name}:\")\n",
        "    print(\"Total columns:\", df.shape[1])\n",
        "    print(\"Total rows:\", df.shape[0])\n",
        "    print(\"Missing values per column:\")\n",
        "    print(df.isnull().sum())\n",
        "    print(\"\\n\")\n",
        "summarize_data(IQKD, \"IQ Key Development Dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPfmTZz1IPoL"
      },
      "outputs": [],
      "source": [
        "IQKD['announcedate'] = pd.to_datetime(IQKD['announcedate'], errors='coerce')\n",
        "\n",
        "# Group by the new 'companyname' and analyze the 'announcedate' column\n",
        "grouped_GVKeycompany = IQKD.groupby('companyname')['announcedate'].agg(['min', 'max', 'count']).reset_index()\n",
        "\n",
        "# Display the results\n",
        "print(\"Unified date range and count of entries for each company before merging:\")\n",
        "grouped_GVKeycompany\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzUTjWk4IPoL"
      },
      "outputs": [],
      "source": [
        "#Summarize of the dataset\n",
        "def summarize_data(df, name):\n",
        "    print(f\"Summary for {name}:\")\n",
        "    print(\"Total columns:\", df.shape[1])\n",
        "    print(\"Total rows:\", df.shape[0])\n",
        "    print(\"Missing values per column:\")\n",
        "    print(df.isnull().sum())\n",
        "    print(\"\\n\")\n",
        "summarize_data(IQKD, \"IQ Key Development Dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHqsk7g8IPoL"
      },
      "source": [
        "## Removing Redundant Headline on the Same Day for every company"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYzz3fCWIPoM"
      },
      "outputs": [],
      "source": [
        "# Group by 'gvkey' and 'announcedate' and collect headlines\n",
        "headline_groups = IQKD.groupby(['gvkey', 'announcedate'])['headline'].agg(list)\n",
        "\n",
        "# Check for redundant headlines within each group\n",
        "redundant_headlines = headline_groups.apply(lambda headlines: len(headlines) != len(set(headlines)))\n",
        "\n",
        "# Filter to show only the groups where there are redundant headlines\n",
        "redundant_headlines = redundant_headlines[redundant_headlines]\n",
        "\n",
        "# Join this with the original DataFrame to get the rows with redundant headlines\n",
        "redundant_data = IQKD[IQKD.set_index(['gvkey', 'announcedate']).index.isin(redundant_headlines.index)]\n",
        "\n",
        "# Display the data\n",
        "print(\"Data with redundant headlines:\")\n",
        "redundant_data.head(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHsuQPEMIPoM"
      },
      "outputs": [],
      "source": [
        "# Filter for the specific ticker and date\n",
        "specific_data = IQKD[(IQKD['gvkey'] == '126136') & (pd.to_datetime(IQKD['announcedate']) == pd.Timestamp('2009-05-20'))]\n",
        "\n",
        "# Display the relevant rows\n",
        "print(\"Entries for Charter Communications:\")\n",
        "print(specific_data)\n",
        "\n",
        "# To explicitly check for redundant headlines\n",
        "if specific_data['headline'].duplicated().any():\n",
        "    print(\"Redundant headlines found.\")\n",
        "else:\n",
        "    print(\"No redundant headlines.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MF1VJ9KmIPoM"
      },
      "outputs": [],
      "source": [
        "# Count rows per Company Name before removing duplicates\n",
        "count_before = IQKD.groupby('companyname').size()\n",
        "\n",
        "# Remove rows where 'Company Name', 'announcedate', and 'headline' are identical, keeping the first occurrence\n",
        "IQKD_cleaned = IQKD.drop_duplicates(subset=['companyname', 'announcedate', 'headline'], keep='first')\n",
        "\n",
        "# Count rows per Company Name after removing duplicates\n",
        "count_after = IQKD_cleaned.groupby('companyname').size()\n",
        "\n",
        "# Combine counts into a DataFrame for side-by-side comparison\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Before': count_before,\n",
        "    'After': count_after\n",
        "})\n",
        "\n",
        "# Fill NaN values with 0 if there are any, in case some Company Name disappear completely after cleaning\n",
        "comparison_df.fillna(0, inplace=True)\n",
        "\n",
        "# Print the comparison DataFrame\n",
        "print(\"Comparison of row counts per Company Name before and after removing Headlines duplicates:\")\n",
        "print(comparison_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3quhGK8JIPoM"
      },
      "outputs": [],
      "source": [
        "# Remove rows where 'gvkey', 'announcedate', and 'headline' are identical except keep the first\n",
        "IQKD = IQKD.drop_duplicates(subset=['gvkey', 'announcedate', 'headline'], keep='first')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5ZJRyVLIPoM"
      },
      "outputs": [],
      "source": [
        "#Summarize of the dataset\n",
        "def summarize_data(df, name):\n",
        "    print(f\"Summary for {name}:\")\n",
        "    print(\"Total columns:\", df.shape[1])\n",
        "    print(\"Total rows:\", df.shape[0])\n",
        "    print(\"Missing values per column:\")\n",
        "    print(df.isnull().sum())\n",
        "    print(\"\\n\")\n",
        "summarize_data(IQKD, \"IQ Key Development Dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rahWZ9WoIPoM"
      },
      "source": [
        "## Dropping the rows that having missing announcedate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkwSaDjeIPoQ"
      },
      "outputs": [],
      "source": [
        "rows_with_missing_values = IQKD[IQKD.isnull().any(axis=1)]\n",
        "\n",
        "# Show the first 10 rows that will be dropped\n",
        "rows_with_missing_values.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agpHrtubIPoQ"
      },
      "outputs": [],
      "source": [
        "# Drop rows where 'announcedate' is NaT\n",
        "IQKD = IQKD[pd.notnull(IQKD['announcedate'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fapwOjh_IPoQ"
      },
      "outputs": [],
      "source": [
        "#Summarize of the dataset\n",
        "def summarize_data(df, name):\n",
        "    print(f\"Summary for {name}:\")\n",
        "    print(\"Total columns:\", df.shape[1])\n",
        "    print(\"Total rows:\", df.shape[0])\n",
        "    print(\"Missing values per column:\")\n",
        "    print(df.isnull().sum())\n",
        "    print(\"\\n\")\n",
        "summarize_data(IQKD, \"IQ Key Development Dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_E_eUvJIPoR"
      },
      "source": [
        "## Put [No_Headline] placeholders on the date where no headlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F45OI4piIPoR"
      },
      "outputs": [],
      "source": [
        "IQKD['announcedate'] = pd.to_datetime(IQKD['announcedate'])\n",
        "\n",
        "# Sort the DataFrame by 'gvkey' and 'announcedate' to ensure correct date difference calculation\n",
        "IQKD.sort_values(['gvkey', 'announcedate'], inplace=True)\n",
        "\n",
        "# Calculate the difference between consecutive dates for each gvkey\n",
        "IQKD['date_diff'] = IQKD.groupby('gvkey')['announcedate'].diff().dt.days - 1\n",
        "\n",
        "# Filter to show only the rows where there is a gap (date_diff > 0)\n",
        "date_gaps = IQKD[IQKD['date_diff'] > 0]\n",
        "\n",
        "# Optional: summarize the maximum gap for each gvkey\n",
        "max_gaps = date_gaps.groupby('gvkey')['date_diff'].max()\n",
        "\n",
        "# Print or view the summary of maximum gaps\n",
        "print(\"Maximum date gaps for each company:\")\n",
        "print(max_gaps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGM1UUx9IPoR"
      },
      "outputs": [],
      "source": [
        "# Sort the DataFrame by 'gvkey' and 'announcedate' to ensure correct date difference calculation\n",
        "IQKD.sort_values(['gvkey', 'announcedate'], inplace=True)\n",
        "\n",
        "# Calculate the difference between consecutive dates for each gvkey\n",
        "IQKD['date_diff'] = IQKD.groupby('gvkey')['announcedate'].diff().dt.days\n",
        "\n",
        "# Filter to show only the rows where there is a gap (date_diff > 0)\n",
        "date_gaps = IQKD[IQKD['date_diff'] > 0]\n",
        "\n",
        "# Summarize the average gap for each gvkey\n",
        "average_gaps = date_gaps.groupby('gvkey')['date_diff'].mean().reset_index()\n",
        "\n",
        "# Retrieve the first company name for each gvkey\n",
        "first_company_name = IQKD.drop_duplicates(subset='gvkey')[['gvkey', 'companyname']]\n",
        "\n",
        "# Merge the average gaps with the first company name\n",
        "average_gaps = average_gaps.merge(first_company_name, on='gvkey')\n",
        "\n",
        "# Print or view the summary of average gaps with gvkey and companyname\n",
        "print(\"Average date gaps for each gvkey and companyname:\")\n",
        "print(average_gaps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdR1NxFKIPoR"
      },
      "outputs": [],
      "source": [
        "# Filter to find gaps specifically for Verizon\n",
        "Verizon_gaps = IQKD[(IQKD['gvkey'] == 2136.0) & (IQKD['date_diff'] > 0)]\n",
        "\n",
        "# Print the rows with gaps for Google Inc (Alphabet)\n",
        "print(\"Rows with gaps for Verizon:\")\n",
        "Verizon_gaps.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtAGph0DIPoR"
      },
      "outputs": [],
      "source": [
        "# Ensure 'announcedate' is in datetime format and sort the DataFrame\n",
        "IQKD['announcedate'] = pd.to_datetime(IQKD['announcedate'])\n",
        "IQKD.sort_values(['gvkey', 'announcedate'], inplace=True)\n",
        "\n",
        "# Function to fill missing dates for each gvkey\n",
        "def fill_missing_dates(group):\n",
        "    all_dates = pd.date_range(start=group['announcedate'].min(), end=group['announcedate'].max(), freq='D')\n",
        "    all_dates_df = pd.DataFrame(all_dates, columns=['announcedate'])\n",
        "    merged = all_dates_df.merge(group, on='announcedate', how='left')\n",
        "    if merged.isna().sum().sum() > 0:  # Check if there are any missing values to fill\n",
        "        merged['gvkey'] = group['gvkey'].iloc[0]\n",
        "        merged['companyname'] = group['companyname'].iloc[0]\n",
        "        merged['headline'].fillna('[No_Headline]', inplace=True)\n",
        "        merged['eventtype'].fillna('[No_Event]', inplace=True)\n",
        "    return merged\n",
        "\n",
        "# Apply the function to each gvkey group\n",
        "IQKD = IQKD.groupby('gvkey').apply(fill_missing_dates).reset_index(drop=True)\n",
        "\n",
        "# Print or examine updated DataFrame\n",
        "IQKD.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzPcBAj8IPoR"
      },
      "outputs": [],
      "source": [
        "# Filter the DataFrame to show rows where 'headline' is '[No_Headline]'\n",
        "no_headline_rows = IQKD[IQKD['headline'] == '[No_Headline]']\n",
        "\n",
        "# Display these rows.\n",
        "no_headline_rows.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRK2WzRWIPoR"
      },
      "outputs": [],
      "source": [
        "IQKD.sort_values(['gvkey', 'announcedate'], inplace=True)\n",
        "\n",
        "# Filter the DataFrame to show rows where 'headline' is '[No_Headline]'\n",
        "no_headline_rows = IQKD[IQKD['headline'] == '[No_Headline]']\n",
        "\n",
        "# Group by 'companyname' and 'announcedate' and count occurrences\n",
        "no_headline_counts = no_headline_rows.groupby(['companyname', 'announcedate']).size()\n",
        "\n",
        "# Filter to keep only groups with more than one occurrence\n",
        "multiple_no_headline = no_headline_counts[no_headline_counts > 1].reset_index()\n",
        "\n",
        "if multiple_no_headline.empty:\n",
        "    print(\"There are no more than 1 [No_Headline] on each company for the same announcedate.\")\n",
        "else:\n",
        "    # Merge to get detailed rows for these groups\n",
        "    detailed_no_headline_rows = no_headline_rows.merge(multiple_no_headline, on=['companyname', 'announcedate'])\n",
        "\n",
        "    # Display these rows\n",
        "    print(\"Rows with more than 1 no headline on the same announcedate for each company:\")\n",
        "    print(detailed_no_headline_rows.head(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMjEGsT5IPoS"
      },
      "outputs": [],
      "source": [
        "Verizon_data = IQKD[IQKD['companyname'] == 'Verizon Communications Inc.']\n",
        "\n",
        "print(\"Head rows for Verizon:\")\n",
        "Verizon_data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzF2mp7RIPoS"
      },
      "source": [
        "## Add 'Situation' from WRDS Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ETQSzXoIPoS"
      },
      "outputs": [],
      "source": [
        "import wrds\n",
        "db = wrds.Connection(wrds_username='baguspranata')\n",
        "db.create_pgpass_file()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qoapn7IIPoS"
      },
      "outputs": [],
      "source": [
        "db.close()\n",
        "db = wrds.Connection(wrds_username='baguspranata')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIcndB47IPoS"
      },
      "outputs": [],
      "source": [
        "table_description = db.describe_table(library='ciq_keydev', table='ciqkeydev')\n",
        "table_description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWDQoDZ8IPoS"
      },
      "outputs": [],
      "source": [
        "unique_keydevids = IQKD['keydevid'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d10QQuL6IPoS"
      },
      "outputs": [],
      "source": [
        "# Convert the list of keydevids to a format suitable for SQL queries\n",
        "keydevid_list = ','.join([f\"'{str(k)}'\" for k in unique_keydevids])\n",
        "\n",
        "# Construct the SQL query\n",
        "sql_query = f\"\"\"\n",
        "SELECT keydevid, headline, situation, announceddate\n",
        "FROM ciq_keydev.ciqkeydev\n",
        "WHERE keydevid IN ({keydevid_list})\n",
        "\"\"\"\n",
        "\n",
        "# Fetch the data\n",
        "matched_data = db.raw_sql(sql_query)\n",
        "matched_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnYkSjDCIPoS"
      },
      "outputs": [],
      "source": [
        "# Convert keydevid in IQKD to integer (remove non-numeric entries first if necessary)\n",
        "IQKD['keydevid'] = pd.to_numeric(IQKD['keydevid'], errors='coerce').dropna().astype(int)\n",
        "\n",
        "# Convert keydevid in matched_data to integer\n",
        "matched_data['keydevid'] = pd.to_numeric(matched_data['keydevid'], errors='coerce').dropna().astype(int)\n",
        "\n",
        "# Merge the situation data from matched_data into IQKD based on keydevid\n",
        "IQKD = pd.merge(IQKD, matched_data[['keydevid', 'situation']], on='keydevid', how='left')\n",
        "\n",
        "# Check and fill missing situations after merge\n",
        "IQKD['situation'] = IQKD['situation'].fillna('[No Situation]')\n",
        "\n",
        "# Print the resulting DataFrame to check the results\n",
        "IQKD.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_c2sYaAIPoS"
      },
      "outputs": [],
      "source": [
        "# Filter the DataFrame to include only rows where 'situation' is not missing\n",
        "filtered_IQKD = IQKD[IQKD['situation'].notna()]\n",
        "\n",
        "# Print the filtered DataFrame\n",
        "filtered_IQKD.head(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDo80tgeIPoT"
      },
      "outputs": [],
      "source": [
        "IQKD.sort_values(['gvkey', 'announcedate'], inplace=True)\n",
        "\n",
        "# Filter the DataFrame to show rows where 'headline' is '[No_Headline]'\n",
        "no_headline_rows = IQKD[IQKD['headline'] == '[No_Headline]']\n",
        "\n",
        "# Group by 'companyname' and 'announcedate' and count occurrences\n",
        "no_headline_counts = no_headline_rows.groupby(['companyname', 'announcedate']).size()\n",
        "\n",
        "# Filter to keep only groups with more than one occurrence\n",
        "multiple_no_headline = no_headline_counts[no_headline_counts > 1].reset_index()\n",
        "\n",
        "if multiple_no_headline.empty:\n",
        "    print(\"There are no more than 1 [No_Headline] on each company for the same announcedate.\")\n",
        "else:\n",
        "    # Merge to get detailed rows for these groups\n",
        "    detailed_no_headline_rows = no_headline_rows.merge(multiple_no_headline, on=['companyname', 'announcedate'])\n",
        "\n",
        "    # Display these rows\n",
        "    print(\"Rows with more than 1 no headline on the same announcedate for each company:\")\n",
        "    print(detailed_no_headline_rows.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVA3n-isIPoT"
      },
      "source": [
        "## Distribution of headlines throughout the time on every ticker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80a7LVuwIPoT"
      },
      "outputs": [],
      "source": [
        "IQKD['announcedate'] = pd.to_datetime(IQKD['announcedate'])\n",
        "\n",
        "# Group by 'ticker' and 'announcedate' and count the number of headlines\n",
        "headline_counts = IQKD.groupby(['companyname', 'announcedate']).size().reset_index(name='headline_count')\n",
        "\n",
        "# Get unique tickers\n",
        "unique_tickers = headline_counts['companyname'].unique()\n",
        "\n",
        "# Calculate the number of rows needed for subplots (2 plots per row)\n",
        "num_rows = (len(unique_tickers) + 1) // 2\n",
        "\n",
        "# Create a figure and array of axes with 2 columns\n",
        "fig, axes = plt.subplots(nrows=num_rows, ncols=2, figsize=(14, num_rows * 4))\n",
        "axes = axes.flatten()  # Flatten to 1D array for easier iteration\n",
        "\n",
        "# Plot each ticker's data in its respective subplot\n",
        "for idx, companyname in enumerate(unique_tickers):\n",
        "    ax = axes[idx]\n",
        "    # Filter data for the current companyname\n",
        "    ticker_data = headline_counts[headline_counts['companyname'] == companyname]\n",
        "\n",
        "    # Create the line plot on the specified axes\n",
        "    sns.lineplot(ax=ax, data=ticker_data, x='announcedate', y='headline_count', marker='o', label=companyname)\n",
        "\n",
        "    # Adding plot details\n",
        "    ax.set_title(f'Number of Headlines Over Time for {companyname}')\n",
        "    ax.set_xlabel('Date')\n",
        "    ax.set_ylabel('Number of Headlines')\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "\n",
        "# If the number of tickers is odd, hide the last subplot (if unused)\n",
        "if len(unique_tickers) % 2 != 0:\n",
        "    axes[-1].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFC4_JW2LyRe"
      },
      "source": [
        "# CRSP Weekly Return Computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCPCtbLzIPoT"
      },
      "outputs": [],
      "source": [
        "CRSP.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPWQZ6PWIPoT"
      },
      "source": [
        "## Checking the frequency of outliers of PRC and RET on each TICKERs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crbThLh4IPoT"
      },
      "outputs": [],
      "source": [
        "# Convert 'RET' and 'PRC' to numeric, coercing errors to NaN\n",
        "CRSP['RET'] = pd.to_numeric(CRSP['RET'], errors='coerce')\n",
        "CRSP['PRC'] = pd.to_numeric(CRSP['PRC'], errors='coerce')\n",
        "\n",
        "# Determine the first company name for each PERMCO\n",
        "first_comnam = CRSP.sort_values(by='date').groupby('PERMCO')['COMNAM'].first().reset_index()\n",
        "\n",
        "# Map this first company name back to the main DataFrame using PERMCO\n",
        "CRSP = CRSP.merge(first_comnam, on='PERMCO', suffixes=('', '_first'))\n",
        "\n",
        "# Handle NaN values if necessary\n",
        "# CRSP.dropna(subset=['RET', 'PRC'], inplace=True)  # Option to drop NaNs\n",
        "\n",
        "# Calculate IQR and bounds within a function to ensure proper grouping\n",
        "def calculate_outliers_and_proportions(group):\n",
        "    Q1 = group[['RET', 'PRC']].quantile(0.25)\n",
        "    Q3 = group[['RET', 'PRC']].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Count outliers\n",
        "    ret_outliers = ((group['RET'] < lower_bound['RET']) | (group['RET'] > upper_bound['RET']))\n",
        "    prc_outliers = ((group['PRC'] < lower_bound['PRC']) | (group['PRC'] > upper_bound['PRC']))\n",
        "\n",
        "    # Calculate proportions\n",
        "    ret_outliers_count = ret_outliers.sum()\n",
        "    prc_outliers_count = prc_outliers.sum()\n",
        "    ret_proportion = ret_outliers_count / len(group)\n",
        "    prc_proportion = prc_outliers_count / len(group)\n",
        "\n",
        "    return pd.Series({'RET_Outliers': ret_outliers_count, 'PRC_Outliers': prc_outliers_count,\n",
        "                      'RET_Outlier_Proportion': ret_proportion, 'PRC_Outlier_Proportion': prc_proportion})\n",
        "\n",
        "# Apply function to each group based on the first COMNAM\n",
        "outlier_counts = CRSP.groupby('COMNAM').apply(calculate_outliers_and_proportions)\n",
        "\n",
        "# Print the result\n",
        "outlier_counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40wxgrUCIPoT"
      },
      "outputs": [],
      "source": [
        "CRSP.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEvgvJp3IPoT"
      },
      "source": [
        "## Weekly Return Computation Without PRC & RET Outliers Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBXh0vzvIPoT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming CRSP is your DataFrame\n",
        "\n",
        "# Step 1: Convert date to datetime and other data to numeric\n",
        "CRSP['date'] = pd.to_datetime(CRSP['date'])\n",
        "CRSP['RET'] = pd.to_numeric(CRSP['RET'], errors='coerce')\n",
        "CRSP['PRC'] = pd.to_numeric(CRSP['PRC'], errors='coerce')\n",
        "CRSP['SHROUT'] = pd.to_numeric(CRSP['SHROUT'], errors='coerce')\n",
        "\n",
        "# Step 2: Filter the data\n",
        "NonMissing_CRSP = CRSP[(CRSP['PRC'] > 0) & (CRSP['RET'].notna())]\n",
        "\n",
        "# Step 3: Calculate weekly returns and market cap\n",
        "# Set index to date for resampling\n",
        "NonMissing_CRSP.set_index('date', inplace=True)\n",
        "\n",
        "# Resample to get weekly data\n",
        "weekly_CRSP = NonMissing_CRSP.groupby(['PERMCO', 'COMNAM']).resample('W-FRI').agg({\n",
        "    'PRC': 'last',  # Get the last price of the week\n",
        "    'RET': lambda x: np.exp(np.sum(np.log(1 + x))) - 1,  # Compound weekly return\n",
        "    'SHROUT': 'last'  # Share outstanding at the end of the week\n",
        "}).reset_index()\n",
        "\n",
        "# Calculate market cap\n",
        "weekly_CRSP['market_cap'] = weekly_CRSP['PRC'] * weekly_CRSP['SHROUT']\n",
        "\n",
        "# Rename the column to indicate it is the weekly compound return\n",
        "weekly_CRSP.rename(columns={'RET': 'Weekly Compound Return'}, inplace=True)\n",
        "\n",
        "# Calculate Return Direction based on price movement compared to the previous week\n",
        "weekly_CRSP['Past Return Direction'] = np.select(\n",
        "    [\n",
        "        weekly_CRSP['PRC'].diff() > 0,\n",
        "        weekly_CRSP['PRC'].diff() < 0\n",
        "    ],\n",
        "    [\n",
        "        'Up',  # Price went up\n",
        "        'Down'  # Price went down\n",
        "    ],\n",
        "    default='No Change'  # No change in price\n",
        ")\n",
        "\n",
        "# Calculate Future Direction by comparing with the next week's price\n",
        "weekly_CRSP['Future Return Direction'] = np.select(\n",
        "    [\n",
        "        weekly_CRSP['PRC'].shift(-1) > weekly_CRSP['PRC'],\n",
        "        weekly_CRSP['PRC'].shift(-1) < weekly_CRSP['PRC']\n",
        "    ],\n",
        "    [\n",
        "        'Up',  # Price will go up next week\n",
        "        'Down'  # Price will go down next week\n",
        "    ],\n",
        "    default='No Change'  # No change in price next week\n",
        ")\n",
        "\n",
        "# Display the head of the modified DataFrame\n",
        "print(weekly_CRSP.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvpNlMccIPoU"
      },
      "outputs": [],
      "source": [
        "# Calculate the 'Date From' by subtracting 6 days from 'date' (since 'date' is the end of the week)\n",
        "weekly_CRSP['Date From'] = weekly_CRSP['date'] - pd.Timedelta(days=6)\n",
        "\n",
        "# 'Date To' is just the 'date' column, which represents the week ending\n",
        "weekly_CRSP['Date To'] = weekly_CRSP['date']\n",
        "\n",
        "# Rearrange columns for better readability, if necessary\n",
        "weekly_CRSP = weekly_CRSP[['COMNAM','PERMCO','date', 'Date From', 'Date To','market_cap', 'Weekly Compound Return', 'Past Return Direction', 'Future Return Direction']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebgCDlDdIPoU"
      },
      "outputs": [],
      "source": [
        "# Provided PERMCO-GVKEY pairs\n",
        "permco_gvkey_mapping = {\n",
        "    45483: 160329,\n",
        "    21645: 9899,\n",
        "    11204: 24708,\n",
        "    16779: 122915,\n",
        "    17322: 126136,\n",
        "    42769: 147204,\n",
        "    43613: 3226,\n",
        "    10303: 16721,\n",
        "    1908: 5284,\n",
        "    13136: 30312,\n",
        "    4922: 11499,\n",
        "    34913: 28378,\n",
        "    12746: 30024,\n",
        "    43145: 147579,\n",
        "    44625: 149177,\n",
        "    1367: 4066,\n",
        "    21866: 13714,\n",
        "    15708: 65460,\n",
        "    11358: 9466,\n",
        "    40213: 9664,\n",
        "    13758: 60800,\n",
        "    15436: 64630,\n",
        "    20782: 4988,\n",
        "    5230: 10411,\n",
        "    20997: 6136,\n",
        "    21280: 7866,\n",
        "    20587: 3980,\n",
        "    17205: 125240,\n",
        "    21826: 14369,\n",
        "    20288: 2136,\n",
        "    16665: 122172\n",
        "}\n",
        "\n",
        "# Mapping the gvkey to the weekly_CRSP DataFrame\n",
        "weekly_CRSP['gvkey'] = weekly_CRSP['PERMCO'].map(permco_gvkey_mapping)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQBGa05jIPoU"
      },
      "outputs": [],
      "source": [
        "# Display the DataFrame\n",
        "weekly_CRSP.tail(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMa4pjfAIPoU"
      },
      "outputs": [],
      "source": [
        "# Count unique TICKER values in the weekly_CRSP DataFrame\n",
        "unique_PERMCO_count = weekly_CRSP['PERMCO'].nunique()\n",
        "print(f\"Number of unique PERMCOs: {unique_PERMCO_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gy-sVqFCIPoU"
      },
      "outputs": [],
      "source": [
        "# Find the latest date\n",
        "latest_date = weekly_CRSP['Date To'].max()\n",
        "\n",
        "# Display the latest date\n",
        "print(\"The latest date in the 'Date To' column is:\", latest_date)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd6FFePMIPoU"
      },
      "outputs": [],
      "source": [
        "# Make sure the 'Date To' column is in datetime format\n",
        "weekly_CRSP['Date To'] = pd.to_datetime(weekly_CRSP['Date To'])\n",
        "\n",
        "# Set Seaborn's aesthetic parameters to make the plots more visually appealing\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Generate a list of years to use as x-axis ticks\n",
        "years = pd.date_range(start='2005-01-01', end='2024-12-31', freq='YS').year\n",
        "\n",
        "# Iterate over each group defined by 'COMNAM'\n",
        "for ticker, group in weekly_CRSP.groupby('COMNAM'):\n",
        "    fig, ax = plt.subplots(figsize=(12, 2))  # Create a new figure for each ticker\n",
        "    group = group.sort_values('Date To')  # Ensure data is sorted by date for plotting\n",
        "\n",
        "    # Use Seaborn's lineplot for better aesthetics; since seaborn is based on matplotlib, it uses the same syntax for ax\n",
        "    sns.lineplot(data=group, x='Date To', y='Weekly Compound Return', marker='o', linestyle='-', label=f'{ticker} Returns', ax=ax)\n",
        "\n",
        "    # Set title and labels with enhanced formatting\n",
        "    ax.set_title(f'Weekly Compound Return for {ticker}', fontsize=12)\n",
        "    ax.set_xlabel('Date', fontsize=10)\n",
        "    ax.set_ylabel('Weekly Compound Return', fontsize=10)\n",
        "    ax.legend()\n",
        "\n",
        "    # Set x-ticks to display each year\n",
        "    ax.set_xticks(pd.to_datetime(years, format='%Y'))\n",
        "    ax.set_xticklabels(years, rotation=45)\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkeiYv6GIPoU"
      },
      "outputs": [],
      "source": [
        "# Setting Seaborn's aesthetic parameters for better visual effects\n",
        "sns.set(style=\"whitegrid\", palette=\"pastel\")\n",
        "\n",
        "# Assuming 'weekly_CRSP' is your DataFrame\n",
        "all_tickers = weekly_CRSP['COMNAM'].unique()  # Get all unique tickers\n",
        "\n",
        "# Setup the figure size to fit all tickers horizontally with improved vertical dimension\n",
        "plt.figure(figsize=(0.5 * len(all_tickers), 5))  # Increase the height for better visibility\n",
        "\n",
        "# Create a seaborn boxplot across a single axis without subplot division\n",
        "sns.boxplot(x='COMNAM', y='Weekly Compound Return', data=weekly_CRSP, width=0.6)\n",
        "\n",
        "# Set the plot title and labels with improved formatting\n",
        "plt.title('Weekly Returns for All COMNAM', fontsize=15)\n",
        "plt.xlabel('COMNAM', fontsize=14)\n",
        "plt.ylabel('Weekly Compound Return', fontsize=14)\n",
        "plt.xticks(rotation=90)  # Rotate ticker labels for better readability if necessary\n",
        "\n",
        "plt.grid(True)  # Ensure the grid is visible for better readability\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQHWEnlxIPoU"
      },
      "source": [
        "# Merging & Split Weekly Return CRSP with IQKD and stored it into desired dataframe structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UConh43IPoU"
      },
      "outputs": [],
      "source": [
        "#To avoiding the memory error issue, save the previous IQKD and weekly_CRSP that had been proccessed before.\n",
        "\n",
        "# Save the IQKD DataFrame to a CSV file\n",
        "IQKD.to_csv('cleaned_IQKD.csv', index=False)\n",
        "\n",
        "# Save the weekly_CRSP DataFrame to a CSV file\n",
        "weekly_CRSP.to_csv('cleaned_weekly_CRSP.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jxUB1MwIPoU"
      },
      "outputs": [],
      "source": [
        "# Reload the CRSP and IQ Key Development dataset\n",
        "weekly_CRSP = pd.read_csv('cleaned_weekly_CRSP.csv')\n",
        "IQKD = pd.read_csv('cleaned_IQKD.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqcNjIREIPoU"
      },
      "outputs": [],
      "source": [
        "# Convert 'announcedate' to datetime format\n",
        "IQKD['announcedate'] = pd.to_datetime(IQKD['announcedate'])\n",
        "\n",
        "# Filter out rows where headline is '[No_Headline]'\n",
        "filtered_IQKD = IQKD[IQKD['headline'] != '[No_Headline]']\n",
        "\n",
        "# Extract year and month from 'announcedate'\n",
        "filtered_IQKD['year'] = filtered_IQKD['announcedate'].dt.year\n",
        "filtered_IQKD['month'] = filtered_IQKD['announcedate'].dt.month_name()\n",
        "\n",
        "# Set the style of the plots\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Define the color\n",
        "viridis_blue = '#1f77b4'  # Hex code for a blue color in the Viridis palette\n",
        "\n",
        "# Plot 1: Total Counts of headlines based on year\n",
        "plt.figure(figsize=(10, 6))\n",
        "yearly_counts = filtered_IQKD['year'].value_counts().sort_index()\n",
        "sns.barplot(x=yearly_counts.index, y=yearly_counts.values, color=viridis_blue)\n",
        "plt.title('Total Counts of Headlines Based on Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Total Counts')\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle='--')\n",
        "plt.show()\n",
        "\n",
        "# Plot 2: Total Counts of headlines based on month\n",
        "plt.figure(figsize=(12, 6))\n",
        "monthly_counts = filtered_IQKD['month'].value_counts().reindex([\n",
        "    'January', 'February', 'March', 'April', 'May', 'June', 'July',\n",
        "    'August', 'September', 'October', 'November', 'December'\n",
        "])\n",
        "sns.barplot(x=monthly_counts.index, y=monthly_counts.values, color=viridis_blue)\n",
        "plt.title('Total Counts of Headlines Based on Month')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Total Counts')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_HIzzQ0IPoU"
      },
      "outputs": [],
      "source": [
        "# Ensure that the 'Date From', 'Date To' in weekly_CRSP and 'announcedate' in IQKD are datetime\n",
        "weekly_CRSP['Date From'] = pd.to_datetime(weekly_CRSP['Date From'], errors='coerce')\n",
        "weekly_CRSP['Date To'] = pd.to_datetime(weekly_CRSP['Date To'], errors='coerce')\n",
        "IQKD['announcedate'] = pd.to_datetime(IQKD['announcedate'], errors='coerce')\n",
        "\n",
        "# Function to split DataFrame into in-sample and out-sample based on a split date\n",
        "def split_dataframes(df, date_column, split_date):\n",
        "    in_sample = df[df[date_column] < split_date]\n",
        "    out_sample = df[df[date_column] >= split_date]\n",
        "    return in_sample, out_sample\n",
        "\n",
        "# Define the split date\n",
        "split_date = pd.to_datetime('2016-01-01')\n",
        "\n",
        "# Split weekly_CRSP DataFrame\n",
        "InSample_weekly_CRSP, OutSample_weekly_CRSP = split_dataframes(weekly_CRSP, 'Date From', split_date)\n",
        "\n",
        "# Split IQKD DataFrame\n",
        "InSample_IQKD, OutSample_IQKD = split_dataframes(IQKD, 'announcedate', split_date)\n",
        "\n",
        "# Display the resulting DataFrames\n",
        "print(\"InSample_weekly_CRSP:\")\n",
        "print(InSample_weekly_CRSP.head(1))\n",
        "\n",
        "print(\"\\nOutSample_weekly_CRSP:\")\n",
        "print(OutSample_weekly_CRSP.head(1))\n",
        "\n",
        "print(\"\\nInSample_IQKD:\")\n",
        "print(InSample_IQKD.head(1))\n",
        "\n",
        "print(\"\\nOutSample_IQKD:\")\n",
        "print(OutSample_IQKD.head(1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UNLQ53oIPoV"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)  # Be cautious with this on large DataFrames\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIhzQJS9IPoV"
      },
      "outputs": [],
      "source": [
        "weekly_CRSP.tail(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNK6gR5OIPoV"
      },
      "outputs": [],
      "source": [
        "IQKD.tail(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zYLH856IPoV"
      },
      "outputs": [],
      "source": [
        "# Merge InSample DataFrames\n",
        "insample_df = pd.merge(InSample_weekly_CRSP, InSample_IQKD, left_on='gvkey', right_on='gvkey')\n",
        "insample_df = insample_df[(insample_df['announcedate'] >= insample_df['Date From']) & (insample_df['announcedate'] <= insample_df['Date To'])]\n",
        "insample_df = insample_df[['companyname', 'Date From', 'Date To', 'date', 'Weekly Compound Return', 'Past Return Direction','Future Return Direction','market_cap', 'headline', 'eventtype']]\n",
        "\n",
        "# Print or return the resulting DataFrames\n",
        "print(\"Merged In-Sample DataFrame:\")\n",
        "insample_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLQNbU-SIPoV"
      },
      "outputs": [],
      "source": [
        "# Filter the DataFrame to show rows where 'headline' is '[No_Headline]'\n",
        "no_headline_rows = insample_df[insample_df['headline'] == '[No_Headline]']\n",
        "\n",
        "# Group by 'companyname' and 'announcedate' and count occurrences\n",
        "no_headline_counts = no_headline_rows.groupby(['companyname', 'date']).size()\n",
        "\n",
        "# Filter to keep only groups with more than one occurrence\n",
        "multiple_no_headline = no_headline_counts[no_headline_counts > 1].reset_index()\n",
        "\n",
        "# Merge to get detailed rows for these groups\n",
        "detailed_no_headline_rows = no_headline_rows.merge(multiple_no_headline, on=['companyname', 'date'])\n",
        "\n",
        "# Display these rows\n",
        "print(\"INSAMPLE Rows with more than 1 no headline on the same date for each company:\")\n",
        "detailed_no_headline_rows.head(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqGDx8BuIPoV"
      },
      "outputs": [],
      "source": [
        "# Filter the DataFrame to show rows where 'headline' is '[No_Headline]'\n",
        "no_headline_rows = insample_df[insample_df['headline'] == '[No_Headline]']\n",
        "\n",
        "# Group by 'companyname' and 'date' and count occurrences\n",
        "no_headline_counts = no_headline_rows.groupby(['companyname', 'date']).size()\n",
        "\n",
        "# Filter to keep only groups with more than one occurrence\n",
        "multiple_no_headline = no_headline_counts[no_headline_counts > 1].reset_index()\n",
        "\n",
        "# Merge to get detailed rows for these groups\n",
        "detailed_no_headline_rows = no_headline_rows.merge(multiple_no_headline, on=['companyname', 'date'])\n",
        "\n",
        "# Display these rows\n",
        "print(\"INSAMPLE Rows with more than 1 no headline on the same date for each company:\")\n",
        "print(detailed_no_headline_rows.head(5))\n",
        "\n",
        "# Remove duplicates keeping only the first occurrence\n",
        "insample_df_cleaned = insample_df.drop_duplicates(subset=['companyname', 'date', 'headline'], keep='first')\n",
        "\n",
        "# Save the cleaned DataFrame back into insample_df\n",
        "insample_df = insample_df_cleaned\n",
        "\n",
        "# Verify the cleaning process\n",
        "no_headline_cleaned = insample_df[insample_df['headline'] == '[No_Headline]']\n",
        "multiple_no_headline_cleaned = no_headline_cleaned.groupby(['companyname', 'date']).size()\n",
        "\n",
        "# Filter to keep only groups with more than one occurrence after cleaning\n",
        "multiple_no_headline_cleaned = multiple_no_headline_cleaned[multiple_no_headline_cleaned > 1].reset_index()\n",
        "\n",
        "# Check if there are any duplicates left\n",
        "if not multiple_no_headline_cleaned.empty:\n",
        "    print(\"There are still rows with more than 1 no headline on the same date for each company after cleaning:\")\n",
        "    print(multiple_no_headline_cleaned.head(5))\n",
        "else:\n",
        "    print(\"No more duplicate 'No_Headline' rows found for the same date and company after cleaning.\")\n",
        "\n",
        "# Display cleaned DataFrame\n",
        "print(\"Cleaned INSAMPLE DataFrame:\")\n",
        "print(insample_df.head(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JT7KSYPMIPoV"
      },
      "outputs": [],
      "source": [
        "insample_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6mEiSG-IPoV"
      },
      "outputs": [],
      "source": [
        "# Merge OutSample DataFrames\n",
        "outsample_df = pd.merge(OutSample_weekly_CRSP, OutSample_IQKD, left_on='gvkey', right_on='gvkey')\n",
        "outsample_df = outsample_df[(outsample_df['announcedate'] >= outsample_df['Date From']) & (outsample_df['announcedate'] <= outsample_df['Date To'])]\n",
        "outsample_df = outsample_df[['companyname', 'Date From', 'Date To', 'date', 'Weekly Compound Return', 'Past Return Direction','Future Return Direction','market_cap', 'headline', 'eventtype']]\n",
        "\n",
        "print(\"\\nMerged Out-Sample DataFrame:\")\n",
        "outsample_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-y9G5JEjIPoV"
      },
      "outputs": [],
      "source": [
        "# Filter the DataFrame to show rows where 'headline' is '[No_Headline]'\n",
        "no_headline_rows = outsample_df[outsample_df['headline'] == '[No_Headline]']\n",
        "\n",
        "# Group by 'companyname' and 'announcedate' and count occurrences\n",
        "no_headline_counts = no_headline_rows.groupby(['companyname', 'date']).size()\n",
        "\n",
        "# Filter to keep only groups with more than one occurrence\n",
        "multiple_no_headline = no_headline_counts[no_headline_counts > 1].reset_index()\n",
        "\n",
        "# Merge to get detailed rows for these groups\n",
        "detailed_no_headline_rows = no_headline_rows.merge(multiple_no_headline, on=['companyname', 'date'])\n",
        "\n",
        "# Display these rows\n",
        "print(\"OUTSAMPLE Rows with more than 1 no headline on the same date for each company:\")\n",
        "detailed_no_headline_rows.head(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXqYSwkMIPoV"
      },
      "outputs": [],
      "source": [
        "# Filter the DataFrame to show rows where 'headline' is '[No_Headline]'\n",
        "no_headline_rows = outsample_df[outsample_df['headline'] == '[No_Headline]']\n",
        "\n",
        "# Group by 'companyname' and 'date' and count occurrences\n",
        "no_headline_counts = no_headline_rows.groupby(['companyname', 'date']).size()\n",
        "\n",
        "# Filter to keep only groups with more than one occurrence\n",
        "multiple_no_headline = no_headline_counts[no_headline_counts > 1].reset_index()\n",
        "\n",
        "# Merge to get detailed rows for these groups\n",
        "detailed_no_headline_rows = no_headline_rows.merge(multiple_no_headline, on=['companyname', 'date'])\n",
        "\n",
        "# Display these rows\n",
        "print(\"OUTSAMPLE Rows with more than 1 no headline on the same date for each company:\")\n",
        "print(detailed_no_headline_rows.head(5))\n",
        "\n",
        "# Remove duplicates keeping only the first occurrence\n",
        "outsample_df_cleaned = outsample_df.drop_duplicates(subset=['companyname', 'date', 'headline'], keep='first')\n",
        "\n",
        "# Save the cleaned DataFrame back into outsample_df\n",
        "outsample_df = outsample_df_cleaned\n",
        "\n",
        "# Verify the cleaning process\n",
        "no_headline_cleaned = outsample_df[outsample_df['headline'] == '[No_Headline]']\n",
        "multiple_no_headline_cleaned = no_headline_cleaned.groupby(['companyname', 'date']).size()\n",
        "\n",
        "# Filter to keep only groups with more than one occurrence after cleaning\n",
        "multiple_no_headline_cleaned = multiple_no_headline_cleaned[multiple_no_headline_cleaned > 1].reset_index()\n",
        "\n",
        "# Check if there are any duplicates left\n",
        "if not multiple_no_headline_cleaned.empty:\n",
        "    print(\"There are still rows with more than 1 no headline on the same date for each company after cleaning:\")\n",
        "    print(multiple_no_headline_cleaned.head(5))\n",
        "else:\n",
        "    print(\"No more duplicate 'No_Headline' rows found for the same date and company after cleaning.\")\n",
        "\n",
        "# Display cleaned DataFrame\n",
        "print(\"Cleaned OUTSAMPLE DataFrame:\")\n",
        "print(outsample_df.head(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRBYWw6CIPoV"
      },
      "outputs": [],
      "source": [
        "# Filter the DataFrame to show rows where 'headline' is '[No_Headline]'\n",
        "no_headline_rows = outsample_df[outsample_df['headline'] == '[No_Headline]']\n",
        "\n",
        "# Group by 'companyname' and 'announcedate' and count occurrences\n",
        "no_headline_counts = no_headline_rows.groupby(['companyname', 'date']).size()\n",
        "\n",
        "# Filter to keep only groups with more than one occurrence\n",
        "multiple_no_headline = no_headline_counts[no_headline_counts > 1].reset_index()\n",
        "\n",
        "# Merge to get detailed rows for these groups\n",
        "detailed_no_headline_rows = no_headline_rows.merge(multiple_no_headline, on=['companyname', 'date'])\n",
        "\n",
        "# Display these rows\n",
        "print(\"OUTSAMPLE Rows with more than 1 no headline on the same date for each company:\")\n",
        "detailed_no_headline_rows.head(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2lNyhKOIPoV"
      },
      "outputs": [],
      "source": [
        "# Remove the column '0' from insample_df if it exists\n",
        "if '0' in insample_df.columns:\n",
        "    insample_df = insample_df.drop(columns=['0'])\n",
        "\n",
        "# Remove the column '0' from outsample_df if it exists\n",
        "if '0' in outsample_df.columns:\n",
        "    outsample_df = outsample_df.drop(columns=['0'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUuohzP5IPoW"
      },
      "outputs": [],
      "source": [
        "# Count the unique company names in insample_df\n",
        "unique_companynames_insample = insample_df['companyname'].nunique()\n",
        "\n",
        "# Count the unique company names in outsample_df\n",
        "unique_companynames_outsample = outsample_df['companyname'].nunique()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Unique company names in insample_df: {unique_companynames_insample}\")\n",
        "print(f\"Unique company names in outsample_df: {unique_companynames_outsample}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk_3MtMQIPoW"
      },
      "outputs": [],
      "source": [
        "# Get unique company names from each DataFrame\n",
        "unique_companynames_insample = insample_df['companyname'].unique()\n",
        "unique_companynames_outsample = outsample_df['companyname'].unique()\n",
        "\n",
        "# Combine the unique company names and get the unique values across both DataFrames\n",
        "all_unique_companynames = pd.unique(pd.concat([pd.Series(unique_companynames_insample), pd.Series(unique_companynames_outsample)]))\n",
        "\n",
        "# Sort the unique company names\n",
        "sorted_unique_companynames = sorted(all_unique_companynames)\n",
        "\n",
        "# Print the sorted unique company names\n",
        "print(\"Sorted unique company names from both DataFrames:\")\n",
        "for company in sorted_unique_companynames:\n",
        "    print(company)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5-LvxxLIPoW"
      },
      "outputs": [],
      "source": [
        "# List of companies to remove\n",
        "companies_to_remove = ['Gray Television, Inc.', 'Scholastic Corporation', 'RADCOM Ltd.', 'ATN International, Inc.']\n",
        "\n",
        "# Check if 'companyname' column exists in both DataFrames\n",
        "if 'companyname' in insample_df.columns and 'companyname' in outsample_df.columns:\n",
        "    # Filter insample_df\n",
        "    insample_df = insample_df[~insample_df['companyname'].isin(companies_to_remove)]\n",
        "\n",
        "    # Filter outsample_df\n",
        "    outsample_df = outsample_df[~outsample_df['companyname'].isin(companies_to_remove)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XBOUmG1IPoW"
      },
      "outputs": [],
      "source": [
        "# Count the unique company names in insample_df\n",
        "unique_companynames_insample = insample_df['companyname'].nunique()\n",
        "\n",
        "# Count the unique company names in outsample_df\n",
        "unique_companynames_outsample = outsample_df['companyname'].nunique()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Unique company names in insample_df: {unique_companynames_insample}\")\n",
        "print(f\"Unique company names in outsample_df: {unique_companynames_outsample}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Si6ibzwdIPoW"
      },
      "outputs": [],
      "source": [
        "# Get unique company names from each DataFrame\n",
        "unique_companynames_insample = insample_df['companyname'].unique()\n",
        "unique_companynames_outsample = outsample_df['companyname'].unique()\n",
        "\n",
        "# Combine the unique company names and get the unique values across both DataFrames\n",
        "all_unique_companynames = pd.unique(pd.concat([pd.Series(unique_companynames_insample), pd.Series(unique_companynames_outsample)]))\n",
        "\n",
        "# Sort the unique company names\n",
        "sorted_unique_companynames = sorted(all_unique_companynames)\n",
        "\n",
        "# Print the sorted unique company names\n",
        "print(\"Sorted unique company names from both DataFrames:\")\n",
        "for company in sorted_unique_companynames:\n",
        "    print(company)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUuAFtmjIPoW"
      },
      "outputs": [],
      "source": [
        "# Save the Insample DataFrame to a CSV file\n",
        "insample_df.to_csv('insample_df.csv', index=False)\n",
        "\n",
        "# Save the Outsample DataFrame to a CSV file\n",
        "outsample_df.to_csv('outsample_df.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3115JILcIPoW"
      },
      "source": [
        "# Tokenize and Word Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opBmbALXIPoW"
      },
      "outputs": [],
      "source": [
        "!pip install nltk wordcloud matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4zjJtWVIPoW"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "\n",
        "# Download stopwords from NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovthGxiDIPoW"
      },
      "outputs": [],
      "source": [
        "# Reload the insample and outsample datasets\n",
        "insample_df = pd.read_csv('insample_df.csv')\n",
        "outsample_df = pd.read_csv('outsample_df.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyVREmJKIPoW"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUa11j84IPoW"
      },
      "outputs": [],
      "source": [
        "insample_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTt8hS-XIPoW"
      },
      "outputs": [],
      "source": [
        "# Function to remove duplicate Date From and Date To for each company\n",
        "def remove_duplicates(df):\n",
        "    return df.drop_duplicates(subset=['companyname', 'Date From', 'Date To'])\n",
        "\n",
        "# Remove duplicates\n",
        "insample_df = remove_duplicates(insample_df)\n",
        "\n",
        "def plot_weekly_return(df, title):\n",
        "    # Convert 'Date From' and 'Date To' to datetime format\n",
        "    df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "    df['Date To'] = pd.to_datetime(df['Date To'])\n",
        "\n",
        "    unique_companies = sorted(df['companyname'].unique())  # Sort companies alphabetically\n",
        "\n",
        "    # Calculate ideal plot dimensions based on number of companies\n",
        "    num_cols = 5\n",
        "    num_rows = (len(unique_companies) + num_cols - 1) // num_cols\n",
        "\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(25, 25), sharey=True)  # Adjusted size\n",
        "    fig.suptitle(title, fontsize=20, fontweight='bold', color='navy')\n",
        "\n",
        "    for i, company in enumerate(unique_companies):\n",
        "        row, col = divmod(i, num_cols)\n",
        "        ax = axes[row, col]\n",
        "\n",
        "        company_data = df[df['companyname'] == company].sort_values(by='Date From')\n",
        "\n",
        "        # Use line plot for better visualization of trends\n",
        "        ax.plot(company_data['Date From'], company_data['Weekly Compound Return'],\n",
        "                linewidth=1.5, alpha=0.7, color='blue')\n",
        "\n",
        "        ax.set_title(company, fontsize=12, fontweight='bold')  # Smaller title\n",
        "        ax.set_xlabel('Date', fontsize=10, fontweight='bold')    # Smaller label\n",
        "        ax.set_ylabel('Weekly Return', fontsize=10, fontweight='bold')\n",
        "        ax.grid(True, linestyle='--', linewidth=0.5)\n",
        "\n",
        "        # Set vertical scale from -0.5 to 0.75\n",
        "        ax.set_ylim(-0.5, 0.75)\n",
        "\n",
        "        # Dynamically adjust x-axis ticks based on date range\n",
        "        date_range = company_data['Date From'].max() - company_data['Date From'].min()\n",
        "        if date_range > pd.Timedelta(days=365*2):\n",
        "            ax.xaxis.set_major_locator(mdates.YearLocator())\n",
        "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "        elif date_range > pd.Timedelta(days=365):\n",
        "            ax.xaxis.set_major_locator(mdates.MonthLocator(bymonth=[1, 4, 7, 10]))\n",
        "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
        "        else:\n",
        "            ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
        "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
        "\n",
        "        ax.tick_params(axis='x', rotation=45, labelsize=10)\n",
        "        ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # Remove any unused subplots\n",
        "    for i in range(len(unique_companies), num_rows * num_cols):\n",
        "        fig.delaxes(axes.flatten()[i])\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "plot_weekly_return(insample_df, 'In-sample Weekly Compound Return')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iazw4_rsIPoX"
      },
      "outputs": [],
      "source": [
        "# Function to remove duplicate Date From and Date To for each company\n",
        "def remove_duplicates(df):\n",
        "    return df.drop_duplicates(subset=['companyname', 'Date From', 'Date To'])\n",
        "\n",
        "# Remove duplicates\n",
        "outsample_df = remove_duplicates(outsample_df)\n",
        "\n",
        "def plot_weekly_return(df, title):\n",
        "    # Convert 'Date From' and 'Date To' to datetime format\n",
        "    df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "    df['Date To'] = pd.to_datetime(df['Date To'])\n",
        "\n",
        "    unique_companies = sorted(df['companyname'].unique())  # Sort companies alphabetically\n",
        "\n",
        "    # Calculate ideal plot dimensions based on number of companies\n",
        "    num_cols = 5\n",
        "    num_rows = (len(unique_companies) + num_cols - 1) // num_cols\n",
        "\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(25, 25), sharey=True)  # Adjusted size\n",
        "    fig.suptitle(title, fontsize=20, fontweight='bold', color='navy')\n",
        "\n",
        "    for i, company in enumerate(unique_companies):\n",
        "        row, col = divmod(i, num_cols)\n",
        "        ax = axes[row, col]\n",
        "\n",
        "        company_data = df[df['companyname'] == company].sort_values(by='Date From')\n",
        "\n",
        "        # Use line plot for better visualization of trends\n",
        "        ax.plot(company_data['Date From'], company_data['Weekly Compound Return'],\n",
        "                linewidth=1.5, alpha=0.7, color='blue')\n",
        "\n",
        "        ax.set_title(company, fontsize=12, fontweight='bold')  # Smaller title\n",
        "        ax.set_xlabel('Date', fontsize=10, fontweight='bold')    # Smaller label\n",
        "        ax.set_ylabel('Weekly Return', fontsize=10, fontweight='bold')\n",
        "        ax.grid(True, linestyle='--', linewidth=0.5)\n",
        "\n",
        "        # Set vertical scale from -0.5 to 0.75\n",
        "        ax.set_ylim(-0.5, 0.75)\n",
        "\n",
        "        # Dynamically adjust x-axis ticks based on date range\n",
        "        date_range = company_data['Date From'].max() - company_data['Date From'].min()\n",
        "        if date_range > pd.Timedelta(days=365*2):\n",
        "            ax.xaxis.set_major_locator(mdates.YearLocator())\n",
        "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "        elif date_range > pd.Timedelta(days=365):\n",
        "            ax.xaxis.set_major_locator(mdates.MonthLocator(bymonth=[1, 4, 7, 10]))\n",
        "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
        "        else:\n",
        "            ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
        "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
        "\n",
        "        ax.tick_params(axis='x', rotation=45, labelsize=10)\n",
        "        ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # Remove any unused subplots\n",
        "    for i in range(len(unique_companies), num_rows * num_cols):\n",
        "        fig.delaxes(axes.flatten()[i])\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "plot_weekly_return(outsample_df, 'Out-of-sample Weekly Compound Return')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3p_nu6C_IPoX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.dates as mdates\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Function to remove duplicate Date From and Date To for each company\n",
        "def remove_duplicates(df):\n",
        "    return df.drop_duplicates(subset=['companyname', 'Date From', 'Date To'])\n",
        "\n",
        "# Remove duplicates from insample_df and outsample_df\n",
        "insample_df = remove_duplicates(insample_df)\n",
        "outsample_df = remove_duplicates(outsample_df)\n",
        "\n",
        "# Set the plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "def plot_weekly_return_for_company(df, company_name, title):\n",
        "    # Convert 'Date From' and 'Date To' to datetime format\n",
        "    df['Date From'] = pd.to_datetime(df['Date From'], format='%Y-%m-%d', errors='coerce')\n",
        "    df['Date To'] = pd.to_datetime(df['Date To'], format='%Y-%m-%d', errors='coerce')\n",
        "\n",
        "    # Drop rows where 'Date From' conversion failed\n",
        "    df = df.dropna(subset=['Date From'])\n",
        "\n",
        "    # Ensure the company_data is sorted by 'Date From'\n",
        "    company_data = df[df['companyname'] == company_name].sort_values(by='Date From')\n",
        "\n",
        "    if company_data.empty:\n",
        "        print(f\"No data available for {company_name} in {title}\")\n",
        "        return\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    # Use line plot for better visualization of trends\n",
        "    ax.plot(company_data['Date From'], company_data['Weekly Compound Return'],\n",
        "            linewidth=1.5, alpha=0.7, color='blue', label='Weekly Returns')\n",
        "\n",
        "    ax.set_title(f\"{company_name} - {title}\", fontsize=20, fontweight='bold', color='navy')\n",
        "    ax.set_xlabel('Date', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('Weekly Return', fontsize=14, fontweight='bold')\n",
        "    ax.grid(True, linestyle='--', linewidth=0.5)\n",
        "\n",
        "    # Set the x-axis to show every year\n",
        "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
        "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "\n",
        "    # Rotate the x-axis labels for better readability\n",
        "    plt.xticks(rotation=45, fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "\n",
        "    # Remove the top and right spines\n",
        "    sns.despine()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot for AT&T Inc. in insample and outsample datasets\n",
        "plot_weekly_return_for_company(insample_df, 'AT&T Inc.', 'In sample Weekly Return')\n",
        "plot_weekly_return_for_company(outsample_df, 'AT&T Inc.', 'Out of sample Weekly Return')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_F13sHfXIPoX"
      },
      "outputs": [],
      "source": [
        "# Filtering out rows where headline is '[No_Headline]'\n",
        "insample_df_filtered = insample_df[insample_df['headline'] != '[No_Headline]']\n",
        "outsample_df_filtered = outsample_df[outsample_df['headline'] != '[No_Headline]']\n",
        "\n",
        "# Grouping by companyname and counting headlines\n",
        "insample_counts = insample_df_filtered.groupby('companyname')['headline'].count().reset_index()\n",
        "insample_counts.columns = ['companyname', 'insample_count']\n",
        "\n",
        "outsample_counts = outsample_df_filtered.groupby('companyname')['headline'].count().reset_index()\n",
        "outsample_counts.columns = ['companyname', 'out-of-sample_count']\n",
        "\n",
        "# Merging counts\n",
        "counts_df = pd.merge(insample_counts, outsample_counts, on='companyname', how='outer').fillna(0)\n",
        "\n",
        "# Calculating the average counts\n",
        "average_count = counts_df[['insample_count', 'out-of-sample_count']].mean().mean()\n",
        "\n",
        "# Melt the dataframe for easier plotting with Seaborn\n",
        "counts_df_melted = counts_df.melt(id_vars='companyname', value_vars=['insample_count', 'out-of-sample_count'],\n",
        "                                  var_name='Type', value_name='Count')\n",
        "\n",
        "# Plotting with Seaborn\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "# Create the bar plot\n",
        "bar_plot = sns.barplot(data=counts_df_melted, x='companyname', y='Count', hue='Type', palette='viridis')\n",
        "\n",
        "# Adding the average line\n",
        "plt.axhline(y=average_count, color='r', linestyle='--', linewidth=2, label='Average Count')\n",
        "\n",
        "# Customizing the plot\n",
        "plt.title('Headline Count per Company in In-sample and Out-of-sample Data')\n",
        "plt.xlabel('Company')\n",
        "plt.ylabel('Headline Count')\n",
        "plt.xticks(rotation=90)\n",
        "plt.legend(title='Type')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C8o2-KYIPoX"
      },
      "source": [
        "## Tokenizing, Top Tokens overall and aggregated on each stocks, and Word Cloud\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03nzUoSBIPoX"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess and tokenize text, removing tokens that contain any part of the company name and specific tokens\n",
        "def tokenize(text, company_name):\n",
        "    company_parts = set(company_name.lower().split())\n",
        "    additional_tokens_to_remove = {\"'s\", \"google\"}\n",
        "\n",
        "    # Generate a set of years from 2004 to 2023\n",
        "    years_to_remove = {str(year) for year in range(2004, 2024)}\n",
        "\n",
        "    tokens_to_remove = company_parts.union(additional_tokens_to_remove, years_to_remove)\n",
        "\n",
        "    tokens = word_tokenize(text.lower())  # tokenize and convert to lower case\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english') and word not in string.punctuation]\n",
        "    tokens = [word for word in tokens if word not in tokens_to_remove]  # remove tokens containing any part of the company name and specific tokens\n",
        "    return tokens\n",
        "\n",
        "# Filter out '[No_Headline]' before tokenizing\n",
        "insample_filtered = insample_df[insample_df['headline'] != '[No_Headline]']\n",
        "outsample_filtered = outsample_df[outsample_df['headline'] != '[No_Headline]']\n",
        "\n",
        "# Tokenize headlines, passing the company name to the tokenize function\n",
        "insample_filtered['tokens'] = insample_filtered.apply(lambda row: tokenize(row['headline'], row['companyname']), axis=1)\n",
        "outsample_filtered['tokens'] = outsample_filtered.apply(lambda row: tokenize(row['headline'], row['companyname']), axis=1)\n",
        "\n",
        "# Function to aggregate tokens by stock and globally\n",
        "def aggregate_tokens(data):\n",
        "    all_tokens = []\n",
        "    token_counts_per_stock = {}\n",
        "\n",
        "    # Iterate over each row and update token counts per stock and globally\n",
        "    for _, row in data.iterrows():\n",
        "        tokens = row['tokens']\n",
        "        stock = row['companyname']\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "        if stock not in token_counts_per_stock:\n",
        "            token_counts_per_stock[stock] = Counter()\n",
        "        token_counts_per_stock[stock].update(tokens)\n",
        "\n",
        "    # Global token counts\n",
        "    global_token_counts = Counter(all_tokens)\n",
        "\n",
        "    return token_counts_per_stock, global_token_counts\n",
        "\n",
        "# Aggregate tokens for insample and outsample data\n",
        "insample_token_counts_per_stock, insample_global_token_counts = aggregate_tokens(insample_filtered)\n",
        "outsample_token_counts_per_stock, outsample_global_token_counts = aggregate_tokens(outsample_filtered)\n",
        "\n",
        "# Function to display top tokens in a table\n",
        "def display_top_tokens(token_counts, num_top_tokens=10):\n",
        "    return pd.DataFrame({\n",
        "        'Token': [token for token, _ in token_counts.most_common(num_top_tokens)],\n",
        "        'Count': [count for _, count in token_counts.most_common(num_top_tokens)]\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7ujk3iwIPoX"
      },
      "outputs": [],
      "source": [
        "# Example usage: Display top 10 tokens for a specific stock and globally\n",
        "print(\"In-sample Global Top Tokens:\")\n",
        "print(display_top_tokens(insample_global_token_counts))\n",
        "print(\"\\nOut-sample Global Top Tokens:\")\n",
        "print(display_top_tokens(outsample_global_token_counts))\n",
        "\n",
        "# Example for a specific stock, say 'AT&T Inc.'\n",
        "print(\"\\nIn-sample AT&T Inc. Top Tokens:\")\n",
        "if 'AT&T Inc.' in insample_token_counts_per_stock:\n",
        "    print(display_top_tokens(insample_token_counts_per_stock['AT&T Inc.']))\n",
        "else:\n",
        "    print(\"No data for AT&T Inc.\")\n",
        "\n",
        "# Example for a specific stock, say 'AT&T Inc.'\n",
        "print(\"\\nOut-sample AT&T Inc. Top Tokens:\")\n",
        "if 'AT&T Inc.' in outsample_token_counts_per_stock:\n",
        "    print(display_top_tokens(outsample_token_counts_per_stock['AT&T Inc.']))\n",
        "else:\n",
        "    print(\"No data for AT&T Inc.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6JRwWM6IPoX"
      },
      "outputs": [],
      "source": [
        "# Function to plot top tokens as a bar chart with improved aesthetics\n",
        "def plot_top_tokens(token_counts, title, num_top_tokens=10):\n",
        "    tokens, counts = zip(*token_counts.most_common(num_top_tokens))\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Use seaborn for better aesthetics\n",
        "    sns.barplot(x=list(tokens), y=list(counts), palette='viridis')\n",
        "\n",
        "    # Add value annotations on bars\n",
        "    for i, count in enumerate(counts):\n",
        "        plt.text(i, count + max(counts) * 0.01, str(count), ha='center', va='bottom')\n",
        "\n",
        "    plt.xlabel('Tokens', fontsize=14)\n",
        "    plt.ylabel('Counts', fontsize=14)\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plotting in-sample and out-sample global top tokens\n",
        "plot_top_tokens(insample_global_token_counts, 'In-sample Global Top-10 Tokens')\n",
        "plot_top_tokens(outsample_global_token_counts, 'Out-of-sample Global Top-10 Tokens')\n",
        "\n",
        "# Plotting for a specific stock, say 'AT&T Inc.'\n",
        "if 'AT&T Inc.' in outsample_token_counts_per_stock:\n",
        "    plot_top_tokens(outsample_token_counts_per_stock['AT&T Inc.'], 'Out-of-sample AT&T Inc. Top-10 Tokens')\n",
        "else:\n",
        "    print(\"No data for AT&T Inc.\")\n",
        "\n",
        "# Plotting for a specific stock, say 'AT&T Inc.'\n",
        "if 'AT&T Inc.' in insample_token_counts_per_stock:\n",
        "    plot_top_tokens(insample_token_counts_per_stock['AT&T Inc.'], 'In-of-sample AT&T Inc. Top-10 Tokens')\n",
        "else:\n",
        "    print(\"No data for AT&T Inc.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkfhNK2rIPoX"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# Function to plot word clouds with improved aesthetics\n",
        "def plot_word_cloud(dataframe, title, cols=5):\n",
        "    # Group by companyname and concatenate tokens\n",
        "    grouped_tokens = dataframe.groupby('companyname')['tokens'].sum()\n",
        "    num_plots = len(grouped_tokens)\n",
        "    rows = math.ceil(num_plots / cols)\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(20, rows * 4))\n",
        "    fig.suptitle(title, fontsize=20)\n",
        "\n",
        "    for idx, (companyname, tokens) in enumerate(grouped_tokens.items()):\n",
        "        row = idx // cols\n",
        "        col = idx % cols\n",
        "        ax = axes[row, col] if rows > 1 else axes[col]\n",
        "\n",
        "        wordcloud = WordCloud(width=800, height=400,\n",
        "                              max_words=100,\n",
        "                              background_color='white',\n",
        "                              colormap='viridis',\n",
        "                              collocations=False).generate(\" \".join(tokens))\n",
        "\n",
        "        ax.imshow(wordcloud, interpolation='bilinear')\n",
        "        ax.set_title(companyname, fontsize=12)\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    # Hide any empty subplots\n",
        "    for idx in range(num_plots, rows * cols):\n",
        "        row = idx // cols\n",
        "        col = idx % cols\n",
        "        ax = axes[row, col] if rows > 1 else axes[col]\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "# Plot word clouds for each companyname in insample and outsample datasets\n",
        "plot_word_cloud(insample_filtered, 'In-sample Word Clouds')\n",
        "plot_word_cloud(outsample_filtered, 'Out-sample Word Clouds')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8V7puSkIPoX"
      },
      "outputs": [],
      "source": [
        "# Function to plot word cloud for a specific company\n",
        "def plot_word_cloud(dataframe, companyname, title):\n",
        "    # Filter dataframe for the specific company\n",
        "    company_data = dataframe[dataframe['companyname'] == companyname]\n",
        "\n",
        "    # Concatenate tokens for the specific company\n",
        "    tokens = company_data['tokens'].sum()\n",
        "\n",
        "    # Generate word cloud with custom settings\n",
        "    wordcloud = WordCloud(width=800, height=400,\n",
        "                          max_words=100,  # limit number of words\n",
        "                          background_color='white',  # set background color\n",
        "                          colormap='viridis',  # use seaborn colormap\n",
        "                          font_path=None,  # you can set a specific font path here\n",
        "                          collocations=False).generate(\" \".join(tokens))\n",
        "\n",
        "    # Plot word cloud\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(f'Word Cloud for {companyname} - {title}', fontsize=16)\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout(pad=0)\n",
        "    plt.show()\n",
        "\n",
        "# Plot word cloud for 'AT&T Inc.' in insample and outsample datasets\n",
        "plot_word_cloud(insample_filtered, 'AT&T Inc.', 'Insample')\n",
        "plot_word_cloud(outsample_filtered, 'AT&T Inc.', 'Outsample')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}