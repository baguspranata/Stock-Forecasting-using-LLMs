{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUlnSDoRlh8r"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yd6jKAAjS5vh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertModel,RobertaTokenizer, RobertaModel, DistilBertTokenizer, DistilBertModel,DistilBertForSequenceClassification, RobertaTokenizer,  AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import timedelta\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import matplotlib.dates as mdates\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "import plotly.graph_objs as go\n",
        "import plotly.express as px\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dySoInPTlh8v"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2tWWJtJlh8v"
      },
      "source": [
        "## Load Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYv0ln-1lh8v"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load embedded Dataframe\n",
        "with open('embedding-BERT-AllCompany-NEW.pkl', 'rb') as f:\n",
        "    insample_df, outsample_df = pd.read_pickle(f)\n",
        "\n",
        "# Display the first 2 rows of outsample_df to check\n",
        "filtered_df = outsample_df[outsample_df['headline'] != '[No_Headline]']\n",
        "filtered_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74rEW5_Blh8w"
      },
      "source": [
        "## Accuracy per-companies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PTU-2SfTlh8w"
      },
      "outputs": [],
      "source": [
        "# Combine insample and outsample data for rolling window\n",
        "df = pd.concat([insample_df, outsample_df])\n",
        "\n",
        "# Convert 'Date From' to datetime\n",
        "df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "\n",
        "# Sort by date\n",
        "df = df.sort_values(by='Date From')\n",
        "\n",
        "# Check available years\n",
        "available_years = df['Date From'].dt.year.unique()\n",
        "print(\"Years available in the data:\", available_years)\n",
        "\n",
        "# Define rolling window parameters\n",
        "window_size = 365 * 10  # 10 years in days\n",
        "prediction_period = 365  # 1 year in days\n",
        "\n",
        "# Get unique companies\n",
        "companies = df['companyname'].unique()\n",
        "\n",
        "# Dictionary to store DataFrames for each company\n",
        "company_results_dfs = {}\n",
        "\n",
        "# Rolling window analysis for each company\n",
        "for company in companies:\n",
        "    company_df = df[df['companyname'] == company].copy()\n",
        "\n",
        "    accuracies = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "    years = []\n",
        "\n",
        "    # Setup time series cross-validation\n",
        "    tscv = TimeSeriesSplit(n_splits=5)  # You can adjust the number of splits as needed\n",
        "\n",
        "    for train_index, test_index in tscv.split(company_df):\n",
        "        train_df, test_df = company_df.iloc[train_index], company_df.iloc[test_index]\n",
        "\n",
        "        if len(test_df) == 0 or len(train_df) == 0:\n",
        "            continue\n",
        "\n",
        "        X_train = np.vstack(train_df['embedding'].values)\n",
        "        y_train = train_df['Future Return Direction'].values\n",
        "\n",
        "        X_test = np.vstack(test_df['embedding'].values)\n",
        "        y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "        # Train logistic regression model\n",
        "        logistic_model = LogisticRegression(max_iter=1000)\n",
        "        logistic_model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = logistic_model.predict(X_test)\n",
        "\n",
        "        # Ensure the length of predictions and test_df matches\n",
        "        if len(y_pred) != len(test_df):\n",
        "            raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_pred)})\")\n",
        "\n",
        "        # Store the predictions and true values\n",
        "        test_df['predicted'] = y_pred\n",
        "\n",
        "        # Update the company-specific DataFrame with predictions\n",
        "        company_df.loc[test_df.index, 'predicted'] = test_df['predicted']\n",
        "\n",
        "        print(f'Company: {company}')\n",
        "        print(f'Training window: {train_df[\"Date From\"].min()} to {train_df[\"Date From\"].max()}')\n",
        "        print(f'Test window: {test_df[\"Date From\"].min()} to {test_df[\"Date From\"].max()}')\n",
        "        print(f'Predicted years: {test_df[\"Date From\"].dt.year.unique()}')\n",
        "\n",
        "    # Evaluate the Model for the company\n",
        "    for year in range(2016, 2024):\n",
        "        year_df = company_df[company_df['Date From'].dt.year == year]\n",
        "\n",
        "        if 'predicted' not in year_df.columns or year_df['predicted'].isnull().all():\n",
        "            print(f\"Missing predictions for year {year} for company {company} due to insufficient data or missing predictions.\")\n",
        "            continue\n",
        "\n",
        "        y_test = year_df['Future Return Direction'].values\n",
        "        y_pred = year_df['predicted'].values\n",
        "\n",
        "        # Remove NaN values in predictions\n",
        "        valid_indices = ~np.isnan(y_pred)\n",
        "        y_test = y_test[valid_indices]\n",
        "        y_pred = y_pred[valid_indices]\n",
        "\n",
        "        if len(y_pred) == 0 or len(y_test) == 0:\n",
        "            print(f\"Insufficient valid predictions for year {year} for company {company}.\")\n",
        "            continue\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "        recall = recall_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "        f1 = f1_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "\n",
        "        accuracies.append(accuracy)\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "        years.append(year)\n",
        "\n",
        "        print(f'Company: {company}, Year: {year}')\n",
        "        print(f'Accuracy: {accuracy:.2f}')\n",
        "        print(f'Precision: {precision:.2f}')\n",
        "        print(f'Recall: {recall:.2f}')\n",
        "        print(f'F1 Score: {f1:.2f}')\n",
        "        print()\n",
        "\n",
        "    # Store the results for the company in a DataFrame\n",
        "    results_df = pd.DataFrame({\n",
        "        'Year': years,\n",
        "        'Accuracy': accuracies,\n",
        "        'Precision': precisions,\n",
        "        'Recall': recalls,\n",
        "        'F1 Score': f1_scores\n",
        "    })\n",
        "\n",
        "    # Save the DataFrame in the dictionary\n",
        "    company_results_dfs[company] = results_df\n",
        "\n",
        "    print(f\"Evaluation results for {company}:\")\n",
        "    print(results_df)\n",
        "\n",
        "    # Plot accuracy over time for the company\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(results_df['Year'], results_df['Accuracy'], marker='o', label='Accuracy')\n",
        "    plt.plot(results_df['Year'], results_df['Precision'], marker='o', label='Precision')\n",
        "    plt.plot(results_df['Year'], results_df['Recall'], marker='o', label='Recall')\n",
        "    plt.plot(results_df['Year'], results_df['F1 Score'], marker='o', label='F1 Score')\n",
        "\n",
        "    plt.title(f'Performance Metrics Over Time for {company}')\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot confusion matrices for each year\n",
        "    n_plots = len(years)\n",
        "    n_cols = 4\n",
        "    n_rows = (n_plots // n_cols) + (n_plots % n_cols > 0)\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 4 * n_rows))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, year in enumerate(years):\n",
        "        year_df = company_df[company_df['Date From'].dt.year == year]\n",
        "        y_test = year_df['Future Return Direction'].values\n",
        "        y_pred = year_df['predicted'].values\n",
        "\n",
        "        # Remove NaN values in predictions\n",
        "        valid_indices = ~np.isnan(y_pred)\n",
        "        y_test = y_test[valid_indices]\n",
        "        y_pred = y_pred[valid_indices]\n",
        "\n",
        "        if len(y_pred) > 0 and len(y_test) > 0:\n",
        "            cm = confusion_matrix(y_test, y_pred)\n",
        "            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logistic_model.classes_)\n",
        "            disp.plot(cmap='Blues', ax=axes[i])\n",
        "            axes[i].set_title(f'{company} {year}')\n",
        "\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Access the stored DataFrames\n",
        "for company, results_df in company_results_dfs.items():\n",
        "    print(f\"\\nResults for {company}:\")\n",
        "    print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZxLBgAqlh8x"
      },
      "outputs": [],
      "source": [
        "# Concatenate all the company results into one DataFrame\n",
        "all_results_df = pd.concat(company_results_dfs.values(), keys=company_results_dfs.keys()).reset_index(level=0).rename(columns={'level_0': 'Company'})\n",
        "\n",
        "# Save the combined DataFrame to a CSV file\n",
        "all_results_df.to_csv('[EVAL] BERT_all_company_results.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8pfL84Nlh8x"
      },
      "source": [
        "## Accuracy all years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RV-n5_41lh8x"
      },
      "outputs": [],
      "source": [
        "# Combine insample and outsample data for rolling window\n",
        "df = pd.concat([insample_df, outsample_df])\n",
        "\n",
        "# Convert 'Date From' to datetime\n",
        "df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "\n",
        "# Sort by date\n",
        "df = df.sort_values(by='Date From')\n",
        "\n",
        "# Check available years\n",
        "available_years = df['Date From'].dt.year.unique()\n",
        "print(\"Years available in the data:\", available_years)\n",
        "\n",
        "# Define rolling window parameters\n",
        "window_size = 365 * 10  # 10 years in days\n",
        "prediction_period = 365  # 1 year in days\n",
        "\n",
        "# Get unique companies\n",
        "companies = df['companyname'].unique()\n",
        "\n",
        "# Dictionary to store DataFrames for each company\n",
        "company_results_dfs = {}\n",
        "\n",
        "# Rolling window analysis for each company\n",
        "for company in companies:\n",
        "    company_df = df[df['companyname'] == company].copy()\n",
        "\n",
        "    accuracies = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "    years = []\n",
        "\n",
        "    # Setup time series cross-validation\n",
        "    tscv = TimeSeriesSplit(n_splits=5)  # You can adjust the number of splits as needed\n",
        "\n",
        "    for train_index, test_index in tscv.split(company_df):\n",
        "        train_df, test_df = company_df.iloc[train_index], company_df.iloc[test_index]\n",
        "\n",
        "        if len(test_df) == 0 or len(train_df) == 0:\n",
        "            continue\n",
        "\n",
        "        X_train = np.vstack(train_df['embedding'].values)\n",
        "        y_train = train_df['Future Return Direction'].values\n",
        "\n",
        "        X_test = np.vstack(test_df['embedding'].values)\n",
        "        y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "        # Train logistic regression model\n",
        "        logistic_model = LogisticRegression(max_iter=1000)\n",
        "        logistic_model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = logistic_model.predict(X_test)\n",
        "\n",
        "        # Ensure the length of predictions and test_df matches\n",
        "        if len(y_pred) != len(test_df):\n",
        "            raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_pred)})\")\n",
        "\n",
        "        # Store the predictions and true values\n",
        "        test_df['predicted'] = y_pred\n",
        "\n",
        "        # Update the company-specific DataFrame with predictions\n",
        "        company_df.loc[test_df.index, 'predicted'] = test_df['predicted']\n",
        "\n",
        "    # Evaluate the Model for the company\n",
        "    for year in range(2016, 2024):\n",
        "        year_df = company_df[company_df['Date From'].dt.year == year]\n",
        "\n",
        "        if 'predicted' not in year_df.columns or year_df['predicted'].isnull().all():\n",
        "            continue\n",
        "\n",
        "        y_test = year_df['Future Return Direction'].values\n",
        "        y_pred = year_df['predicted'].values\n",
        "\n",
        "        # Remove NaN values in predictions\n",
        "        valid_indices = ~np.isnan(y_pred)\n",
        "        y_test = y_test[valid_indices]\n",
        "        y_pred = y_pred[valid_indices]\n",
        "\n",
        "        if len(y_pred) == 0 or len(y_test) == 0:\n",
        "            continue\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "        recall = recall_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "        f1 = f1_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "\n",
        "        accuracies.append(accuracy)\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "        years.append(year)\n",
        "\n",
        "    # Store the results for the company in a DataFrame\n",
        "    results_df = pd.DataFrame({\n",
        "        'Year': years,\n",
        "        'Accuracy': accuracies,\n",
        "        'Precision': precisions,\n",
        "        'Recall': recalls,\n",
        "        'F1 Score': f1_scores\n",
        "    })\n",
        "\n",
        "    # Save the DataFrame in the dictionary\n",
        "    company_results_dfs[company] = results_df\n",
        "\n",
        "# Combine results of all companies into a single DataFrame\n",
        "combined_results = pd.concat(company_results_dfs.values(), ignore_index=True)\n",
        "\n",
        "# Calculate average accuracy per year across all companies\n",
        "average_metrics_per_year = combined_results.groupby('Year').mean().reset_index()\n",
        "\n",
        "# Print average metrics for each year\n",
        "print(\"Average Metrics Per Year:\")\n",
        "print(average_metrics_per_year)\n",
        "\n",
        "# Plot average accuracy per year\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "plt.plot(average_metrics_per_year['Year'], average_metrics_per_year['Accuracy'], marker='o', label='Accuracy')\n",
        "plt.plot(average_metrics_per_year['Year'], average_metrics_per_year['Precision'], marker='o', label='Precision')\n",
        "plt.plot(average_metrics_per_year['Year'], average_metrics_per_year['Recall'], marker='o', label='Recall')\n",
        "plt.plot(average_metrics_per_year['Year'], average_metrics_per_year['F1 Score'], marker='o', label='F1 Score')\n",
        "\n",
        "plt.title('Average Performance Metrics Over Time Across All Companies')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56tzw0eflh8y"
      },
      "source": [
        "## Standard Deviation all years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-Ukpad9lh8y"
      },
      "outputs": [],
      "source": [
        "# Combine insample and outsample data for rolling window\n",
        "df = pd.concat([insample_df, outsample_df])\n",
        "\n",
        "# Convert 'Date From' to datetime\n",
        "df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "\n",
        "# Sort by date\n",
        "df = df.sort_values(by='Date From')\n",
        "\n",
        "# Check available years\n",
        "available_years = df['Date From'].dt.year.unique()\n",
        "print(\"Years available in the data:\", available_years)\n",
        "\n",
        "# Get unique companies\n",
        "companies = df['companyname'].unique()\n",
        "\n",
        "# Dictionary to store DataFrames for each company\n",
        "company_results_dfs = {}\n",
        "\n",
        "# Rolling window analysis for each company\n",
        "for company in companies:\n",
        "    company_df = df[df['companyname'] == company].copy()\n",
        "\n",
        "    accuracies = []\n",
        "    years = []\n",
        "\n",
        "    # Setup time series cross-validation\n",
        "    tscv = TimeSeriesSplit(n_splits=5)  # You can adjust the number of splits as needed\n",
        "\n",
        "    for train_index, test_index in tscv.split(company_df):\n",
        "        train_df, test_df = company_df.iloc[train_index], company_df.iloc[test_index]\n",
        "\n",
        "        if len(test_df) == 0 or len(train_df) == 0:\n",
        "            continue\n",
        "\n",
        "        X_train = np.vstack(train_df['embedding'].values)\n",
        "        y_train = train_df['Future Return Direction'].values\n",
        "\n",
        "        X_test = np.vstack(test_df['embedding'].values)\n",
        "        y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "        # Train logistic regression model\n",
        "        logistic_model = LogisticRegression(max_iter=1000)\n",
        "        logistic_model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = logistic_model.predict(X_test)\n",
        "\n",
        "        # Ensure the length of predictions and test_df matches\n",
        "        if len(y_pred) != len(test_df):\n",
        "            raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_pred)})\")\n",
        "\n",
        "        # Store the predictions and true values\n",
        "        test_df['predicted'] = y_pred\n",
        "\n",
        "        # Update the company-specific DataFrame with predictions\n",
        "        company_df.loc[test_df.index, 'predicted'] = test_df['predicted']\n",
        "\n",
        "    # Evaluate the Model for the company\n",
        "    for year in range(2016, 2024):\n",
        "        year_df = company_df[company_df['Date From'].dt.year == year]\n",
        "\n",
        "        if 'predicted' not in year_df.columns or year_df['predicted'].isnull().all():\n",
        "            continue\n",
        "\n",
        "        y_test = year_df['Future Return Direction'].values\n",
        "        y_pred = year_df['predicted'].values\n",
        "\n",
        "        # Remove NaN values in predictions\n",
        "        valid_indices = ~np.isnan(y_pred)\n",
        "        y_test = y_test[valid_indices]\n",
        "        y_pred = y_pred[valid_indices]\n",
        "\n",
        "        if len(y_pred) == 0 or len(y_test) == 0:\n",
        "            continue\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        accuracies.append(accuracy)\n",
        "        years.append(year)\n",
        "\n",
        "    # Store the results for the company in a DataFrame\n",
        "    results_df = pd.DataFrame({\n",
        "        'Year': years,\n",
        "        'Accuracy': accuracies\n",
        "    })\n",
        "\n",
        "    # Save the DataFrame in the dictionary\n",
        "    company_results_dfs[company] = results_df\n",
        "\n",
        "# Combine results of all companies into a single DataFrame\n",
        "combined_results = pd.concat(company_results_dfs.values(), ignore_index=True)\n",
        "\n",
        "# Calculate average accuracy per year across all companies\n",
        "average_metrics_per_year = combined_results.groupby('Year').agg(\n",
        "    Accuracy_mean=('Accuracy', 'mean'),\n",
        "    Accuracy_std=('Accuracy', 'std')\n",
        ").reset_index()\n",
        "\n",
        "# Print average metrics for each year\n",
        "print(\"Average Metrics Per Year:\")\n",
        "print(average_metrics_per_year)\n",
        "\n",
        "# Get a color sequence from Plotly's default colors\n",
        "colors = px.colors.qualitative.Plotly\n",
        "\n",
        "# Function to make the color more transparent\n",
        "def get_transparent_color(color, alpha=0.2):\n",
        "    # Convert hex to RGB and then to RGBA\n",
        "    hex_color = color.lstrip('#')\n",
        "    rgb_color = tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n",
        "    return f'rgba({rgb_color[0]}, {rgb_color[1]}, {rgb_color[2]}, {alpha})'\n",
        "\n",
        "# Create a figure\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add the mean line\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=average_metrics_per_year['Year'],\n",
        "    y=average_metrics_per_year['Accuracy_mean'],\n",
        "    mode='lines',\n",
        "    name='Accuracy',\n",
        "    line=dict(color=colors[0], width=2)\n",
        "))\n",
        "\n",
        "# Add the standard deviation shaded area\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=pd.concat([average_metrics_per_year['Year'], average_metrics_per_year['Year'][::-1]]),\n",
        "    y=pd.concat([average_metrics_per_year['Accuracy_mean'] + average_metrics_per_year['Accuracy_std'],\n",
        "                 (average_metrics_per_year['Accuracy_mean'] - average_metrics_per_year['Accuracy_std'])[::-1]]),\n",
        "    fill='toself',\n",
        "    fillcolor=get_transparent_color(colors[0], alpha=0.2),  # Use the same color with transparency\n",
        "    line=dict(color='rgba(255,255,255,0)'),\n",
        "    hoverinfo=\"skip\",\n",
        "    showlegend=False,\n",
        "    name='Accuracy std dev'\n",
        "))\n",
        "\n",
        "# Customize layout\n",
        "fig.update_layout(\n",
        "    title='Average Rolling Window Accuracy Over Time Across All Companies',\n",
        "    xaxis_title='Year',\n",
        "    yaxis_title='Accuracy',\n",
        "    template='plotly_white',\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rX9dKZclh8y"
      },
      "source": [
        "## Portofolio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdRGlqY6lh8y"
      },
      "outputs": [],
      "source": [
        "def prepare_data(insample_df, outsample_df):\n",
        "    df = pd.concat([insample_df, outsample_df])\n",
        "    df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "    df = df.sort_values(by='Date From')\n",
        "    available_years = df['Date From'].dt.year.unique()\n",
        "    print(\"Years available in the data:\", available_years)\n",
        "    return df\n",
        "\n",
        "def rolling_window_analysis(df):\n",
        "    companies = df['companyname'].unique()\n",
        "    predictions_df = pd.DataFrame()\n",
        "\n",
        "    for company in companies:\n",
        "        company_df = df[df['companyname'] == company].copy()\n",
        "        company_df = company_df.sort_values(by='Date From')\n",
        "\n",
        "        # Define the rolling window parameters\n",
        "        start_year = company_df['Date From'].dt.year.min()\n",
        "        end_year = company_df['Date From'].dt.year.max()\n",
        "        window_size = 10\n",
        "        validation_size = 1\n",
        "\n",
        "        for start in range(start_year, end_year - window_size - validation_size + 1):\n",
        "            train_start = start\n",
        "            train_end = start + window_size\n",
        "            val_start = train_end\n",
        "            val_end = train_end + validation_size\n",
        "\n",
        "            train_df = company_df[(company_df['Date From'].dt.year >= train_start) &\n",
        "                                  (company_df['Date From'].dt.year < train_end)]\n",
        "            test_df = company_df[(company_df['Date From'].dt.year >= val_start) &\n",
        "                                 (company_df['Date From'].dt.year < val_end)]\n",
        "\n",
        "            if len(test_df) == 0 or len(train_df) == 0:\n",
        "                continue\n",
        "\n",
        "            X_train = np.vstack(train_df['embedding'].values)\n",
        "            y_train = train_df['Future Return Direction'].values\n",
        "            X_test = np.vstack(test_df['embedding'].values)\n",
        "            y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "            # Train logistic regression model\n",
        "            logistic_model = LogisticRegression(max_iter=1000)\n",
        "            logistic_model.fit(X_train, y_train)\n",
        "\n",
        "            # Get prediction probabilities\n",
        "            y_prob = logistic_model.predict_proba(X_test)[:, 1]  # Probability of the positive class\n",
        "\n",
        "            if len(y_prob) != len(test_df):\n",
        "                raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_prob)})\")\n",
        "\n",
        "            test_df['predicted_prob'] = y_prob\n",
        "            predictions_df = pd.concat([predictions_df, test_df[['Date From', 'companyname', 'predicted_prob', 'Weekly Compound Return']]], ignore_index=True)\n",
        "\n",
        "    df = df.merge(predictions_df, on=['Date From', 'companyname', 'Weekly Compound Return'], how='left', suffixes=('', '_pred'))\n",
        "    return df\n",
        "\n",
        "def construct_portfolio(df, time_period='Week'):\n",
        "    df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "    if time_period == 'Week':\n",
        "        df['Period'] = df['Date From'].dt.to_period('W').dt.to_timestamp()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid time_period. Use 'Week'.\")\n",
        "\n",
        "    portfolio_returns = []\n",
        "\n",
        "    for period, group in df.groupby('Period'):\n",
        "        if period.year < 2016:\n",
        "            continue\n",
        "\n",
        "        # Sort group by predicted_prob descending\n",
        "        group_sorted = group.sort_values(by='predicted_prob', ascending=False)\n",
        "\n",
        "        # Select top and bottom companies\n",
        "        num_top_companies = 5\n",
        "        num_bottom_companies = 5\n",
        "        top_companies = group_sorted.head(num_top_companies)\n",
        "        bottom_companies = group_sorted.tail(num_bottom_companies)\n",
        "\n",
        "        # Equal-weighted returns\n",
        "        long_return_eq = np.mean(np.log1p(top_companies['Weekly Compound Return']))\n",
        "        short_return_eq = np.mean(np.log1p(bottom_companies['Weekly Compound Return']))\n",
        "        long_short_return_eq = long_return_eq - short_return_eq\n",
        "\n",
        "        # Value-weighted returns\n",
        "        long_return_val = np.sum(np.log1p(top_companies['Weekly Compound Return']) * top_companies['market_cap']) / np.sum(top_companies['market_cap'])\n",
        "        short_return_val = np.sum(np.log1p(bottom_companies['Weekly Compound Return']) * bottom_companies['market_cap']) / np.sum(bottom_companies['market_cap'])\n",
        "        long_short_return_val = long_return_val - short_return_val\n",
        "\n",
        "        portfolio_returns.append({\n",
        "            'Period': period,\n",
        "            'Long Return (Eq)': long_return_eq,\n",
        "            'Short Return (Eq)': short_return_eq,\n",
        "            'Long-Short Return (Eq)': long_short_return_eq,\n",
        "            'Long Return (Val)': long_return_val,\n",
        "            'Short Return (Val)': short_return_val,\n",
        "            'Long-Short Return (Val)': long_short_return_val\n",
        "        })\n",
        "\n",
        "    portfolio_df = pd.DataFrame(portfolio_returns)\n",
        "    portfolio_df['EW L'] = portfolio_df['Long Return (Eq)'].cumsum()\n",
        "    portfolio_df['EW S'] = portfolio_df['Short Return (Eq)'].cumsum()\n",
        "    portfolio_df['EW LS'] = portfolio_df['Long-Short Return (Eq)'].cumsum()\n",
        "    portfolio_df['VW L'] = portfolio_df['Long Return (Val)'].cumsum()\n",
        "    portfolio_df['VW S'] = portfolio_df['Short Return (Val)'].cumsum()\n",
        "    portfolio_df['VW LS'] = portfolio_df['Long-Short Return (Val)'].cumsum()\n",
        "\n",
        "    actual_returns = df[df['Date From'].dt.year >= 2016].groupby('Period')['Weekly Compound Return'].mean()\n",
        "    actual_cumulative_returns = np.log1p(actual_returns).cumsum()\n",
        "    portfolio_df = portfolio_df.merge(actual_cumulative_returns.rename('Market'), on='Period', how='left')\n",
        "\n",
        "    metrics = {}\n",
        "    for portfolio in ['EW L', 'EW S', 'EW LS', 'VW L', 'VW S', 'VW LS']:\n",
        "        returns = portfolio_df[portfolio]\n",
        "\n",
        "        if returns.isnull().all() or returns.eq(0).all():\n",
        "            sharpe_ratio = np.nan\n",
        "            max_drawdown = np.nan\n",
        "            volatility = np.nan\n",
        "        else:\n",
        "            sharpe_ratio = returns.mean() / returns.std() * np.sqrt(52) if returns.std() != 0 else np.nan\n",
        "            cumulative_returns = returns.cumsum()\n",
        "            max_drawdown = (cumulative_returns.cummax() - cumulative_returns).max()\n",
        "            volatility = returns.std() * np.sqrt(52)\n",
        "\n",
        "        metrics[portfolio] = {\n",
        "            'Sharpe Ratio': sharpe_ratio\n",
        "        }\n",
        "\n",
        "        print(f\"Metrics for {portfolio}:\")\n",
        "        print(f\"Sharpe Ratio: {sharpe_ratio}\")\n",
        "        print()\n",
        "\n",
        "    portfolio_df.to_csv('BERT_portfolio_returns.csv', index=False)\n",
        "    print(\"Portfolio returns saved to 'BERT_portfolio_returns.csv'\")\n",
        "    return portfolio_df\n",
        "\n",
        "def plot_portfolio_returns(portfolio_df, title_suffix=''):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['EW L'], marker='o', markersize=1, label='EW L')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['EW S'], marker='o', markersize=1, label='EW S')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['EW LS'], marker='o', markersize=1, label='EW LS')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['VW L'], marker='o', markersize=1, label='VW L')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['VW S'], marker='o', markersize=1, label='VW S')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['VW LS'], marker='o', markersize=1, label='VW LS')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['Market'], marker='o', markersize=1, label='Market')\n",
        "\n",
        "    plt.title(f'Cumulative {title_suffix} Portfolio Returns Over Time')\n",
        "    plt.xlabel('Period')\n",
        "    plt.ylabel('Cumulative Log Return')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage for Weekly\n",
        "df = prepare_data(insample_df, outsample_df)\n",
        "df = rolling_window_analysis(df)\n",
        "\n",
        "# Weekly Portfolio\n",
        "portfolio_df_week = construct_portfolio(df, time_period='Week')\n",
        "portfolio_df_week = portfolio_df_week[portfolio_df_week['Period'].dt.year >= 2016]\n",
        "plot_portfolio_returns(portfolio_df_week, title_suffix='Weekly')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWrzOc1Qlh8z"
      },
      "source": [
        "## Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hw0tKh90lh8z"
      },
      "outputs": [],
      "source": [
        "# Suppress SettingWithCopyWarning\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "def prepare_data(insample_df, outsample_df):\n",
        "    df = pd.concat([insample_df, outsample_df])\n",
        "    df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "    df = df.sort_values(by='Date From')\n",
        "    available_years = df['Date From'].dt.year.unique()\n",
        "    print(\"Years available in the data:\", available_years)\n",
        "    return df\n",
        "\n",
        "def rolling_window_analysis(df):\n",
        "    companies = df['companyname'].unique()\n",
        "    predictions_df = pd.DataFrame()\n",
        "\n",
        "    for company in companies:\n",
        "        company_df = df[df['companyname'] == company].copy()\n",
        "        company_df = company_df.sort_values(by='Date From')\n",
        "\n",
        "        start_year = company_df['Date From'].dt.year.min()\n",
        "        end_year = company_df['Date From'].dt.year.max()\n",
        "        window_size = 10\n",
        "        validation_size = 1\n",
        "\n",
        "        for start in range(start_year, end_year - window_size - validation_size + 1):\n",
        "            train_start = start\n",
        "            train_end = start + window_size\n",
        "            val_start = train_end\n",
        "            val_end = train_end + validation_size\n",
        "\n",
        "            train_df = company_df[(company_df['Date From'].dt.year >= train_start) &\n",
        "                                  (company_df['Date From'].dt.year < train_end)]\n",
        "            test_df = company_df[(company_df['Date From'].dt.year >= val_start) &\n",
        "                                 (company_df['Date From'].dt.year < val_end)]\n",
        "\n",
        "            if len(test_df) == 0 or len(train_df) == 0:\n",
        "                continue\n",
        "\n",
        "            X_train = np.vstack(train_df['embedding'].values)\n",
        "            y_train = train_df['Future Return Direction'].values\n",
        "            X_test = np.vstack(test_df['embedding'].values)\n",
        "            y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "            logistic_model = LogisticRegression(max_iter=1000)\n",
        "            logistic_model.fit(X_train, y_train)\n",
        "\n",
        "            y_prob = logistic_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "            if len(y_prob) != len(test_df):\n",
        "                raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_prob)})\")\n",
        "\n",
        "            test_df['predicted_prob'] = y_prob\n",
        "            predictions_df = pd.concat([predictions_df, test_df[['Date From', 'companyname', 'predicted_prob', 'Weekly Compound Return', 'market_cap']]], ignore_index=True)\n",
        "\n",
        "    df = df.merge(predictions_df, on=['Date From', 'companyname', 'Weekly Compound Return', 'market_cap'], how='left', suffixes=('', '_pred'))\n",
        "    return df\n",
        "\n",
        "def calculate_transaction_costs(df):\n",
        "    np.random.seed(None)  # Ensure we're not using a fixed seed\n",
        "\n",
        "    median_market_cap = df['market_cap'].median()\n",
        "    df['is_large_cap'] = df['market_cap'] > median_market_cap\n",
        "\n",
        "    # Convert transaction costs to basis points\n",
        "    df['transaction_cost'] = np.where(df['is_large_cap'],\n",
        "                                      np.random.normal(10.25, 2.05, df.shape[0]),\n",
        "                                      np.random.normal(21, 4.2, df.shape[0]))\n",
        "\n",
        "    # Ensure no negative transaction costs\n",
        "    df['transaction_cost'] = np.maximum(df['transaction_cost'], 0)\n",
        "\n",
        "    print(f\"Average cost for large-cap stocks: {df[df['is_large_cap']]['transaction_cost'].mean():.2f} bps\")\n",
        "    print(f\"Average cost for small-cap stocks: {df[~df['is_large_cap']]['transaction_cost'].mean():.2f} bps\")\n",
        "\n",
        "    print(f\"\\nLarge-cap cost range: {df[df['is_large_cap']]['transaction_cost'].min():.2f} bps to {df[df['is_large_cap']]['transaction_cost'].max():.2f} bps\")\n",
        "    print(f\"Small-cap cost range: {df[~df['is_large_cap']]['transaction_cost'].min():.2f} bps to {df[~df['is_large_cap']]['transaction_cost'].max():.2f} bps\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def construct_portfolio_with_costs(df, time_period='Week'):\n",
        "    df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "    if time_period == 'Week':\n",
        "        df['Period'] = df['Date From'].dt.to_period('W').dt.to_timestamp()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid time_period. Use 'Week'.\")\n",
        "\n",
        "    portfolio_returns = []\n",
        "\n",
        "    for period, group in df.groupby('Period'):\n",
        "        if period.year < 2016:\n",
        "            continue\n",
        "\n",
        "        group_sorted = group.sort_values(by='predicted_prob', ascending=False)\n",
        "\n",
        "        num_top_companies = 5\n",
        "        num_bottom_companies = 5\n",
        "        top_companies = group_sorted.head(num_top_companies)\n",
        "        bottom_companies = group_sorted.tail(num_bottom_companies)\n",
        "\n",
        "        def calculate_return_with_costs(companies, long_position=True):\n",
        "            returns = np.log1p(companies['Weekly Compound Return'])\n",
        "            costs = companies['transaction_cost'] / 10000  # Convert bps to decimal\n",
        "            if long_position:\n",
        "                return returns - costs\n",
        "            else:\n",
        "                return -returns - costs\n",
        "\n",
        "        long_return_eq = np.mean(calculate_return_with_costs(top_companies, long_position=True))\n",
        "        short_return_eq = np.mean(calculate_return_with_costs(bottom_companies, long_position=False))\n",
        "        long_short_return_eq = long_return_eq - short_return_eq\n",
        "\n",
        "        total_market_cap_long = np.sum(top_companies['market_cap'])\n",
        "        total_market_cap_short = np.sum(bottom_companies['market_cap'])\n",
        "\n",
        "        long_return_val = np.sum(calculate_return_with_costs(top_companies, long_position=True) * top_companies['market_cap']) / total_market_cap_long\n",
        "        short_return_val = np.sum(calculate_return_with_costs(bottom_companies, long_position=False) * bottom_companies['market_cap']) / total_market_cap_short\n",
        "        long_short_return_val = long_return_val - short_return_val\n",
        "\n",
        "        portfolio_returns.append({\n",
        "            'Period': period,\n",
        "            'Long Return (Eq)': long_return_eq,\n",
        "            'Short Return (Eq)': short_return_eq,\n",
        "            'Long-Short Return (Eq)': long_short_return_eq,\n",
        "            'Long Return (Val)': long_return_val,\n",
        "            'Short Return (Val)': short_return_val,\n",
        "            'Long-Short Return (Val)': long_short_return_val\n",
        "        })\n",
        "\n",
        "    portfolio_df = pd.DataFrame(portfolio_returns)\n",
        "    portfolio_df['EW L'] = portfolio_df['Long Return (Eq)'].cumsum()\n",
        "    portfolio_df['EW S'] = portfolio_df['Short Return (Eq)'].cumsum()\n",
        "    portfolio_df['EW LS'] = portfolio_df['Long-Short Return (Eq)'].cumsum()\n",
        "    portfolio_df['VW L'] = portfolio_df['Long Return (Val)'].cumsum()\n",
        "    portfolio_df['VW S'] = portfolio_df['Short Return (Val)'].cumsum()\n",
        "    portfolio_df['VW LS'] = portfolio_df['Long-Short Return (Val)'].cumsum()\n",
        "\n",
        "    actual_returns = df[df['Date From'].dt.year >= 2016].groupby('Period')['Weekly Compound Return'].mean()\n",
        "    actual_cumulative_returns = np.log1p(actual_returns).cumsum()\n",
        "    portfolio_df = portfolio_df.merge(actual_cumulative_returns.rename('Market'), on='Period', how='left')\n",
        "\n",
        "    return portfolio_df\n",
        "\n",
        "# Main execution\n",
        "df = prepare_data(insample_df, outsample_df)\n",
        "df = rolling_window_analysis(df)\n",
        "df = calculate_transaction_costs(df)\n",
        "\n",
        "# Weekly Portfolio with transaction costs\n",
        "portfolio_df_week_with_costs = construct_portfolio_with_costs(df, time_period='Week')\n",
        "portfolio_df_week_with_costs = portfolio_df_week_with_costs[portfolio_df_week_with_costs['Period'].dt.year >= 2016]\n",
        "\n",
        "print(\"Portfolio construction with transaction costs completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2rDyxxalh8z"
      },
      "source": [
        "# RoBERTa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmJY_IFslh8z"
      },
      "source": [
        "## Load Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJcTPpaxlh8z"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained RoBERTa model and tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaModel.from_pretrained('roberta-base')\n",
        "\n",
        "# Load embedded Dataframe\n",
        "with open('embedding-RoBERTa-AllCompany-NEW.pkl', 'rb') as f:\n",
        "    insample_df, outsample_df = pd.read_pickle(f)\n",
        "\n",
        "# Display the first 2 rows of outsample_df to check\n",
        "filtered_df = outsample_df[outsample_df['headline'] != '[No_Headline]']\n",
        "filtered_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79OsM-r8lh8z"
      },
      "source": [
        "## Accuracy per-companies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A09l-10Xlh8z"
      },
      "outputs": [],
      "source": [
        "# Combine insample and outsample data for rolling window\n",
        "df = pd.concat([insample_df, outsample_df])\n",
        "\n",
        "# Convert 'Date From' to datetime\n",
        "df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "\n",
        "# Sort by date\n",
        "df = df.sort_values(by='Date From')\n",
        "\n",
        "# Check available years\n",
        "available_years = df['Date From'].dt.year.unique()\n",
        "print(\"Years available in the data:\", available_years)\n",
        "\n",
        "# Define rolling window parameters\n",
        "window_size = 365 * 10  # 10 years in days\n",
        "prediction_period = 365  # 1 year in days\n",
        "\n",
        "# Get unique companies\n",
        "companies = df['companyname'].unique()\n",
        "\n",
        "# Dictionary to store DataFrames for each company\n",
        "company_results_dfs = {}\n",
        "\n",
        "# Rolling window analysis for each company\n",
        "for company in companies:\n",
        "    company_df = df[df['companyname'] == company].copy()\n",
        "\n",
        "    accuracies = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "    years = []\n",
        "\n",
        "    # Setup time series cross-validation\n",
        "    tscv = TimeSeriesSplit(n_splits=5)  # You can adjust the number of splits as needed\n",
        "\n",
        "    for train_index, test_index in tscv.split(company_df):\n",
        "        train_df, test_df = company_df.iloc[train_index], company_df.iloc[test_index]\n",
        "\n",
        "        if len(test_df) == 0 or len(train_df) == 0:\n",
        "            continue\n",
        "\n",
        "        X_train = np.vstack(train_df['embedding'].values)\n",
        "        y_train = train_df['Future Return Direction'].values\n",
        "\n",
        "        X_test = np.vstack(test_df['embedding'].values)\n",
        "        y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "        # Train logistic regression model\n",
        "        logistic_model = LogisticRegression(max_iter=1000)\n",
        "        logistic_model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = logistic_model.predict(X_test)\n",
        "\n",
        "        # Ensure the length of predictions and test_df matches\n",
        "        if len(y_pred) != len(test_df):\n",
        "            raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_pred)})\")\n",
        "\n",
        "        # Store the predictions and true values\n",
        "        test_df['predicted'] = y_pred\n",
        "\n",
        "        # Update the company-specific DataFrame with predictions\n",
        "        company_df.loc[test_df.index, 'predicted'] = test_df['predicted']\n",
        "\n",
        "        print(f'Company: {company}')\n",
        "        print(f'Training window: {train_df[\"Date From\"].min()} to {train_df[\"Date From\"].max()}')\n",
        "        print(f'Test window: {test_df[\"Date From\"].min()} to {test_df[\"Date From\"].max()}')\n",
        "        print(f'Predicted years: {test_df[\"Date From\"].dt.year.unique()}')\n",
        "\n",
        "    # Evaluate the Model for the company\n",
        "    for year in range(2016, 2024):\n",
        "        year_df = company_df[company_df['Date From'].dt.year == year]\n",
        "\n",
        "        if 'predicted' not in year_df.columns or year_df['predicted'].isnull().all():\n",
        "            print(f\"Missing predictions for year {year} for company {company} due to insufficient data or missing predictions.\")\n",
        "            continue\n",
        "\n",
        "        y_test = year_df['Future Return Direction'].values\n",
        "        y_pred = year_df['predicted'].values\n",
        "\n",
        "        # Remove NaN values in predictions\n",
        "        valid_indices = ~np.isnan(y_pred)\n",
        "        y_test = y_test[valid_indices]\n",
        "        y_pred = y_pred[valid_indices]\n",
        "\n",
        "        if len(y_pred) == 0 or len(y_test) == 0:\n",
        "            print(f\"Insufficient valid predictions for year {year} for company {company}.\")\n",
        "            continue\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "        recall = recall_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "        f1 = f1_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "\n",
        "        accuracies.append(accuracy)\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "        years.append(year)\n",
        "\n",
        "        print(f'Company: {company}, Year: {year}')\n",
        "        print(f'Accuracy: {accuracy:.2f}')\n",
        "        print(f'Precision: {precision:.2f}')\n",
        "        print(f'Recall: {recall:.2f}')\n",
        "        print(f'F1 Score: {f1:.2f}')\n",
        "        print()\n",
        "\n",
        "    # Store the results for the company in a DataFrame\n",
        "    results_df = pd.DataFrame({\n",
        "        'Year': years,\n",
        "        'Accuracy': accuracies,\n",
        "        'Precision': precisions,\n",
        "        'Recall': recalls,\n",
        "        'F1 Score': f1_scores\n",
        "    })\n",
        "\n",
        "    # Save the DataFrame in the dictionary\n",
        "    company_results_dfs[company] = results_df\n",
        "\n",
        "    print(f\"Evaluation results for {company}:\")\n",
        "    print(results_df)\n",
        "\n",
        "    # Plot accuracy over time for the company\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(results_df['Year'], results_df['Accuracy'], marker='o', label='Accuracy')\n",
        "    plt.plot(results_df['Year'], results_df['Precision'], marker='o', label='Precision')\n",
        "    plt.plot(results_df['Year'], results_df['Recall'], marker='o', label='Recall')\n",
        "    plt.plot(results_df['Year'], results_df['F1 Score'], marker='o', label='F1 Score')\n",
        "\n",
        "    plt.title(f'Performance Metrics Over Time for {company}')\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot confusion matrices for each year\n",
        "    n_plots = len(years)\n",
        "    n_cols = 4\n",
        "    n_rows = (n_plots // n_cols) + (n_plots % n_cols > 0)\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 4 * n_rows))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, year in enumerate(years):\n",
        "        year_df = company_df[company_df['Date From'].dt.year == year]\n",
        "        y_test = year_df['Future Return Direction'].values\n",
        "        y_pred = year_df['predicted'].values\n",
        "\n",
        "        # Remove NaN values in predictions\n",
        "        valid_indices = ~np.isnan(y_pred)\n",
        "        y_test = y_test[valid_indices]\n",
        "        y_pred = y_pred[valid_indices]\n",
        "\n",
        "        if len(y_pred) > 0 and len(y_test) > 0:\n",
        "            cm = confusion_matrix(y_test, y_pred)\n",
        "            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logistic_model.classes_)\n",
        "            disp.plot(cmap='Blues', ax=axes[i])\n",
        "            axes[i].set_title(f'{company} {year}')\n",
        "\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Access the stored DataFrames\n",
        "for company, results_df in company_results_dfs.items():\n",
        "    print(f\"\\nResults for {company}:\")\n",
        "    print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMkyZrv7lh80"
      },
      "outputs": [],
      "source": [
        "# Concatenate all the company results into one DataFrame\n",
        "all_results_df = pd.concat(company_results_dfs.values(), keys=company_results_dfs.keys()).reset_index(level=0).rename(columns={'level_0': 'Company'})\n",
        "\n",
        "# Save the combined DataFrame to a CSV file\n",
        "all_results_df.to_csv('[EVAL] RoBERTa_all_company_results.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC-vEn3hlh80"
      },
      "source": [
        "## Accuracy all years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f9zYyYolh80"
      },
      "outputs": [],
      "source": [
        "# Combine insample and outsample data for rolling window\n",
        "df = pd.concat([insample_df, outsample_df])\n",
        "\n",
        "# Convert 'Date From' to datetime\n",
        "df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "\n",
        "# Sort by date\n",
        "df = df.sort_values(by='Date From')\n",
        "\n",
        "# Check available years\n",
        "available_years = df['Date From'].dt.year.unique()\n",
        "print(\"Years available in the data:\", available_years)\n",
        "\n",
        "# Define rolling window parameters\n",
        "window_size = 365 * 10  # 10 years in days\n",
        "prediction_period = 365  # 1 year in days\n",
        "\n",
        "# Get unique companies\n",
        "companies = df['companyname'].unique()\n",
        "\n",
        "# Dictionary to store DataFrames for each company\n",
        "company_results_dfs = {}\n",
        "\n",
        "# Rolling window analysis for each company\n",
        "for company in companies:\n",
        "    company_df = df[df['companyname'] == company].copy()\n",
        "\n",
        "    accuracies = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "    years = []\n",
        "\n",
        "    # Setup time series cross-validation\n",
        "    tscv = TimeSeriesSplit(n_splits=5)  # You can adjust the number of splits as needed\n",
        "\n",
        "    for train_index, test_index in tscv.split(company_df):\n",
        "        train_df, test_df = company_df.iloc[train_index], company_df.iloc[test_index]\n",
        "\n",
        "        if len(test_df) == 0 or len(train_df) == 0:\n",
        "            continue\n",
        "\n",
        "        X_train = np.vstack(train_df['embedding'].values)\n",
        "        y_train = train_df['Future Return Direction'].values\n",
        "\n",
        "        X_test = np.vstack(test_df['embedding'].values)\n",
        "        y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "        # Train logistic regression model\n",
        "        logistic_model = LogisticRegression(max_iter=1000)\n",
        "        logistic_model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = logistic_model.predict(X_test)\n",
        "\n",
        "        # Ensure the length of predictions and test_df matches\n",
        "        if len(y_pred) != len(test_df):\n",
        "            raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_pred)})\")\n",
        "\n",
        "        # Store the predictions and true values\n",
        "        test_df['predicted'] = y_pred\n",
        "\n",
        "        # Update the company-specific DataFrame with predictions\n",
        "        company_df.loc[test_df.index, 'predicted'] = test_df['predicted']\n",
        "\n",
        "    # Evaluate the Model for the company\n",
        "    for year in range(2016, 2024):\n",
        "        year_df = company_df[company_df['Date From'].dt.year == year]\n",
        "\n",
        "        if 'predicted' not in year_df.columns or year_df['predicted'].isnull().all():\n",
        "            continue\n",
        "\n",
        "        y_test = year_df['Future Return Direction'].values\n",
        "        y_pred = year_df['predicted'].values\n",
        "\n",
        "        # Remove NaN values in predictions\n",
        "        valid_indices = ~np.isnan(y_pred)\n",
        "        y_test = y_test[valid_indices]\n",
        "        y_pred = y_pred[valid_indices]\n",
        "\n",
        "        if len(y_pred) == 0 or len(y_test) == 0:\n",
        "            continue\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "        recall = recall_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "        f1 = f1_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "\n",
        "        accuracies.append(accuracy)\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "        years.append(year)\n",
        "\n",
        "    # Store the results for the company in a DataFrame\n",
        "    results_df = pd.DataFrame({\n",
        "        'Year': years,\n",
        "        'Accuracy': accuracies,\n",
        "        'Precision': precisions,\n",
        "        'Recall': recalls,\n",
        "        'F1 Score': f1_scores\n",
        "    })\n",
        "\n",
        "    # Save the DataFrame in the dictionary\n",
        "    company_results_dfs[company] = results_df\n",
        "\n",
        "# Combine results of all companies into a single DataFrame\n",
        "combined_results = pd.concat(company_results_dfs.values(), ignore_index=True)\n",
        "\n",
        "# Calculate average accuracy per year across all companies\n",
        "average_metrics_per_year = combined_results.groupby('Year').mean().reset_index()\n",
        "\n",
        "# Print average metrics for each year\n",
        "print(\"Average Metrics Per Year:\")\n",
        "print(average_metrics_per_year)\n",
        "\n",
        "# Plot average accuracy per year\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "plt.plot(average_metrics_per_year['Year'], average_metrics_per_year['Accuracy'], marker='o', label='Accuracy')\n",
        "plt.plot(average_metrics_per_year['Year'], average_metrics_per_year['Precision'], marker='o', label='Precision')\n",
        "plt.plot(average_metrics_per_year['Year'], average_metrics_per_year['Recall'], marker='o', label='Recall')\n",
        "plt.plot(average_metrics_per_year['Year'], average_metrics_per_year['F1 Score'], marker='o', label='F1 Score')\n",
        "\n",
        "plt.title('Average Performance Metrics Over Time Across All Companies')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0aNL4BTlh80"
      },
      "source": [
        "## Standard Deviation all years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvcZpJzClh80"
      },
      "outputs": [],
      "source": [
        "# Combine insample and outsample data for rolling window\n",
        "df = pd.concat([insample_df, outsample_df])\n",
        "\n",
        "# Convert 'Date From' to datetime\n",
        "df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "\n",
        "# Sort by date\n",
        "df = df.sort_values(by='Date From')\n",
        "\n",
        "# Check available years\n",
        "available_years = df['Date From'].dt.year.unique()\n",
        "print(\"Years available in the data:\", available_years)\n",
        "\n",
        "# Get unique companies\n",
        "companies = df['companyname'].unique()\n",
        "\n",
        "# Dictionary to store DataFrames for each company\n",
        "company_results_dfs = {}\n",
        "\n",
        "# Rolling window analysis for each company\n",
        "for company in companies:\n",
        "    company_df = df[df['companyname'] == company].copy()\n",
        "\n",
        "    accuracies = []\n",
        "    years = []\n",
        "\n",
        "    # Setup time series cross-validation\n",
        "    tscv = TimeSeriesSplit(n_splits=5)  # You can adjust the number of splits as needed\n",
        "\n",
        "    for train_index, test_index in tscv.split(company_df):\n",
        "        train_df, test_df = company_df.iloc[train_index], company_df.iloc[test_index]\n",
        "\n",
        "        if len(test_df) == 0 or len(train_df) == 0:\n",
        "            continue\n",
        "\n",
        "        X_train = np.vstack(train_df['embedding'].values)\n",
        "        y_train = train_df['Future Return Direction'].values\n",
        "\n",
        "        X_test = np.vstack(test_df['embedding'].values)\n",
        "        y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "        # Train logistic regression model\n",
        "        logistic_model = LogisticRegression(max_iter=1000)\n",
        "        logistic_model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = logistic_model.predict(X_test)\n",
        "\n",
        "        # Ensure the length of predictions and test_df matches\n",
        "        if len(y_pred) != len(test_df):\n",
        "            raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_pred)})\")\n",
        "\n",
        "        # Store the predictions and true values\n",
        "        test_df['predicted'] = y_pred\n",
        "\n",
        "        # Update the company-specific DataFrame with predictions\n",
        "        company_df.loc[test_df.index, 'predicted'] = test_df['predicted']\n",
        "\n",
        "    # Evaluate the Model for the company\n",
        "    for year in range(2016, 2024):\n",
        "        year_df = company_df[company_df['Date From'].dt.year == year]\n",
        "\n",
        "        if 'predicted' not in year_df.columns or year_df['predicted'].isnull().all():\n",
        "            continue\n",
        "\n",
        "        y_test = year_df['Future Return Direction'].values\n",
        "        y_pred = year_df['predicted'].values\n",
        "\n",
        "        # Remove NaN values in predictions\n",
        "        valid_indices = ~np.isnan(y_pred)\n",
        "        y_test = y_test[valid_indices]\n",
        "        y_pred = y_pred[valid_indices]\n",
        "\n",
        "        if len(y_pred) == 0 or len(y_test) == 0:\n",
        "            continue\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        accuracies.append(accuracy)\n",
        "        years.append(year)\n",
        "\n",
        "    # Store the results for the company in a DataFrame\n",
        "    results_df = pd.DataFrame({\n",
        "        'Year': years,\n",
        "        'Accuracy': accuracies\n",
        "    })\n",
        "\n",
        "    # Save the DataFrame in the dictionary\n",
        "    company_results_dfs[company] = results_df\n",
        "\n",
        "# Combine results of all companies into a single DataFrame\n",
        "combined_results = pd.concat(company_results_dfs.values(), ignore_index=True)\n",
        "\n",
        "# Calculate average accuracy per year across all companies\n",
        "average_metrics_per_year = combined_results.groupby('Year').agg(\n",
        "    Accuracy_mean=('Accuracy', 'mean'),\n",
        "    Accuracy_std=('Accuracy', 'std')\n",
        ").reset_index()\n",
        "\n",
        "# Print average metrics for each year\n",
        "print(\"Average Metrics Per Year:\")\n",
        "print(average_metrics_per_year)\n",
        "\n",
        "# Get a color sequence from Plotly's default colors\n",
        "colors = px.colors.qualitative.Plotly\n",
        "\n",
        "# Function to make the color more transparent\n",
        "def get_transparent_color(color, alpha=0.2):\n",
        "    # Convert hex to RGB and then to RGBA\n",
        "    hex_color = color.lstrip('#')\n",
        "    rgb_color = tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n",
        "    return f'rgba({rgb_color[0]}, {rgb_color[1]}, {rgb_color[2]}, {alpha})'\n",
        "\n",
        "# Create a figure\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add the mean line\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=average_metrics_per_year['Year'],\n",
        "    y=average_metrics_per_year['Accuracy_mean'],\n",
        "    mode='lines',\n",
        "    name='Accuracy',\n",
        "    line=dict(color=colors[0], width=2)\n",
        "))\n",
        "\n",
        "# Add the standard deviation shaded area\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=pd.concat([average_metrics_per_year['Year'], average_metrics_per_year['Year'][::-1]]),\n",
        "    y=pd.concat([average_metrics_per_year['Accuracy_mean'] + average_metrics_per_year['Accuracy_std'],\n",
        "                 (average_metrics_per_year['Accuracy_mean'] - average_metrics_per_year['Accuracy_std'])[::-1]]),\n",
        "    fill='toself',\n",
        "    fillcolor=get_transparent_color(colors[0], alpha=0.2),  # Use the same color with transparency\n",
        "    line=dict(color='rgba(255,255,255,0)'),\n",
        "    hoverinfo=\"skip\",\n",
        "    showlegend=False,\n",
        "    name='Accuracy std dev'\n",
        "))\n",
        "\n",
        "# Customize layout\n",
        "fig.update_layout(\n",
        "    title='Average Rolling Window Accuracy Over Time Across All Companies',\n",
        "    xaxis_title='Year',\n",
        "    yaxis_title='Accuracy',\n",
        "    template='plotly_white',\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9x-zw84lh80"
      },
      "source": [
        "## Portofolio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXb0HcKBlh80"
      },
      "outputs": [],
      "source": [
        "def prepare_data(insample_df, outsample_df):\n",
        "    df = pd.concat([insample_df, outsample_df])\n",
        "    df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "    df = df.sort_values(by='Date From')\n",
        "    available_years = df['Date From'].dt.year.unique()\n",
        "    print(\"Years available in the data:\", available_years)\n",
        "    return df\n",
        "\n",
        "def rolling_window_analysis(df):\n",
        "    companies = df['companyname'].unique()\n",
        "    predictions_df = pd.DataFrame()\n",
        "\n",
        "    for company in companies:\n",
        "        company_df = df[df['companyname'] == company].copy()\n",
        "        company_df = company_df.sort_values(by='Date From')\n",
        "\n",
        "        # Define the rolling window parameters\n",
        "        start_year = company_df['Date From'].dt.year.min()\n",
        "        end_year = company_df['Date From'].dt.year.max()\n",
        "        window_size = 10\n",
        "        validation_size = 1\n",
        "\n",
        "        for start in range(start_year, end_year - window_size - validation_size + 1):\n",
        "            train_start = start\n",
        "            train_end = start + window_size\n",
        "            val_start = train_end\n",
        "            val_end = train_end + validation_size\n",
        "\n",
        "            train_df = company_df[(company_df['Date From'].dt.year >= train_start) &\n",
        "                                  (company_df['Date From'].dt.year < train_end)]\n",
        "            test_df = company_df[(company_df['Date From'].dt.year >= val_start) &\n",
        "                                 (company_df['Date From'].dt.year < val_end)]\n",
        "\n",
        "            if len(test_df) == 0 or len(train_df) == 0:\n",
        "                continue\n",
        "\n",
        "            X_train = np.vstack(train_df['embedding'].values)\n",
        "            y_train = train_df['Future Return Direction'].values\n",
        "            X_test = np.vstack(test_df['embedding'].values)\n",
        "            y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "            # Train logistic regression model\n",
        "            logistic_model = LogisticRegression(max_iter=1000)\n",
        "            logistic_model.fit(X_train, y_train)\n",
        "\n",
        "            # Get prediction probabilities\n",
        "            y_prob = logistic_model.predict_proba(X_test)[:, 1]  # Probability of the positive class\n",
        "\n",
        "            if len(y_prob) != len(test_df):\n",
        "                raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_prob)})\")\n",
        "\n",
        "            test_df['predicted_prob'] = y_prob\n",
        "            predictions_df = pd.concat([predictions_df, test_df[['Date From', 'companyname', 'predicted_prob', 'Weekly Compound Return']]], ignore_index=True)\n",
        "\n",
        "    df = df.merge(predictions_df, on=['Date From', 'companyname', 'Weekly Compound Return'], how='left', suffixes=('', '_pred'))\n",
        "    return df\n",
        "\n",
        "def construct_portfolio(df, time_period='Week'):\n",
        "    df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "    if time_period == 'Week':\n",
        "        df['Period'] = df['Date From'].dt.to_period('W').dt.to_timestamp()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid time_period. Use 'Week'.\")\n",
        "\n",
        "    portfolio_returns = []\n",
        "\n",
        "    for period, group in df.groupby('Period'):\n",
        "        if period.year < 2016:\n",
        "            continue\n",
        "\n",
        "        # Sort group by predicted_prob descending\n",
        "        group_sorted = group.sort_values(by='predicted_prob', ascending=False)\n",
        "\n",
        "        # Select top and bottom companies\n",
        "        num_top_companies = 5\n",
        "        num_bottom_companies = 5\n",
        "        top_companies = group_sorted.head(num_top_companies)\n",
        "        bottom_companies = group_sorted.tail(num_bottom_companies)\n",
        "\n",
        "        # Equal-weighted returns\n",
        "        long_return_eq = np.mean(np.log1p(top_companies['Weekly Compound Return']))\n",
        "        short_return_eq = np.mean(np.log1p(bottom_companies['Weekly Compound Return']))\n",
        "        long_short_return_eq = long_return_eq - short_return_eq\n",
        "\n",
        "        # Value-weighted returns\n",
        "        long_return_val = np.sum(np.log1p(top_companies['Weekly Compound Return']) * top_companies['market_cap']) / np.sum(top_companies['market_cap'])\n",
        "        short_return_val = np.sum(np.log1p(bottom_companies['Weekly Compound Return']) * bottom_companies['market_cap']) / np.sum(bottom_companies['market_cap'])\n",
        "        long_short_return_val = long_return_val - short_return_val\n",
        "\n",
        "        portfolio_returns.append({\n",
        "            'Period': period,\n",
        "            'Long Return (Eq)': long_return_eq,\n",
        "            'Short Return (Eq)': short_return_eq,\n",
        "            'Long-Short Return (Eq)': long_short_return_eq,\n",
        "            'Long Return (Val)': long_return_val,\n",
        "            'Short Return (Val)': short_return_val,\n",
        "            'Long-Short Return (Val)': long_short_return_val\n",
        "        })\n",
        "\n",
        "    portfolio_df = pd.DataFrame(portfolio_returns)\n",
        "    portfolio_df['EW L'] = portfolio_df['Long Return (Eq)'].cumsum()\n",
        "    portfolio_df['EW S'] = portfolio_df['Short Return (Eq)'].cumsum()\n",
        "    portfolio_df['EW LS'] = portfolio_df['Long-Short Return (Eq)'].cumsum()\n",
        "    portfolio_df['VW L'] = portfolio_df['Long Return (Val)'].cumsum()\n",
        "    portfolio_df['VW S'] = portfolio_df['Short Return (Val)'].cumsum()\n",
        "    portfolio_df['VW LS'] = portfolio_df['Long-Short Return (Val)'].cumsum()\n",
        "\n",
        "    actual_returns = df[df['Date From'].dt.year >= 2016].groupby('Period')['Weekly Compound Return'].mean()\n",
        "    actual_cumulative_returns = np.log1p(actual_returns).cumsum()\n",
        "    portfolio_df = portfolio_df.merge(actual_cumulative_returns.rename('Market'), on='Period', how='left')\n",
        "\n",
        "    metrics = {}\n",
        "    for portfolio in ['EW L', 'EW S', 'EW LS', 'VW L', 'VW S', 'VW LS']:\n",
        "        returns = portfolio_df[portfolio]\n",
        "\n",
        "        if returns.isnull().all() or returns.eq(0).all():\n",
        "            sharpe_ratio = np.nan\n",
        "            max_drawdown = np.nan\n",
        "            volatility = np.nan\n",
        "        else:\n",
        "            sharpe_ratio = returns.mean() / returns.std() * np.sqrt(52) if returns.std() != 0 else np.nan\n",
        "            cumulative_returns = returns.cumsum()\n",
        "            max_drawdown = (cumulative_returns.cummax() - cumulative_returns).max()\n",
        "            volatility = returns.std() * np.sqrt(52)\n",
        "\n",
        "        metrics[portfolio] = {\n",
        "            'Sharpe Ratio': sharpe_ratio\n",
        "        }\n",
        "\n",
        "        print(f\"Metrics for {portfolio}:\")\n",
        "        print(f\"Sharpe Ratio: {sharpe_ratio}\")\n",
        "        print()\n",
        "\n",
        "    portfolio_df.to_csv('RoBERTa_portfolio_returns.csv', index=False)\n",
        "    print(\"Portfolio returns saved to 'RoBERTa_portfolio_returns.csv'\")\n",
        "    return portfolio_df\n",
        "\n",
        "def plot_portfolio_returns(portfolio_df, title_suffix=''):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['EW L'], marker='o', markersize=1, label='EW L')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['EW S'], marker='o', markersize=1, label='EW S')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['EW LS'], marker='o', markersize=1, label='EW LS')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['VW L'], marker='o', markersize=1, label='VW L')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['VW S'], marker='o', markersize=1, label='VW S')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['VW LS'], marker='o', markersize=1, label='VW LS')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['Market'], marker='o', markersize=1, label='Market')\n",
        "\n",
        "    plt.title(f'Cumulative {title_suffix} Portfolio Returns Over Time')\n",
        "    plt.xlabel('Period')\n",
        "    plt.ylabel('Cumulative Log Return')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage for Weekly\n",
        "df = prepare_data(insample_df, outsample_df)\n",
        "df = rolling_window_analysis(df)\n",
        "\n",
        "# Weekly Portfolio\n",
        "portfolio_df_week = construct_portfolio(df, time_period='Week')\n",
        "portfolio_df_week = portfolio_df_week[portfolio_df_week['Period'].dt.year >= 2016]\n",
        "plot_portfolio_returns(portfolio_df_week, title_suffix='Weekly')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdVhvURvlh80"
      },
      "source": [
        "## Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kvV6oAYlh80"
      },
      "outputs": [],
      "source": [
        "# Suppress SettingWithCopyWarning\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "def prepare_data(insample_df, outsample_df):\n",
        "    df = pd.concat([insample_df, outsample_df])\n",
        "    df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "    df = df.sort_values(by='Date From')\n",
        "    available_years = df['Date From'].dt.year.unique()\n",
        "    print(\"Years available in the data:\", available_years)\n",
        "    return df\n",
        "\n",
        "def rolling_window_analysis(df):\n",
        "    companies = df['companyname'].unique()\n",
        "    predictions_df = pd.DataFrame()\n",
        "\n",
        "    for company in companies:\n",
        "        company_df = df[df['companyname'] == company].copy()\n",
        "        company_df = company_df.sort_values(by='Date From')\n",
        "\n",
        "        start_year = company_df['Date From'].dt.year.min()\n",
        "        end_year = company_df['Date From'].dt.year.max()\n",
        "        window_size = 10\n",
        "        validation_size = 1\n",
        "\n",
        "        for start in range(start_year, end_year - window_size - validation_size + 1):\n",
        "            train_start = start\n",
        "            train_end = start + window_size\n",
        "            val_start = train_end\n",
        "            val_end = train_end + validation_size\n",
        "\n",
        "            train_df = company_df[(company_df['Date From'].dt.year >= train_start) &\n",
        "                                  (company_df['Date From'].dt.year < train_end)]\n",
        "            test_df = company_df[(company_df['Date From'].dt.year >= val_start) &\n",
        "                                 (company_df['Date From'].dt.year < val_end)]\n",
        "\n",
        "            if len(test_df) == 0 or len(train_df) == 0:\n",
        "                continue\n",
        "\n",
        "            X_train = np.vstack(train_df['embedding'].values)\n",
        "            y_train = train_df['Future Return Direction'].values\n",
        "            X_test = np.vstack(test_df['embedding'].values)\n",
        "            y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "            logistic_model = LogisticRegression(max_iter=1000)\n",
        "            logistic_model.fit(X_train, y_train)\n",
        "\n",
        "            y_prob = logistic_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "            if len(y_prob) != len(test_df):\n",
        "                raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_prob)})\")\n",
        "\n",
        "            test_df['predicted_prob'] = y_prob\n",
        "            predictions_df = pd.concat([predictions_df, test_df[['Date From', 'companyname', 'predicted_prob', 'Weekly Compound Return', 'market_cap']]], ignore_index=True)\n",
        "\n",
        "    df = df.merge(predictions_df, on=['Date From', 'companyname', 'Weekly Compound Return', 'market_cap'], how='left', suffixes=('', '_pred'))\n",
        "    return df\n",
        "\n",
        "def calculate_transaction_costs(df):\n",
        "    np.random.seed(None)  # Ensure we're not using a fixed seed\n",
        "\n",
        "    median_market_cap = df['market_cap'].median()\n",
        "    df['is_large_cap'] = df['market_cap'] > median_market_cap\n",
        "\n",
        "    # Convert transaction costs to basis points\n",
        "    df['transaction_cost'] = np.where(df['is_large_cap'],\n",
        "                                      np.random.normal(10.25, 2.05, df.shape[0]),\n",
        "                                      np.random.normal(21, 4.2, df.shape[0]))\n",
        "\n",
        "    # Ensure no negative transaction costs\n",
        "    df['transaction_cost'] = np.maximum(df['transaction_cost'], 0)\n",
        "\n",
        "    print(f\"Average cost for large-cap stocks: {df[df['is_large_cap']]['transaction_cost'].mean():.2f} bps\")\n",
        "    print(f\"Average cost for small-cap stocks: {df[~df['is_large_cap']]['transaction_cost'].mean():.2f} bps\")\n",
        "\n",
        "    print(f\"\\nLarge-cap cost range: {df[df['is_large_cap']]['transaction_cost'].min():.2f} bps to {df[df['is_large_cap']]['transaction_cost'].max():.2f} bps\")\n",
        "    print(f\"Small-cap cost range: {df[~df['is_large_cap']]['transaction_cost'].min():.2f} bps to {df[~df['is_large_cap']]['transaction_cost'].max():.2f} bps\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def construct_portfolio_with_costs(df, time_period='Week'):\n",
        "    df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "    if time_period == 'Week':\n",
        "        df['Period'] = df['Date From'].dt.to_period('W').dt.to_timestamp()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid time_period. Use 'Week'.\")\n",
        "\n",
        "    portfolio_returns = []\n",
        "\n",
        "    for period, group in df.groupby('Period'):\n",
        "        if period.year < 2016:\n",
        "            continue\n",
        "\n",
        "        group_sorted = group.sort_values(by='predicted_prob', ascending=False)\n",
        "\n",
        "        num_top_companies = 5\n",
        "        num_bottom_companies = 5\n",
        "        top_companies = group_sorted.head(num_top_companies)\n",
        "        bottom_companies = group_sorted.tail(num_bottom_companies)\n",
        "\n",
        "        def calculate_return_with_costs(companies, long_position=True):\n",
        "            returns = np.log1p(companies['Weekly Compound Return'])\n",
        "            costs = companies['transaction_cost'] / 10000  # Convert bps to decimal\n",
        "            if long_position:\n",
        "                return returns - costs\n",
        "            else:\n",
        "                return -returns - costs\n",
        "\n",
        "        long_return_eq = np.mean(calculate_return_with_costs(top_companies, long_position=True))\n",
        "        short_return_eq = np.mean(calculate_return_with_costs(bottom_companies, long_position=False))\n",
        "        long_short_return_eq = long_return_eq - short_return_eq\n",
        "\n",
        "        total_market_cap_long = np.sum(top_companies['market_cap'])\n",
        "        total_market_cap_short = np.sum(bottom_companies['market_cap'])\n",
        "\n",
        "        long_return_val = np.sum(calculate_return_with_costs(top_companies, long_position=True) * top_companies['market_cap']) / total_market_cap_long\n",
        "        short_return_val = np.sum(calculate_return_with_costs(bottom_companies, long_position=False) * bottom_companies['market_cap']) / total_market_cap_short\n",
        "        long_short_return_val = long_return_val - short_return_val\n",
        "\n",
        "        portfolio_returns.append({\n",
        "            'Period': period,\n",
        "            'Long Return (Eq)': long_return_eq,\n",
        "            'Short Return (Eq)': short_return_eq,\n",
        "            'Long-Short Return (Eq)': long_short_return_eq,\n",
        "            'Long Return (Val)': long_return_val,\n",
        "            'Short Return (Val)': short_return_val,\n",
        "            'Long-Short Return (Val)': long_short_return_val\n",
        "        })\n",
        "\n",
        "    portfolio_df = pd.DataFrame(portfolio_returns)\n",
        "    portfolio_df['EW L'] = portfolio_df['Long Return (Eq)'].cumsum()\n",
        "    portfolio_df['EW S'] = portfolio_df['Short Return (Eq)'].cumsum()\n",
        "    portfolio_df['EW LS'] = portfolio_df['Long-Short Return (Eq)'].cumsum()\n",
        "    portfolio_df['VW L'] = portfolio_df['Long Return (Val)'].cumsum()\n",
        "    portfolio_df['VW S'] = portfolio_df['Short Return (Val)'].cumsum()\n",
        "    portfolio_df['VW LS'] = portfolio_df['Long-Short Return (Val)'].cumsum()\n",
        "\n",
        "    actual_returns = df[df['Date From'].dt.year >= 2016].groupby('Period')['Weekly Compound Return'].mean()\n",
        "    actual_cumulative_returns = np.log1p(actual_returns).cumsum()\n",
        "    portfolio_df = portfolio_df.merge(actual_cumulative_returns.rename('Market'), on='Period', how='left')\n",
        "\n",
        "    return portfolio_df\n",
        "\n",
        "# Main execution\n",
        "df = prepare_data(insample_df, outsample_df)\n",
        "df = rolling_window_analysis(df)\n",
        "df = calculate_transaction_costs(df)\n",
        "\n",
        "# Weekly Portfolio with transaction costs\n",
        "portfolio_df_week_with_costs = construct_portfolio_with_costs(df, time_period='Week')\n",
        "portfolio_df_week_with_costs = portfolio_df_week_with_costs[portfolio_df_week_with_costs['Period'].dt.year >= 2016]\n",
        "\n",
        "print(\"Portfolio construction with transaction costs completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oW0xdD-lh81"
      },
      "source": [
        "# Distil BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf23hFrxlh81"
      },
      "source": [
        "## Load Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEQPxH2_lh81"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained DistilBERT model and tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Load embedded Dataframe\n",
        "with open('embedding-DistilBERT-AllCompany-NEW.pkl', 'rb') as f:\n",
        "    insample_df, outsample_df = pd.read_pickle(f)\n",
        "\n",
        "# Display the first 2 rows of outsample_df to check\n",
        "filtered_df = outsample_df[outsample_df['headline'] != '[No_Headline]']\n",
        "filtered_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KLlsRFilh81"
      },
      "source": [
        "## Accuracy per-companies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pQs78Aolh81"
      },
      "outputs": [],
      "source": [
        "# Combine insample and outsample data for rolling window\n",
        "df = pd.concat([insample_df, outsample_df])\n",
        "\n",
        "# Convert 'Date From' to datetime\n",
        "df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "\n",
        "# Sort by date\n",
        "df = df.sort_values(by='Date From')\n",
        "\n",
        "# Check available years\n",
        "available_years = df['Date From'].dt.year.unique()\n",
        "print(\"Years available in the data:\", available_years)\n",
        "\n",
        "# Define rolling window parameters\n",
        "window_size = 365 * 10  # 10 years in days\n",
        "prediction_period = 365  # 1 year in days\n",
        "\n",
        "# Get unique companies\n",
        "companies = df['companyname'].unique()\n",
        "\n",
        "# Dictionary to store DataFrames for each company\n",
        "company_results_dfs = {}\n",
        "\n",
        "# Rolling window analysis for each company\n",
        "for company in companies:\n",
        "    company_df = df[df['companyname'] == company].copy()\n",
        "\n",
        "    accuracies = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "    years = []\n",
        "\n",
        "    # Setup time series cross-validation\n",
        "    tscv = TimeSeriesSplit(n_splits=5)  # You can adjust the number of splits as needed\n",
        "\n",
        "    for train_index, test_index in tscv.split(company_df):\n",
        "        train_df, test_df = company_df.iloc[train_index], company_df.iloc[test_index]\n",
        "\n",
        "        if len(test_df) == 0 or len(train_df) == 0:\n",
        "            continue\n",
        "\n",
        "        X_train = np.vstack(train_df['embedding'].values)\n",
        "        y_train = train_df['Future Return Direction'].values\n",
        "\n",
        "        X_test = np.vstack(test_df['embedding'].values)\n",
        "        y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "        # Train logistic regression model\n",
        "        logistic_model = LogisticRegression(max_iter=1000)\n",
        "        logistic_model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = logistic_model.predict(X_test)\n",
        "\n",
        "        # Ensure the length of predictions and test_df matches\n",
        "        if len(y_pred) != len(test_df):\n",
        "            raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_pred)})\")\n",
        "\n",
        "        # Store the predictions and true values\n",
        "        test_df['predicted'] = y_pred\n",
        "\n",
        "        # Update the company-specific DataFrame with predictions\n",
        "        company_df.loc[test_df.index, 'predicted'] = test_df['predicted']\n",
        "\n",
        "        print(f'Company: {company}')\n",
        "        print(f'Training window: {train_df[\"Date From\"].min()} to {train_df[\"Date From\"].max()}')\n",
        "        print(f'Test window: {test_df[\"Date From\"].min()} to {test_df[\"Date From\"].max()}')\n",
        "        print(f'Predicted years: {test_df[\"Date From\"].dt.year.unique()}')\n",
        "\n",
        "    # Evaluate the Model for the company\n",
        "    for year in range(2016, 2024):\n",
        "        year_df = company_df[company_df['Date From'].dt.year == year]\n",
        "\n",
        "        if 'predicted' not in year_df.columns or year_df['predicted'].isnull().all():\n",
        "            print(f\"Missing predictions for year {year} for company {company} due to insufficient data or missing predictions.\")\n",
        "            continue\n",
        "\n",
        "        y_test = year_df['Future Return Direction'].values\n",
        "        y_pred = year_df['predicted'].values\n",
        "\n",
        "        # Remove NaN values in predictions\n",
        "        valid_indices = ~np.isnan(y_pred)\n",
        "        y_test = y_test[valid_indices]\n",
        "        y_pred = y_pred[valid_indices]\n",
        "\n",
        "        if len(y_pred) == 0 or len(y_test) == 0:\n",
        "            print(f\"Insufficient valid predictions for year {year} for company {company}.\")\n",
        "            continue\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "        recall = recall_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "        f1 = f1_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "\n",
        "        accuracies.append(accuracy)\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "        years.append(year)\n",
        "\n",
        "        print(f'Company: {company}, Year: {year}')\n",
        "        print(f'Accuracy: {accuracy:.2f}')\n",
        "        print(f'Precision: {precision:.2f}')\n",
        "        print(f'Recall: {recall:.2f}')\n",
        "        print(f'F1 Score: {f1:.2f}')\n",
        "        print()\n",
        "\n",
        "    # Store the results for the company in a DataFrame\n",
        "    results_df = pd.DataFrame({\n",
        "        'Year': years,\n",
        "        'Accuracy': accuracies,\n",
        "        'Precision': precisions,\n",
        "        'Recall': recalls,\n",
        "        'F1 Score': f1_scores\n",
        "    })\n",
        "\n",
        "    # Save the DataFrame in the dictionary\n",
        "    company_results_dfs[company] = results_df\n",
        "\n",
        "    print(f\"Evaluation results for {company}:\")\n",
        "    print(results_df)\n",
        "\n",
        "    # Plot accuracy over time for the company\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(results_df['Year'], results_df['Accuracy'], marker='o', label='Accuracy')\n",
        "    plt.plot(results_df['Year'], results_df['Precision'], marker='o', label='Precision')\n",
        "    plt.plot(results_df['Year'], results_df['Recall'], marker='o', label='Recall')\n",
        "    plt.plot(results_df['Year'], results_df['F1 Score'], marker='o', label='F1 Score')\n",
        "\n",
        "    plt.title(f'Performance Metrics Over Time for {company}')\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot confusion matrices for each year\n",
        "    n_plots = len(years)\n",
        "    n_cols = 4\n",
        "    n_rows = (n_plots // n_cols) + (n_plots % n_cols > 0)\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 4 * n_rows))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, year in enumerate(years):\n",
        "        year_df = company_df[company_df['Date From'].dt.year == year]\n",
        "        y_test = year_df['Future Return Direction'].values\n",
        "        y_pred = year_df['predicted'].values\n",
        "\n",
        "        # Remove NaN values in predictions\n",
        "        valid_indices = ~np.isnan(y_pred)\n",
        "        y_test = y_test[valid_indices]\n",
        "        y_pred = y_pred[valid_indices]\n",
        "\n",
        "        if len(y_pred) > 0 and len(y_test) > 0:\n",
        "            cm = confusion_matrix(y_test, y_pred)\n",
        "            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logistic_model.classes_)\n",
        "            disp.plot(cmap='Blues', ax=axes[i])\n",
        "            axes[i].set_title(f'{company} {year}')\n",
        "\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Access the stored DataFrames\n",
        "for company, results_df in company_results_dfs.items():\n",
        "    print(f\"\\nResults for {company}:\")\n",
        "    print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_Cxiisylh81"
      },
      "outputs": [],
      "source": [
        "# Concatenate all the company results into one DataFrame\n",
        "all_results_df = pd.concat(company_results_dfs.values(), keys=company_results_dfs.keys()).reset_index(level=0).rename(columns={'level_0': 'Company'})\n",
        "\n",
        "# Save the combined DataFrame to a CSV file\n",
        "all_results_df.to_csv('[EVAL] DistilBERT_all_company_results.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaDI_AN9lh81"
      },
      "source": [
        "## Accuracy all years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJC-lIJJlh81"
      },
      "outputs": [],
      "source": [
        "# Combine insample and outsample data for rolling window\n",
        "df = pd.concat([insample_df, outsample_df])\n",
        "\n",
        "# Convert 'Date From' to datetime\n",
        "df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "\n",
        "# Sort by date\n",
        "df = df.sort_values(by='Date From')\n",
        "\n",
        "# Check available years\n",
        "available_years = df['Date From'].dt.year.unique()\n",
        "print(\"Years available in the data:\", available_years)\n",
        "\n",
        "# Define rolling window parameters\n",
        "window_size = 365 * 10  # 10 years in days\n",
        "prediction_period = 365  # 1 year in days\n",
        "\n",
        "# Get unique companies\n",
        "companies = df['companyname'].unique()\n",
        "\n",
        "# Dictionary to store DataFrames for each company\n",
        "company_results_dfs = {}\n",
        "\n",
        "# Rolling window analysis for each company\n",
        "for company in companies:\n",
        "    company_df = df[df['companyname'] == company].copy()\n",
        "\n",
        "    accuracies = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "    years = []\n",
        "\n",
        "    # Setup time series cross-validation\n",
        "    tscv = TimeSeriesSplit(n_splits=5)  # You can adjust the number of splits as needed\n",
        "\n",
        "    for train_index, test_index in tscv.split(company_df):\n",
        "        train_df, test_df = company_df.iloc[train_index], company_df.iloc[test_index]\n",
        "\n",
        "        if len(test_df) == 0 or len(train_df) == 0:\n",
        "            continue\n",
        "\n",
        "        X_train = np.vstack(train_df['embedding'].values)\n",
        "        y_train = train_df['Future Return Direction'].values\n",
        "\n",
        "        X_test = np.vstack(test_df['embedding'].values)\n",
        "        y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "        # Train logistic regression model\n",
        "        logistic_model = LogisticRegression(max_iter=1000)\n",
        "        logistic_model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = logistic_model.predict(X_test)\n",
        "\n",
        "        # Ensure the length of predictions and test_df matches\n",
        "        if len(y_pred) != len(test_df):\n",
        "            raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_pred)})\")\n",
        "\n",
        "        # Store the predictions and true values\n",
        "        test_df['predicted'] = y_pred\n",
        "\n",
        "        # Update the company-specific DataFrame with predictions\n",
        "        company_df.loc[test_df.index, 'predicted'] = test_df['predicted']\n",
        "\n",
        "    # Evaluate the Model for the company\n",
        "    for year in range(2016, 2024):\n",
        "        year_df = company_df[company_df['Date From'].dt.year == year]\n",
        "\n",
        "        if 'predicted' not in year_df.columns or year_df['predicted'].isnull().all():\n",
        "            continue\n",
        "\n",
        "        y_test = year_df['Future Return Direction'].values\n",
        "        y_pred = year_df['predicted'].values\n",
        "\n",
        "        # Remove NaN values in predictions\n",
        "        valid_indices = ~np.isnan(y_pred)\n",
        "        y_test = y_test[valid_indices]\n",
        "        y_pred = y_pred[valid_indices]\n",
        "\n",
        "        if len(y_pred) == 0 or len(y_test) == 0:\n",
        "            continue\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "        recall = recall_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "        f1 = f1_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "\n",
        "        accuracies.append(accuracy)\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "        years.append(year)\n",
        "\n",
        "    # Store the results for the company in a DataFrame\n",
        "    results_df = pd.DataFrame({\n",
        "        'Year': years,\n",
        "        'Accuracy': accuracies,\n",
        "        'Precision': precisions,\n",
        "        'Recall': recalls,\n",
        "        'F1 Score': f1_scores\n",
        "    })\n",
        "\n",
        "    # Save the DataFrame in the dictionary\n",
        "    company_results_dfs[company] = results_df\n",
        "\n",
        "# Combine results of all companies into a single DataFrame\n",
        "combined_results = pd.concat(company_results_dfs.values(), ignore_index=True)\n",
        "\n",
        "# Calculate average accuracy per year across all companies\n",
        "average_metrics_per_year = combined_results.groupby('Year').mean().reset_index()\n",
        "\n",
        "# Print average metrics for each year\n",
        "print(\"Average Metrics Per Year:\")\n",
        "print(average_metrics_per_year)\n",
        "\n",
        "# Plot average accuracy per year\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "plt.plot(average_metrics_per_year['Year'], average_metrics_per_year['Accuracy'], marker='o', label='Accuracy')\n",
        "plt.plot(average_metrics_per_year['Year'], average_metrics_per_year['Precision'], marker='o', label='Precision')\n",
        "plt.plot(average_metrics_per_year['Year'], average_metrics_per_year['Recall'], marker='o', label='Recall')\n",
        "plt.plot(average_metrics_per_year['Year'], average_metrics_per_year['F1 Score'], marker='o', label='F1 Score')\n",
        "\n",
        "plt.title('Average Performance Metrics Over Time Across All Companies')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjVJlGVFlh81"
      },
      "source": [
        "## Standard Deviation all years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rwlO_LIlh85"
      },
      "outputs": [],
      "source": [
        "# Combine insample and outsample data for rolling window\n",
        "df = pd.concat([insample_df, outsample_df])\n",
        "\n",
        "# Convert 'Date From' to datetime\n",
        "df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "\n",
        "# Sort by date\n",
        "df = df.sort_values(by='Date From')\n",
        "\n",
        "# Check available years\n",
        "available_years = df['Date From'].dt.year.unique()\n",
        "print(\"Years available in the data:\", available_years)\n",
        "\n",
        "# Get unique companies\n",
        "companies = df['companyname'].unique()\n",
        "\n",
        "# Dictionary to store DataFrames for each company\n",
        "company_results_dfs = {}\n",
        "\n",
        "# Rolling window analysis for each company\n",
        "for company in companies:\n",
        "    company_df = df[df['companyname'] == company].copy()\n",
        "\n",
        "    accuracies = []\n",
        "    years = []\n",
        "\n",
        "    # Setup time series cross-validation\n",
        "    tscv = TimeSeriesSplit(n_splits=5)  # You can adjust the number of splits as needed\n",
        "\n",
        "    for train_index, test_index in tscv.split(company_df):\n",
        "        train_df, test_df = company_df.iloc[train_index], company_df.iloc[test_index]\n",
        "\n",
        "        if len(test_df) == 0 or len(train_df) == 0:\n",
        "            continue\n",
        "\n",
        "        X_train = np.vstack(train_df['embedding'].values)\n",
        "        y_train = train_df['Future Return Direction'].values\n",
        "\n",
        "        X_test = np.vstack(test_df['embedding'].values)\n",
        "        y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "        # Train logistic regression model\n",
        "        logistic_model = LogisticRegression(max_iter=1000)\n",
        "        logistic_model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = logistic_model.predict(X_test)\n",
        "\n",
        "        # Ensure the length of predictions and test_df matches\n",
        "        if len(y_pred) != len(test_df):\n",
        "            raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_pred)})\")\n",
        "\n",
        "        # Store the predictions and true values\n",
        "        test_df['predicted'] = y_pred\n",
        "\n",
        "        # Update the company-specific DataFrame with predictions\n",
        "        company_df.loc[test_df.index, 'predicted'] = test_df['predicted']\n",
        "\n",
        "    # Evaluate the Model for the company\n",
        "    for year in range(2016, 2024):\n",
        "        year_df = company_df[company_df['Date From'].dt.year == year]\n",
        "\n",
        "        if 'predicted' not in year_df.columns or year_df['predicted'].isnull().all():\n",
        "            continue\n",
        "\n",
        "        y_test = year_df['Future Return Direction'].values\n",
        "        y_pred = year_df['predicted'].values\n",
        "\n",
        "        # Remove NaN values in predictions\n",
        "        valid_indices = ~np.isnan(y_pred)\n",
        "        y_test = y_test[valid_indices]\n",
        "        y_pred = y_pred[valid_indices]\n",
        "\n",
        "        if len(y_pred) == 0 or len(y_test) == 0:\n",
        "            continue\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        accuracies.append(accuracy)\n",
        "        years.append(year)\n",
        "\n",
        "    # Store the results for the company in a DataFrame\n",
        "    results_df = pd.DataFrame({\n",
        "        'Year': years,\n",
        "        'Accuracy': accuracies\n",
        "    })\n",
        "\n",
        "    # Save the DataFrame in the dictionary\n",
        "    company_results_dfs[company] = results_df\n",
        "\n",
        "# Combine results of all companies into a single DataFrame\n",
        "combined_results = pd.concat(company_results_dfs.values(), ignore_index=True)\n",
        "\n",
        "# Calculate average accuracy per year across all companies\n",
        "average_metrics_per_year = combined_results.groupby('Year').agg(\n",
        "    Accuracy_mean=('Accuracy', 'mean'),\n",
        "    Accuracy_std=('Accuracy', 'std')\n",
        ").reset_index()\n",
        "\n",
        "# Print average metrics for each year\n",
        "print(\"Average Metrics Per Year:\")\n",
        "print(average_metrics_per_year)\n",
        "\n",
        "# Get a color sequence from Plotly's default colors\n",
        "colors = px.colors.qualitative.Plotly\n",
        "\n",
        "# Function to make the color more transparent\n",
        "def get_transparent_color(color, alpha=0.2):\n",
        "    # Convert hex to RGB and then to RGBA\n",
        "    hex_color = color.lstrip('#')\n",
        "    rgb_color = tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n",
        "    return f'rgba({rgb_color[0]}, {rgb_color[1]}, {rgb_color[2]}, {alpha})'\n",
        "\n",
        "# Create a figure\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add the mean line\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=average_metrics_per_year['Year'],\n",
        "    y=average_metrics_per_year['Accuracy_mean'],\n",
        "    mode='lines',\n",
        "    name='Accuracy',\n",
        "    line=dict(color=colors[0], width=2)\n",
        "))\n",
        "\n",
        "# Add the standard deviation shaded area\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=pd.concat([average_metrics_per_year['Year'], average_metrics_per_year['Year'][::-1]]),\n",
        "    y=pd.concat([average_metrics_per_year['Accuracy_mean'] + average_metrics_per_year['Accuracy_std'],\n",
        "                 (average_metrics_per_year['Accuracy_mean'] - average_metrics_per_year['Accuracy_std'])[::-1]]),\n",
        "    fill='toself',\n",
        "    fillcolor=get_transparent_color(colors[0], alpha=0.2),  # Use the same color with transparency\n",
        "    line=dict(color='rgba(255,255,255,0)'),\n",
        "    hoverinfo=\"skip\",\n",
        "    showlegend=False,\n",
        "    name='Accuracy std dev'\n",
        "))\n",
        "\n",
        "# Customize layout\n",
        "fig.update_layout(\n",
        "    title='Average Rolling Window Accuracy Over Time Across All Companies',\n",
        "    xaxis_title='Year',\n",
        "    yaxis_title='Accuracy',\n",
        "    template='plotly_white',\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiyPcx-0lh86"
      },
      "source": [
        "## Portfolio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y24s6ZH7lh86"
      },
      "outputs": [],
      "source": [
        "def prepare_data(insample_df, outsample_df):\n",
        "    df = pd.concat([insample_df, outsample_df])\n",
        "    df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "    df = df.sort_values(by='Date From')\n",
        "    available_years = df['Date From'].dt.year.unique()\n",
        "    print(\"Years available in the data:\", available_years)\n",
        "    return df\n",
        "\n",
        "def rolling_window_analysis(df):\n",
        "    companies = df['companyname'].unique()\n",
        "    predictions_df = pd.DataFrame()\n",
        "\n",
        "    for company in companies:\n",
        "        company_df = df[df['companyname'] == company].copy()\n",
        "        company_df = company_df.sort_values(by='Date From')\n",
        "\n",
        "        # Define the rolling window parameters\n",
        "        start_year = company_df['Date From'].dt.year.min()\n",
        "        end_year = company_df['Date From'].dt.year.max()\n",
        "        window_size = 10\n",
        "        validation_size = 1\n",
        "\n",
        "        for start in range(start_year, end_year - window_size - validation_size + 1):\n",
        "            train_start = start\n",
        "            train_end = start + window_size\n",
        "            val_start = train_end\n",
        "            val_end = train_end + validation_size\n",
        "\n",
        "            train_df = company_df[(company_df['Date From'].dt.year >= train_start) &\n",
        "                                  (company_df['Date From'].dt.year < train_end)]\n",
        "            test_df = company_df[(company_df['Date From'].dt.year >= val_start) &\n",
        "                                 (company_df['Date From'].dt.year < val_end)]\n",
        "\n",
        "            if len(test_df) == 0 or len(train_df) == 0:\n",
        "                continue\n",
        "\n",
        "            X_train = np.vstack(train_df['embedding'].values)\n",
        "            y_train = train_df['Future Return Direction'].values\n",
        "            X_test = np.vstack(test_df['embedding'].values)\n",
        "            y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "            # Train logistic regression model\n",
        "            logistic_model = LogisticRegression(max_iter=1000)\n",
        "            logistic_model.fit(X_train, y_train)\n",
        "\n",
        "            # Get prediction probabilities\n",
        "            y_prob = logistic_model.predict_proba(X_test)[:, 1]  # Probability of the positive class\n",
        "\n",
        "            if len(y_prob) != len(test_df):\n",
        "                raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_prob)})\")\n",
        "\n",
        "            test_df['predicted_prob'] = y_prob\n",
        "            predictions_df = pd.concat([predictions_df, test_df[['Date From', 'companyname', 'predicted_prob', 'Weekly Compound Return']]], ignore_index=True)\n",
        "\n",
        "    df = df.merge(predictions_df, on=['Date From', 'companyname', 'Weekly Compound Return'], how='left', suffixes=('', '_pred'))\n",
        "    return df\n",
        "\n",
        "def construct_portfolio(df, time_period='Week'):\n",
        "    df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "    if time_period == 'Week':\n",
        "        df['Period'] = df['Date From'].dt.to_period('W').dt.to_timestamp()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid time_period. Use 'Week'.\")\n",
        "\n",
        "    portfolio_returns = []\n",
        "\n",
        "    for period, group in df.groupby('Period'):\n",
        "        if period.year < 2016:\n",
        "            continue\n",
        "\n",
        "        # Sort group by predicted_prob descending\n",
        "        group_sorted = group.sort_values(by='predicted_prob', ascending=False)\n",
        "\n",
        "        # Select top and bottom companies\n",
        "        num_top_companies = 5\n",
        "        num_bottom_companies = 5\n",
        "        top_companies = group_sorted.head(num_top_companies)\n",
        "        bottom_companies = group_sorted.tail(num_bottom_companies)\n",
        "\n",
        "        # Equal-weighted returns\n",
        "        long_return_eq = np.mean(np.log1p(top_companies['Weekly Compound Return']))\n",
        "        short_return_eq = np.mean(np.log1p(bottom_companies['Weekly Compound Return']))\n",
        "        long_short_return_eq = long_return_eq - short_return_eq\n",
        "\n",
        "        # Value-weighted returns\n",
        "        long_return_val = np.sum(np.log1p(top_companies['Weekly Compound Return']) * top_companies['market_cap']) / np.sum(top_companies['market_cap'])\n",
        "        short_return_val = np.sum(np.log1p(bottom_companies['Weekly Compound Return']) * bottom_companies['market_cap']) / np.sum(bottom_companies['market_cap'])\n",
        "        long_short_return_val = long_return_val - short_return_val\n",
        "\n",
        "        portfolio_returns.append({\n",
        "            'Period': period,\n",
        "            'Long Return (Eq)': long_return_eq,\n",
        "            'Short Return (Eq)': short_return_eq,\n",
        "            'Long-Short Return (Eq)': long_short_return_eq,\n",
        "            'Long Return (Val)': long_return_val,\n",
        "            'Short Return (Val)': short_return_val,\n",
        "            'Long-Short Return (Val)': long_short_return_val\n",
        "        })\n",
        "\n",
        "    portfolio_df = pd.DataFrame(portfolio_returns)\n",
        "    portfolio_df['EW L'] = portfolio_df['Long Return (Eq)'].cumsum()\n",
        "    portfolio_df['EW S'] = portfolio_df['Short Return (Eq)'].cumsum()\n",
        "    portfolio_df['EW LS'] = portfolio_df['Long-Short Return (Eq)'].cumsum()\n",
        "    portfolio_df['VW L'] = portfolio_df['Long Return (Val)'].cumsum()\n",
        "    portfolio_df['VW S'] = portfolio_df['Short Return (Val)'].cumsum()\n",
        "    portfolio_df['VW LS'] = portfolio_df['Long-Short Return (Val)'].cumsum()\n",
        "\n",
        "    actual_returns = df[df['Date From'].dt.year >= 2016].groupby('Period')['Weekly Compound Return'].mean()\n",
        "    actual_cumulative_returns = np.log1p(actual_returns).cumsum()\n",
        "    portfolio_df = portfolio_df.merge(actual_cumulative_returns.rename('Market'), on='Period', how='left')\n",
        "\n",
        "    metrics = {}\n",
        "    for portfolio in ['EW L', 'EW S', 'EW LS', 'VW L', 'VW S', 'VW LS']:\n",
        "        returns = portfolio_df[portfolio]\n",
        "\n",
        "        if returns.isnull().all() or returns.eq(0).all():\n",
        "            sharpe_ratio = np.nan\n",
        "            max_drawdown = np.nan\n",
        "            volatility = np.nan\n",
        "        else:\n",
        "            sharpe_ratio = returns.mean() / returns.std() * np.sqrt(52) if returns.std() != 0 else np.nan\n",
        "            cumulative_returns = returns.cumsum()\n",
        "            max_drawdown = (cumulative_returns.cummax() - cumulative_returns).max()\n",
        "            volatility = returns.std() * np.sqrt(52)\n",
        "\n",
        "        metrics[portfolio] = {\n",
        "            'Sharpe Ratio': sharpe_ratio\n",
        "        }\n",
        "\n",
        "        print(f\"Metrics for {portfolio}:\")\n",
        "        print(f\"Sharpe Ratio: {sharpe_ratio}\")\n",
        "        print()\n",
        "\n",
        "    portfolio_df.to_csv('DistilRoBERTa_portfolio_returns.csv', index=False)\n",
        "    print(\"Portfolio returns saved to 'DistilRoBERTa_portfolio_returns.csv'\")\n",
        "    return portfolio_df\n",
        "\n",
        "def plot_portfolio_returns(portfolio_df, title_suffix=''):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['EW L'], marker='o', markersize=1, label='EW L')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['EW S'], marker='o', markersize=1, label='EW S')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['EW LS'], marker='o', markersize=1, label='EW LS')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['VW L'], marker='o', markersize=1, label='VW L')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['VW S'], marker='o', markersize=1, label='VW S')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['VW LS'], marker='o', markersize=1, label='VW LS')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['Market'], marker='o', markersize=1, label='Market')\n",
        "\n",
        "    plt.title(f'Cumulative {title_suffix} Portfolio Returns Over Time')\n",
        "    plt.xlabel('Period')\n",
        "    plt.ylabel('Cumulative Log Return')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage for Weekly\n",
        "df = prepare_data(insample_df, outsample_df)\n",
        "df = rolling_window_analysis(df)\n",
        "\n",
        "# Weekly Portfolio\n",
        "portfolio_df_week = construct_portfolio(df, time_period='Week')\n",
        "portfolio_df_week = portfolio_df_week[portfolio_df_week['Period'].dt.year >= 2016]\n",
        "plot_portfolio_returns(portfolio_df_week, title_suffix='Weekly')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynZEWZPmlh86"
      },
      "source": [
        "## Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sInPmQsYlh86"
      },
      "outputs": [],
      "source": [
        "# Suppress SettingWithCopyWarning\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "def prepare_data(insample_df, outsample_df):\n",
        "    df = pd.concat([insample_df, outsample_df])\n",
        "    df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "    df = df.sort_values(by='Date From')\n",
        "    available_years = df['Date From'].dt.year.unique()\n",
        "    print(\"Years available in the data:\", available_years)\n",
        "    return df\n",
        "\n",
        "def rolling_window_analysis(df):\n",
        "    companies = df['companyname'].unique()\n",
        "    predictions_df = pd.DataFrame()\n",
        "\n",
        "    for company in companies:\n",
        "        company_df = df[df['companyname'] == company].copy()\n",
        "        company_df = company_df.sort_values(by='Date From')\n",
        "\n",
        "        start_year = company_df['Date From'].dt.year.min()\n",
        "        end_year = company_df['Date From'].dt.year.max()\n",
        "        window_size = 10\n",
        "        validation_size = 1\n",
        "\n",
        "        for start in range(start_year, end_year - window_size - validation_size + 1):\n",
        "            train_start = start\n",
        "            train_end = start + window_size\n",
        "            val_start = train_end\n",
        "            val_end = train_end + validation_size\n",
        "\n",
        "            train_df = company_df[(company_df['Date From'].dt.year >= train_start) &\n",
        "                                  (company_df['Date From'].dt.year < train_end)]\n",
        "            test_df = company_df[(company_df['Date From'].dt.year >= val_start) &\n",
        "                                 (company_df['Date From'].dt.year < val_end)]\n",
        "\n",
        "            if len(test_df) == 0 or len(train_df) == 0:\n",
        "                continue\n",
        "\n",
        "            X_train = np.vstack(train_df['embedding'].values)\n",
        "            y_train = train_df['Future Return Direction'].values\n",
        "            X_test = np.vstack(test_df['embedding'].values)\n",
        "            y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "            logistic_model = LogisticRegression(max_iter=1000)\n",
        "            logistic_model.fit(X_train, y_train)\n",
        "\n",
        "            y_prob = logistic_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "            if len(y_prob) != len(test_df):\n",
        "                raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_prob)})\")\n",
        "\n",
        "            test_df['predicted_prob'] = y_prob\n",
        "            predictions_df = pd.concat([predictions_df, test_df[['Date From', 'companyname', 'predicted_prob', 'Weekly Compound Return', 'market_cap']]], ignore_index=True)\n",
        "\n",
        "    df = df.merge(predictions_df, on=['Date From', 'companyname', 'Weekly Compound Return', 'market_cap'], how='left', suffixes=('', '_pred'))\n",
        "    return df\n",
        "\n",
        "def calculate_transaction_costs(df):\n",
        "    np.random.seed(None)  # Ensure we're not using a fixed seed\n",
        "\n",
        "    median_market_cap = df['market_cap'].median()\n",
        "    df['is_large_cap'] = df['market_cap'] > median_market_cap\n",
        "\n",
        "    # Convert transaction costs to basis points\n",
        "    df['transaction_cost'] = np.where(df['is_large_cap'],\n",
        "                                      np.random.normal(10.25, 2.05, df.shape[0]),\n",
        "                                      np.random.normal(21, 4.2, df.shape[0]))\n",
        "\n",
        "    # Ensure no negative transaction costs\n",
        "    df['transaction_cost'] = np.maximum(df['transaction_cost'], 0)\n",
        "\n",
        "    print(f\"Average cost for large-cap stocks: {df[df['is_large_cap']]['transaction_cost'].mean():.2f} bps\")\n",
        "    print(f\"Average cost for small-cap stocks: {df[~df['is_large_cap']]['transaction_cost'].mean():.2f} bps\")\n",
        "\n",
        "    print(f\"\\nLarge-cap cost range: {df[df['is_large_cap']]['transaction_cost'].min():.2f} bps to {df[df['is_large_cap']]['transaction_cost'].max():.2f} bps\")\n",
        "    print(f\"Small-cap cost range: {df[~df['is_large_cap']]['transaction_cost'].min():.2f} bps to {df[~df['is_large_cap']]['transaction_cost'].max():.2f} bps\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def construct_portfolio_with_costs(df, time_period='Week'):\n",
        "    df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "    if time_period == 'Week':\n",
        "        df['Period'] = df['Date From'].dt.to_period('W').dt.to_timestamp()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid time_period. Use 'Week'.\")\n",
        "\n",
        "    portfolio_returns = []\n",
        "\n",
        "    for period, group in df.groupby('Period'):\n",
        "        if period.year < 2016:\n",
        "            continue\n",
        "\n",
        "        group_sorted = group.sort_values(by='predicted_prob', ascending=False)\n",
        "\n",
        "        num_top_companies = 5\n",
        "        num_bottom_companies = 5\n",
        "        top_companies = group_sorted.head(num_top_companies)\n",
        "        bottom_companies = group_sorted.tail(num_bottom_companies)\n",
        "\n",
        "        def calculate_return_with_costs(companies, long_position=True):\n",
        "            returns = np.log1p(companies['Weekly Compound Return'])\n",
        "            costs = companies['transaction_cost'] / 10000  # Convert bps to decimal\n",
        "            if long_position:\n",
        "                return returns - costs\n",
        "            else:\n",
        "                return -returns - costs\n",
        "\n",
        "        long_return_eq = np.mean(calculate_return_with_costs(top_companies, long_position=True))\n",
        "        short_return_eq = np.mean(calculate_return_with_costs(bottom_companies, long_position=False))\n",
        "        long_short_return_eq = long_return_eq - short_return_eq\n",
        "\n",
        "        total_market_cap_long = np.sum(top_companies['market_cap'])\n",
        "        total_market_cap_short = np.sum(bottom_companies['market_cap'])\n",
        "\n",
        "        long_return_val = np.sum(calculate_return_with_costs(top_companies, long_position=True) * top_companies['market_cap']) / total_market_cap_long\n",
        "        short_return_val = np.sum(calculate_return_with_costs(bottom_companies, long_position=False) * bottom_companies['market_cap']) / total_market_cap_short\n",
        "        long_short_return_val = long_return_val - short_return_val\n",
        "\n",
        "        portfolio_returns.append({\n",
        "            'Period': period,\n",
        "            'Long Return (Eq)': long_return_eq,\n",
        "            'Short Return (Eq)': short_return_eq,\n",
        "            'Long-Short Return (Eq)': long_short_return_eq,\n",
        "            'Long Return (Val)': long_return_val,\n",
        "            'Short Return (Val)': short_return_val,\n",
        "            'Long-Short Return (Val)': long_short_return_val\n",
        "        })\n",
        "\n",
        "    portfolio_df = pd.DataFrame(portfolio_returns)\n",
        "    portfolio_df['EW L'] = portfolio_df['Long Return (Eq)'].cumsum()\n",
        "    portfolio_df['EW S'] = portfolio_df['Short Return (Eq)'].cumsum()\n",
        "    portfolio_df['EW LS'] = portfolio_df['Long-Short Return (Eq)'].cumsum()\n",
        "    portfolio_df['VW L'] = portfolio_df['Long Return (Val)'].cumsum()\n",
        "    portfolio_df['VW S'] = portfolio_df['Short Return (Val)'].cumsum()\n",
        "    portfolio_df['VW LS'] = portfolio_df['Long-Short Return (Val)'].cumsum()\n",
        "\n",
        "    actual_returns = df[df['Date From'].dt.year >= 2016].groupby('Period')['Weekly Compound Return'].mean()\n",
        "    actual_cumulative_returns = np.log1p(actual_returns).cumsum()\n",
        "    portfolio_df = portfolio_df.merge(actual_cumulative_returns.rename('Market'), on='Period', how='left')\n",
        "\n",
        "    return portfolio_df\n",
        "\n",
        "# Main execution\n",
        "df = prepare_data(insample_df, outsample_df)\n",
        "df = rolling_window_analysis(df)\n",
        "df = calculate_transaction_costs(df)\n",
        "\n",
        "# Weekly Portfolio with transaction costs\n",
        "portfolio_df_week_with_costs = construct_portfolio_with_costs(df, time_period='Week')\n",
        "portfolio_df_week_with_costs = portfolio_df_week_with_costs[portfolio_df_week_with_costs['Period'].dt.year >= 2016]\n",
        "\n",
        "print(\"Portfolio construction with transaction costs completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLELZ8Fblh86"
      },
      "source": [
        "# Distil RoBERTa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMtTEz2Rlh86"
      },
      "source": [
        "## Load Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eryBt7lTlh86"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained DistilRoBERTa model and tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n",
        "model = RobertaModel.from_pretrained('distilroberta-base')\n",
        "\n",
        "# Load embedded Dataframe\n",
        "with open('embedding-DistilRoBERTa-AllCompany-NEW.pkl', 'rb') as f:\n",
        "    insample_df, outsample_df = pd.read_pickle(f)\n",
        "\n",
        "# Display the first 2 rows of outsample_df to check\n",
        "filtered_df = outsample_df[outsample_df['headline'] != '[No_Headline]']\n",
        "filtered_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nui0-DaClh86"
      },
      "source": [
        "## Accuracy per-companies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iybpv7CVlh86"
      },
      "outputs": [],
      "source": [
        "# Combine insample and outsample data for rolling window\n",
        "df = pd.concat([insample_df, outsample_df])\n",
        "\n",
        "# Convert 'Date From' to datetime\n",
        "df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "\n",
        "# Sort by date\n",
        "df = df.sort_values(by='Date From')\n",
        "\n",
        "# Check available years\n",
        "available_years = df['Date From'].dt.year.unique()\n",
        "print(\"Years available in the data:\", available_years)\n",
        "\n",
        "# Define rolling window parameters\n",
        "window_size = 365 * 10  # 10 years in days\n",
        "prediction_period = 365  # 1 year in days\n",
        "\n",
        "# Get unique companies\n",
        "companies = df['companyname'].unique()\n",
        "\n",
        "# Dictionary to store DataFrames for each company\n",
        "company_results_dfs = {}\n",
        "\n",
        "# Rolling window analysis for each company\n",
        "for company in companies:\n",
        "    company_df = df[df['companyname'] == company].copy()\n",
        "\n",
        "    accuracies = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "    years = []\n",
        "\n",
        "    # Setup time series cross-validation\n",
        "    tscv = TimeSeriesSplit(n_splits=5)  # You can adjust the number of splits as needed\n",
        "\n",
        "    for train_index, test_index in tscv.split(company_df):\n",
        "        train_df, test_df = company_df.iloc[train_index], company_df.iloc[test_index]\n",
        "\n",
        "        if len(test_df) == 0 or len(train_df) == 0:\n",
        "            continue\n",
        "\n",
        "        X_train = np.vstack(train_df['embedding'].values)\n",
        "        y_train = train_df['Future Return Direction'].values\n",
        "\n",
        "        X_test = np.vstack(test_df['embedding'].values)\n",
        "        y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "        # Train logistic regression model\n",
        "        logistic_model = LogisticRegression(max_iter=1000)\n",
        "        logistic_model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = logistic_model.predict(X_test)\n",
        "\n",
        "        # Ensure the length of predictions and test_df matches\n",
        "        if len(y_pred) != len(test_df):\n",
        "            raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_pred)})\")\n",
        "\n",
        "        # Store the predictions and true values\n",
        "        test_df['predicted'] = y_pred\n",
        "\n",
        "        # Update the company-specific DataFrame with predictions\n",
        "        company_df.loc[test_df.index, 'predicted'] = test_df['predicted']\n",
        "\n",
        "        print(f'Company: {company}')\n",
        "        print(f'Training window: {train_df[\"Date From\"].min()} to {train_df[\"Date From\"].max()}')\n",
        "        print(f'Test window: {test_df[\"Date From\"].min()} to {test_df[\"Date From\"].max()}')\n",
        "        print(f'Predicted years: {test_df[\"Date From\"].dt.year.unique()}')\n",
        "\n",
        "    # Evaluate the Model for the company\n",
        "    for year in range(2016, 2024):\n",
        "        year_df = company_df[company_df['Date From'].dt.year == year]\n",
        "\n",
        "        if 'predicted' not in year_df.columns or year_df['predicted'].isnull().all():\n",
        "            print(f\"Missing predictions for year {year} for company {company} due to insufficient data or missing predictions.\")\n",
        "            continue\n",
        "\n",
        "        y_test = year_df['Future Return Direction'].values\n",
        "        y_pred = year_df['predicted'].values\n",
        "\n",
        "        # Remove NaN values in predictions\n",
        "        valid_indices = ~np.isnan(y_pred)\n",
        "        y_test = y_test[valid_indices]\n",
        "        y_pred = y_pred[valid_indices]\n",
        "\n",
        "        if len(y_pred) == 0 or len(y_test) == 0:\n",
        "            print(f\"Insufficient valid predictions for year {year} for company {company}.\")\n",
        "            continue\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "        recall = recall_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "        f1 = f1_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "\n",
        "        accuracies.append(accuracy)\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "        years.append(year)\n",
        "\n",
        "        print(f'Company: {company}, Year: {year}')\n",
        "        print(f'Accuracy: {accuracy:.2f}')\n",
        "        print(f'Precision: {precision:.2f}')\n",
        "        print(f'Recall: {recall:.2f}')\n",
        "        print(f'F1 Score: {f1:.2f}')\n",
        "        print()\n",
        "\n",
        "    # Store the results for the company in a DataFrame\n",
        "    results_df = pd.DataFrame({\n",
        "        'Year': years,\n",
        "        'Accuracy': accuracies,\n",
        "        'Precision': precisions,\n",
        "        'Recall': recalls,\n",
        "        'F1 Score': f1_scores\n",
        "    })\n",
        "\n",
        "    # Save the DataFrame in the dictionary\n",
        "    company_results_dfs[company] = results_df\n",
        "\n",
        "    print(f\"Evaluation results for {company}:\")\n",
        "    print(results_df)\n",
        "\n",
        "    # Plot accuracy over time for the company\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(results_df['Year'], results_df['Accuracy'], marker='o', label='Accuracy')\n",
        "    plt.plot(results_df['Year'], results_df['Precision'], marker='o', label='Precision')\n",
        "    plt.plot(results_df['Year'], results_df['Recall'], marker='o', label='Recall')\n",
        "    plt.plot(results_df['Year'], results_df['F1 Score'], marker='o', label='F1 Score')\n",
        "\n",
        "    plt.title(f'Performance Metrics Over Time for {company}')\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot confusion matrices for each year\n",
        "    n_plots = len(years)\n",
        "    n_cols = 4\n",
        "    n_rows = (n_plots // n_cols) + (n_plots % n_cols > 0)\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 4 * n_rows))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, year in enumerate(years):\n",
        "        year_df = company_df[company_df['Date From'].dt.year == year]\n",
        "        y_test = year_df['Future Return Direction'].values\n",
        "        y_pred = year_df['predicted'].values\n",
        "\n",
        "        # Remove NaN values in predictions\n",
        "        valid_indices = ~np.isnan(y_pred)\n",
        "        y_test = y_test[valid_indices]\n",
        "        y_pred = y_pred[valid_indices]\n",
        "\n",
        "        if len(y_pred) > 0 and len(y_test) > 0:\n",
        "            cm = confusion_matrix(y_test, y_pred)\n",
        "            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logistic_model.classes_)\n",
        "            disp.plot(cmap='Blues', ax=axes[i])\n",
        "            axes[i].set_title(f'{company} {year}')\n",
        "\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Access the stored DataFrames\n",
        "for company, results_df in company_results_dfs.items():\n",
        "    print(f\"\\nResults for {company}:\")\n",
        "    print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIJDrnBIlh87"
      },
      "outputs": [],
      "source": [
        "# Concatenate all the company results into one DataFrame\n",
        "all_results_df = pd.concat(company_results_dfs.values(), keys=company_results_dfs.keys()).reset_index(level=0).rename(columns={'level_0': 'Company'})\n",
        "\n",
        "# Save the combined DataFrame to a CSV file\n",
        "all_results_df.to_csv('[EVAL] DistilRoBERTa_all_company_results.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97Q7oRjwlh87"
      },
      "source": [
        "## Accuracy all years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5jS-lg7lh87"
      },
      "outputs": [],
      "source": [
        "# Combine insample and outsample data for rolling window\n",
        "df = pd.concat([insample_df, outsample_df])\n",
        "\n",
        "# Convert 'Date From' to datetime\n",
        "df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "\n",
        "# Sort by date\n",
        "df = df.sort_values(by='Date From')\n",
        "\n",
        "# Check available years\n",
        "available_years = df['Date From'].dt.year.unique()\n",
        "print(\"Years available in the data:\", available_years)\n",
        "\n",
        "# Define rolling window parameters\n",
        "window_size = 365 * 10  # 10 years in days\n",
        "prediction_period = 365  # 1 year in days\n",
        "\n",
        "# Get unique companies\n",
        "companies = df['companyname'].unique()\n",
        "\n",
        "# Dictionary to store DataFrames for each company\n",
        "company_results_dfs = {}\n",
        "\n",
        "# Rolling window analysis for each company\n",
        "for company in companies:\n",
        "    company_df = df[df['companyname'] == company].copy()\n",
        "\n",
        "    accuracies = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "    years = []\n",
        "\n",
        "    # Setup time series cross-validation\n",
        "    tscv = TimeSeriesSplit(n_splits=5)  # You can adjust the number of splits as needed\n",
        "\n",
        "    for train_index, test_index in tscv.split(company_df):\n",
        "        train_df, test_df = company_df.iloc[train_index], company_df.iloc[test_index]\n",
        "\n",
        "        if len(test_df) == 0 or len(train_df) == 0:\n",
        "            continue\n",
        "\n",
        "        X_train = np.vstack(train_df['embedding'].values)\n",
        "        y_train = train_df['Future Return Direction'].values\n",
        "\n",
        "        X_test = np.vstack(test_df['embedding'].values)\n",
        "        y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "        # Train logistic regression model\n",
        "        logistic_model = LogisticRegression(max_iter=1000)\n",
        "        logistic_model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = logistic_model.predict(X_test)\n",
        "\n",
        "        # Ensure the length of predictions and test_df matches\n",
        "        if len(y_pred) != len(test_df):\n",
        "            raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_pred)})\")\n",
        "\n",
        "        # Store the predictions and true values\n",
        "        test_df['predicted'] = y_pred\n",
        "\n",
        "        # Update the company-specific DataFrame with predictions\n",
        "        company_df.loc[test_df.index, 'predicted'] = test_df['predicted']\n",
        "\n",
        "    # Evaluate the Model for the company\n",
        "    for year in range(2016, 2024):\n",
        "        year_df = company_df[company_df['Date From'].dt.year == year]\n",
        "\n",
        "        if 'predicted' not in year_df.columns or year_df['predicted'].isnull().all():\n",
        "            continue\n",
        "\n",
        "        y_test = year_df['Future Return Direction'].values\n",
        "        y_pred = year_df['predicted'].values\n",
        "\n",
        "        # Remove NaN values in predictions\n",
        "        valid_indices = ~np.isnan(y_pred)\n",
        "        y_test = y_test[valid_indices]\n",
        "        y_pred = y_pred[valid_indices]\n",
        "\n",
        "        if len(y_pred) == 0 or len(y_test) == 0:\n",
        "            continue\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "        recall = recall_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "        f1 = f1_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "\n",
        "        accuracies.append(accuracy)\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "        years.append(year)\n",
        "\n",
        "    # Store the results for the company in a DataFrame\n",
        "    results_df = pd.DataFrame({\n",
        "        'Year': years,\n",
        "        'Accuracy': accuracies,\n",
        "        'Precision': precisions,\n",
        "        'Recall': recalls,\n",
        "        'F1 Score': f1_scores\n",
        "    })\n",
        "\n",
        "    # Save the DataFrame in the dictionary\n",
        "    company_results_dfs[company] = results_df\n",
        "\n",
        "# Combine results of all companies into a single DataFrame\n",
        "combined_results = pd.concat(company_results_dfs.values(), ignore_index=True)\n",
        "\n",
        "# Calculate average accuracy per year across all companies\n",
        "average_metrics_per_year = combined_results.groupby('Year').mean().reset_index()\n",
        "\n",
        "# Print average metrics for each year\n",
        "print(\"Average Metrics Per Year:\")\n",
        "print(average_metrics_per_year)\n",
        "\n",
        "# Plot average accuracy per year\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "plt.plot(average_metrics_per_year['Year'], average_metrics_per_year['Accuracy'], marker='o', label='Accuracy')\n",
        "plt.plot(average_metrics_per_year['Year'], average_metrics_per_year['Precision'], marker='o', label='Precision')\n",
        "plt.plot(average_metrics_per_year['Year'], average_metrics_per_year['Recall'], marker='o', label='Recall')\n",
        "plt.plot(average_metrics_per_year['Year'], average_metrics_per_year['F1 Score'], marker='o', label='F1 Score')\n",
        "\n",
        "plt.title('Average Performance Metrics Over Time Across All Companies')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzB0eR7Flh87"
      },
      "source": [
        "## Standard Deviation All Years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix4XzvxGlh87"
      },
      "outputs": [],
      "source": [
        "# Combine insample and outsample data for rolling window\n",
        "df = pd.concat([insample_df, outsample_df])\n",
        "\n",
        "# Convert 'Date From' to datetime\n",
        "df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "\n",
        "# Sort by date\n",
        "df = df.sort_values(by='Date From')\n",
        "\n",
        "# Check available years\n",
        "available_years = df['Date From'].dt.year.unique()\n",
        "print(\"Years available in the data:\", available_years)\n",
        "\n",
        "# Get unique companies\n",
        "companies = df['companyname'].unique()\n",
        "\n",
        "# Dictionary to store DataFrames for each company\n",
        "company_results_dfs = {}\n",
        "\n",
        "# Rolling window analysis for each company\n",
        "for company in companies:\n",
        "    company_df = df[df['companyname'] == company].copy()\n",
        "\n",
        "    accuracies = []\n",
        "    years = []\n",
        "\n",
        "    # Setup time series cross-validation\n",
        "    tscv = TimeSeriesSplit(n_splits=5)  # You can adjust the number of splits as needed\n",
        "\n",
        "    for train_index, test_index in tscv.split(company_df):\n",
        "        train_df, test_df = company_df.iloc[train_index], company_df.iloc[test_index]\n",
        "\n",
        "        if len(test_df) == 0 or len(train_df) == 0:\n",
        "            continue\n",
        "\n",
        "        X_train = np.vstack(train_df['embedding'].values)\n",
        "        y_train = train_df['Future Return Direction'].values\n",
        "\n",
        "        X_test = np.vstack(test_df['embedding'].values)\n",
        "        y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "        # Train logistic regression model\n",
        "        logistic_model = LogisticRegression(max_iter=1000)\n",
        "        logistic_model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = logistic_model.predict(X_test)\n",
        "\n",
        "        # Ensure the length of predictions and test_df matches\n",
        "        if len(y_pred) != len(test_df):\n",
        "            raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_pred)})\")\n",
        "\n",
        "        # Store the predictions and true values\n",
        "        test_df['predicted'] = y_pred\n",
        "\n",
        "        # Update the company-specific DataFrame with predictions\n",
        "        company_df.loc[test_df.index, 'predicted'] = test_df['predicted']\n",
        "\n",
        "    # Evaluate the Model for the company\n",
        "    for year in range(2016, 2024):\n",
        "        year_df = company_df[company_df['Date From'].dt.year == year]\n",
        "\n",
        "        if 'predicted' not in year_df.columns or year_df['predicted'].isnull().all():\n",
        "            continue\n",
        "\n",
        "        y_test = year_df['Future Return Direction'].values\n",
        "        y_pred = year_df['predicted'].values\n",
        "\n",
        "        # Remove NaN values in predictions\n",
        "        valid_indices = ~np.isnan(y_pred)\n",
        "        y_test = y_test[valid_indices]\n",
        "        y_pred = y_pred[valid_indices]\n",
        "\n",
        "        if len(y_pred) == 0 or len(y_test) == 0:\n",
        "            continue\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        accuracies.append(accuracy)\n",
        "        years.append(year)\n",
        "\n",
        "    # Store the results for the company in a DataFrame\n",
        "    results_df = pd.DataFrame({\n",
        "        'Year': years,\n",
        "        'Accuracy': accuracies\n",
        "    })\n",
        "\n",
        "    # Save the DataFrame in the dictionary\n",
        "    company_results_dfs[company] = results_df\n",
        "\n",
        "# Combine results of all companies into a single DataFrame\n",
        "combined_results = pd.concat(company_results_dfs.values(), ignore_index=True)\n",
        "\n",
        "# Calculate average accuracy per year across all companies\n",
        "average_metrics_per_year = combined_results.groupby('Year').agg(\n",
        "    Accuracy_mean=('Accuracy', 'mean'),\n",
        "    Accuracy_std=('Accuracy', 'std')\n",
        ").reset_index()\n",
        "\n",
        "# Print average metrics for each year\n",
        "print(\"Average Metrics Per Year:\")\n",
        "print(average_metrics_per_year)\n",
        "\n",
        "# Get a color sequence from Plotly's default colors\n",
        "colors = px.colors.qualitative.Plotly\n",
        "\n",
        "# Function to make the color more transparent\n",
        "def get_transparent_color(color, alpha=0.2):\n",
        "    # Convert hex to RGB and then to RGBA\n",
        "    hex_color = color.lstrip('#')\n",
        "    rgb_color = tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n",
        "    return f'rgba({rgb_color[0]}, {rgb_color[1]}, {rgb_color[2]}, {alpha})'\n",
        "\n",
        "# Create a figure\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add the mean line\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=average_metrics_per_year['Year'],\n",
        "    y=average_metrics_per_year['Accuracy_mean'],\n",
        "    mode='lines',\n",
        "    name='Accuracy',\n",
        "    line=dict(color=colors[0], width=2)\n",
        "))\n",
        "\n",
        "# Add the standard deviation shaded area\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=pd.concat([average_metrics_per_year['Year'], average_metrics_per_year['Year'][::-1]]),\n",
        "    y=pd.concat([average_metrics_per_year['Accuracy_mean'] + average_metrics_per_year['Accuracy_std'],\n",
        "                 (average_metrics_per_year['Accuracy_mean'] - average_metrics_per_year['Accuracy_std'])[::-1]]),\n",
        "    fill='toself',\n",
        "    fillcolor=get_transparent_color(colors[0], alpha=0.2),  # Use the same color with transparency\n",
        "    line=dict(color='rgba(255,255,255,0)'),\n",
        "    hoverinfo=\"skip\",\n",
        "    showlegend=False,\n",
        "    name='Accuracy std dev'\n",
        "))\n",
        "\n",
        "# Customize layout\n",
        "fig.update_layout(\n",
        "    title='Average Rolling Window Accuracy Over Time Across All Companies',\n",
        "    xaxis_title='Year',\n",
        "    yaxis_title='Accuracy',\n",
        "    template='plotly_white',\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIjOCzJ9lh87"
      },
      "source": [
        "## Portfolio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWuI0RPxlh87"
      },
      "outputs": [],
      "source": [
        "def prepare_data(insample_df, outsample_df):\n",
        "    df = pd.concat([insample_df, outsample_df])\n",
        "    df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "    df = df.sort_values(by='Date From')\n",
        "    available_years = df['Date From'].dt.year.unique()\n",
        "    print(\"Years available in the data:\", available_years)\n",
        "    return df\n",
        "\n",
        "def rolling_window_analysis(df):\n",
        "    companies = df['companyname'].unique()\n",
        "    predictions_df = pd.DataFrame()\n",
        "\n",
        "    for company in companies:\n",
        "        company_df = df[df['companyname'] == company].copy()\n",
        "        company_df = company_df.sort_values(by='Date From')\n",
        "\n",
        "        # Define the rolling window parameters\n",
        "        start_year = company_df['Date From'].dt.year.min()\n",
        "        end_year = company_df['Date From'].dt.year.max()\n",
        "        window_size = 10\n",
        "        validation_size = 1\n",
        "\n",
        "        for start in range(start_year, end_year - window_size - validation_size + 1):\n",
        "            train_start = start\n",
        "            train_end = start + window_size\n",
        "            val_start = train_end\n",
        "            val_end = train_end + validation_size\n",
        "\n",
        "            train_df = company_df[(company_df['Date From'].dt.year >= train_start) &\n",
        "                                  (company_df['Date From'].dt.year < train_end)]\n",
        "            test_df = company_df[(company_df['Date From'].dt.year >= val_start) &\n",
        "                                 (company_df['Date From'].dt.year < val_end)]\n",
        "\n",
        "            if len(test_df) == 0 or len(train_df) == 0:\n",
        "                continue\n",
        "\n",
        "            X_train = np.vstack(train_df['embedding'].values)\n",
        "            y_train = train_df['Future Return Direction'].values\n",
        "            X_test = np.vstack(test_df['embedding'].values)\n",
        "            y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "            # Train logistic regression model\n",
        "            logistic_model = LogisticRegression(max_iter=1000)\n",
        "            logistic_model.fit(X_train, y_train)\n",
        "\n",
        "            # Get prediction probabilities\n",
        "            y_prob = logistic_model.predict_proba(X_test)[:, 1]  # Probability of the positive class\n",
        "\n",
        "            if len(y_prob) != len(test_df):\n",
        "                raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_prob)})\")\n",
        "\n",
        "            test_df['predicted_prob'] = y_prob\n",
        "            predictions_df = pd.concat([predictions_df, test_df[['Date From', 'companyname', 'predicted_prob', 'Weekly Compound Return']]], ignore_index=True)\n",
        "\n",
        "    df = df.merge(predictions_df, on=['Date From', 'companyname', 'Weekly Compound Return'], how='left', suffixes=('', '_pred'))\n",
        "    return df\n",
        "\n",
        "def construct_portfolio(df, time_period='Week'):\n",
        "    df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "    if time_period == 'Week':\n",
        "        df['Period'] = df['Date From'].dt.to_period('W').dt.to_timestamp()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid time_period. Use 'Week'.\")\n",
        "\n",
        "    portfolio_returns = []\n",
        "\n",
        "    for period, group in df.groupby('Period'):\n",
        "        if period.year < 2016:\n",
        "            continue\n",
        "\n",
        "        # Sort group by predicted_prob descending\n",
        "        group_sorted = group.sort_values(by='predicted_prob', ascending=False)\n",
        "\n",
        "        # Select top and bottom companies\n",
        "        num_top_companies = 5\n",
        "        num_bottom_companies = 5\n",
        "        top_companies = group_sorted.head(num_top_companies)\n",
        "        bottom_companies = group_sorted.tail(num_bottom_companies)\n",
        "\n",
        "        # Equal-weighted returns\n",
        "        long_return_eq = np.mean(np.log1p(top_companies['Weekly Compound Return']))\n",
        "        short_return_eq = np.mean(np.log1p(bottom_companies['Weekly Compound Return']))\n",
        "        long_short_return_eq = long_return_eq - short_return_eq\n",
        "\n",
        "        # Value-weighted returns\n",
        "        long_return_val = np.sum(np.log1p(top_companies['Weekly Compound Return']) * top_companies['market_cap']) / np.sum(top_companies['market_cap'])\n",
        "        short_return_val = np.sum(np.log1p(bottom_companies['Weekly Compound Return']) * bottom_companies['market_cap']) / np.sum(bottom_companies['market_cap'])\n",
        "        long_short_return_val = long_return_val - short_return_val\n",
        "\n",
        "        portfolio_returns.append({\n",
        "            'Period': period,\n",
        "            'Long Return (Eq)': long_return_eq,\n",
        "            'Short Return (Eq)': short_return_eq,\n",
        "            'Long-Short Return (Eq)': long_short_return_eq,\n",
        "            'Long Return (Val)': long_return_val,\n",
        "            'Short Return (Val)': short_return_val,\n",
        "            'Long-Short Return (Val)': long_short_return_val\n",
        "        })\n",
        "\n",
        "    portfolio_df = pd.DataFrame(portfolio_returns)\n",
        "    portfolio_df['EW L'] = portfolio_df['Long Return (Eq)'].cumsum()\n",
        "    portfolio_df['EW S'] = portfolio_df['Short Return (Eq)'].cumsum()\n",
        "    portfolio_df['EW LS'] = portfolio_df['Long-Short Return (Eq)'].cumsum()\n",
        "    portfolio_df['VW L'] = portfolio_df['Long Return (Val)'].cumsum()\n",
        "    portfolio_df['VW S'] = portfolio_df['Short Return (Val)'].cumsum()\n",
        "    portfolio_df['VW LS'] = portfolio_df['Long-Short Return (Val)'].cumsum()\n",
        "\n",
        "    actual_returns = df[df['Date From'].dt.year >= 2016].groupby('Period')['Weekly Compound Return'].mean()\n",
        "    actual_cumulative_returns = np.log1p(actual_returns).cumsum()\n",
        "    portfolio_df = portfolio_df.merge(actual_cumulative_returns.rename('Market'), on='Period', how='left')\n",
        "\n",
        "    metrics = {}\n",
        "    for portfolio in ['EW L', 'EW S', 'EW LS', 'VW L', 'VW S', 'VW LS']:\n",
        "        returns = portfolio_df[portfolio]\n",
        "\n",
        "        if returns.isnull().all() or returns.eq(0).all():\n",
        "            sharpe_ratio = np.nan\n",
        "            max_drawdown = np.nan\n",
        "            volatility = np.nan\n",
        "        else:\n",
        "            sharpe_ratio = returns.mean() / returns.std() * np.sqrt(52) if returns.std() != 0 else np.nan\n",
        "            cumulative_returns = returns.cumsum()\n",
        "            max_drawdown = (cumulative_returns.cummax() - cumulative_returns).max()\n",
        "            volatility = returns.std() * np.sqrt(52)\n",
        "\n",
        "        metrics[portfolio] = {\n",
        "            'Sharpe Ratio': sharpe_ratio\n",
        "        }\n",
        "\n",
        "        print(f\"Metrics for {portfolio}:\")\n",
        "        print(f\"Sharpe Ratio: {sharpe_ratio}\")\n",
        "        print()\n",
        "\n",
        "    portfolio_df.to_csv('DistilRoBERTa_portfolio_returns.csv', index=False)\n",
        "    print(\"Portfolio returns saved to 'DistilRoBERTa_portfolio_returns.csv'\")\n",
        "    return portfolio_df\n",
        "\n",
        "def plot_portfolio_returns(portfolio_df, title_suffix=''):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['EW L'], marker='o', markersize=1, label='EW L')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['EW S'], marker='o', markersize=1, label='EW S')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['EW LS'], marker='o', markersize=1, label='EW LS')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['VW L'], marker='o', markersize=1, label='VW L')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['VW S'], marker='o', markersize=1, label='VW S')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['VW LS'], marker='o', markersize=1, label='VW LS')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['Market'], marker='o', markersize=1, label='Market')\n",
        "\n",
        "    plt.title(f'Cumulative {title_suffix} Portfolio Returns Over Time')\n",
        "    plt.xlabel('Period')\n",
        "    plt.ylabel('Cumulative Log Return')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage for Weekly\n",
        "df = prepare_data(insample_df, outsample_df)\n",
        "df = rolling_window_analysis(df)\n",
        "\n",
        "# Weekly Portfolio\n",
        "portfolio_df_week = construct_portfolio(df, time_period='Week')\n",
        "portfolio_df_week = portfolio_df_week[portfolio_df_week['Period'].dt.year >= 2016]\n",
        "plot_portfolio_returns(portfolio_df_week, title_suffix='Weekly')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaMGfNl0lh88"
      },
      "source": [
        "## Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PQaLuYrlh88"
      },
      "outputs": [],
      "source": [
        "# Suppress SettingWithCopyWarning\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "def prepare_data(insample_df, outsample_df):\n",
        "    df = pd.concat([insample_df, outsample_df])\n",
        "    df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "    df = df.sort_values(by='Date From')\n",
        "    available_years = df['Date From'].dt.year.unique()\n",
        "    print(\"Years available in the data:\", available_years)\n",
        "    return df\n",
        "\n",
        "def rolling_window_analysis(df):\n",
        "    companies = df['companyname'].unique()\n",
        "    predictions_df = pd.DataFrame()\n",
        "\n",
        "    for company in companies:\n",
        "        company_df = df[df['companyname'] == company].copy()\n",
        "        company_df = company_df.sort_values(by='Date From')\n",
        "\n",
        "        start_year = company_df['Date From'].dt.year.min()\n",
        "        end_year = company_df['Date From'].dt.year.max()\n",
        "        window_size = 10\n",
        "        validation_size = 1\n",
        "\n",
        "        for start in range(start_year, end_year - window_size - validation_size + 1):\n",
        "            train_start = start\n",
        "            train_end = start + window_size\n",
        "            val_start = train_end\n",
        "            val_end = train_end + validation_size\n",
        "\n",
        "            train_df = company_df[(company_df['Date From'].dt.year >= train_start) &\n",
        "                                  (company_df['Date From'].dt.year < train_end)]\n",
        "            test_df = company_df[(company_df['Date From'].dt.year >= val_start) &\n",
        "                                 (company_df['Date From'].dt.year < val_end)]\n",
        "\n",
        "            if len(test_df) == 0 or len(train_df) == 0:\n",
        "                continue\n",
        "\n",
        "            X_train = np.vstack(train_df['embedding'].values)\n",
        "            y_train = train_df['Future Return Direction'].values\n",
        "            X_test = np.vstack(test_df['embedding'].values)\n",
        "            y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "            logistic_model = LogisticRegression(max_iter=1000)\n",
        "            logistic_model.fit(X_train, y_train)\n",
        "\n",
        "            y_prob = logistic_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "            if len(y_prob) != len(test_df):\n",
        "                raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_prob)})\")\n",
        "\n",
        "            test_df['predicted_prob'] = y_prob\n",
        "            predictions_df = pd.concat([predictions_df, test_df[['Date From', 'companyname', 'predicted_prob', 'Weekly Compound Return', 'market_cap']]], ignore_index=True)\n",
        "\n",
        "    df = df.merge(predictions_df, on=['Date From', 'companyname', 'Weekly Compound Return', 'market_cap'], how='left', suffixes=('', '_pred'))\n",
        "    return df\n",
        "\n",
        "def calculate_transaction_costs(df):\n",
        "    np.random.seed(None)  # Ensure we're not using a fixed seed\n",
        "\n",
        "    median_market_cap = df['market_cap'].median()\n",
        "    df['is_large_cap'] = df['market_cap'] > median_market_cap\n",
        "\n",
        "    # Convert transaction costs to basis points\n",
        "    df['transaction_cost'] = np.where(df['is_large_cap'],\n",
        "                                      np.random.normal(10.25, 2.05, df.shape[0]),\n",
        "                                      np.random.normal(21, 4.2, df.shape[0]))\n",
        "\n",
        "    # Ensure no negative transaction costs\n",
        "    df['transaction_cost'] = np.maximum(df['transaction_cost'], 0)\n",
        "\n",
        "    print(f\"Average cost for large-cap stocks: {df[df['is_large_cap']]['transaction_cost'].mean():.2f} bps\")\n",
        "    print(f\"Average cost for small-cap stocks: {df[~df['is_large_cap']]['transaction_cost'].mean():.2f} bps\")\n",
        "\n",
        "    print(f\"\\nLarge-cap cost range: {df[df['is_large_cap']]['transaction_cost'].min():.2f} bps to {df[df['is_large_cap']]['transaction_cost'].max():.2f} bps\")\n",
        "    print(f\"Small-cap cost range: {df[~df['is_large_cap']]['transaction_cost'].min():.2f} bps to {df[~df['is_large_cap']]['transaction_cost'].max():.2f} bps\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def construct_portfolio_with_costs(df, time_period='Week'):\n",
        "    df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "    if time_period == 'Week':\n",
        "        df['Period'] = df['Date From'].dt.to_period('W').dt.to_timestamp()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid time_period. Use 'Week'.\")\n",
        "\n",
        "    portfolio_returns = []\n",
        "\n",
        "    for period, group in df.groupby('Period'):\n",
        "        if period.year < 2016:\n",
        "            continue\n",
        "\n",
        "        group_sorted = group.sort_values(by='predicted_prob', ascending=False)\n",
        "\n",
        "        num_top_companies = 5\n",
        "        num_bottom_companies = 5\n",
        "        top_companies = group_sorted.head(num_top_companies)\n",
        "        bottom_companies = group_sorted.tail(num_bottom_companies)\n",
        "\n",
        "        def calculate_return_with_costs(companies, long_position=True):\n",
        "            returns = np.log1p(companies['Weekly Compound Return'])\n",
        "            costs = companies['transaction_cost'] / 10000  # Convert bps to decimal\n",
        "            if long_position:\n",
        "                return returns - costs\n",
        "            else:\n",
        "                return -returns - costs\n",
        "\n",
        "        long_return_eq = np.mean(calculate_return_with_costs(top_companies, long_position=True))\n",
        "        short_return_eq = np.mean(calculate_return_with_costs(bottom_companies, long_position=False))\n",
        "        long_short_return_eq = long_return_eq - short_return_eq\n",
        "\n",
        "        total_market_cap_long = np.sum(top_companies['market_cap'])\n",
        "        total_market_cap_short = np.sum(bottom_companies['market_cap'])\n",
        "\n",
        "        long_return_val = np.sum(calculate_return_with_costs(top_companies, long_position=True) * top_companies['market_cap']) / total_market_cap_long\n",
        "        short_return_val = np.sum(calculate_return_with_costs(bottom_companies, long_position=False) * bottom_companies['market_cap']) / total_market_cap_short\n",
        "        long_short_return_val = long_return_val - short_return_val\n",
        "\n",
        "        portfolio_returns.append({\n",
        "            'Period': period,\n",
        "            'Long Return (Eq)': long_return_eq,\n",
        "            'Short Return (Eq)': short_return_eq,\n",
        "            'Long-Short Return (Eq)': long_short_return_eq,\n",
        "            'Long Return (Val)': long_return_val,\n",
        "            'Short Return (Val)': short_return_val,\n",
        "            'Long-Short Return (Val)': long_short_return_val\n",
        "        })\n",
        "\n",
        "    portfolio_df = pd.DataFrame(portfolio_returns)\n",
        "    portfolio_df['EW L'] = portfolio_df['Long Return (Eq)'].cumsum()\n",
        "    portfolio_df['EW S'] = portfolio_df['Short Return (Eq)'].cumsum()\n",
        "    portfolio_df['EW LS'] = portfolio_df['Long-Short Return (Eq)'].cumsum()\n",
        "    portfolio_df['VW L'] = portfolio_df['Long Return (Val)'].cumsum()\n",
        "    portfolio_df['VW S'] = portfolio_df['Short Return (Val)'].cumsum()\n",
        "    portfolio_df['VW LS'] = portfolio_df['Long-Short Return (Val)'].cumsum()\n",
        "\n",
        "    actual_returns = df[df['Date From'].dt.year >= 2016].groupby('Period')['Weekly Compound Return'].mean()\n",
        "    actual_cumulative_returns = np.log1p(actual_returns).cumsum()\n",
        "    portfolio_df = portfolio_df.merge(actual_cumulative_returns.rename('Market'), on='Period', how='left')\n",
        "\n",
        "    return portfolio_df\n",
        "\n",
        "# Main execution\n",
        "df = prepare_data(insample_df, outsample_df)\n",
        "df = rolling_window_analysis(df)\n",
        "df = calculate_transaction_costs(df)\n",
        "\n",
        "# Weekly Portfolio with transaction costs\n",
        "portfolio_df_week_with_costs = construct_portfolio_with_costs(df, time_period='Week')\n",
        "portfolio_df_week_with_costs = portfolio_df_week_with_costs[portfolio_df_week_with_costs['Period'].dt.year >= 2016]\n",
        "\n",
        "print(\"Portfolio construction with transaction costs completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZPz29Lxlh88"
      },
      "source": [
        "# FinBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaaBrXtRlh88"
      },
      "source": [
        "## Load Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9B-tHmHlh88"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained FinBERT model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('yiyanghkust/finbert-tone', use_fast=False)\n",
        "model = AutoModel.from_pretrained('yiyanghkust/finbert-tone')\n",
        "\n",
        "# Load embedded Dataframe\n",
        "with open('embedding-FinBERT-AllCompany-NEW.pkl', 'rb') as f:\n",
        "    insample_df, outsample_df = pd.read_pickle(f)\n",
        "\n",
        "# Display the first 2 rows of outsample_df to check\n",
        "filtered_df = outsample_df[outsample_df['headline'] != '[No_Headline]']\n",
        "filtered_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MczIpDd2lh88"
      },
      "source": [
        "## Accuracy per-companies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgBaWM4ylh88"
      },
      "outputs": [],
      "source": [
        "# Combine insample and outsample data for rolling window\n",
        "df = pd.concat([insample_df, outsample_df])\n",
        "\n",
        "# Convert 'Date From' to datetime\n",
        "df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "\n",
        "# Sort by date\n",
        "df = df.sort_values(by='Date From')\n",
        "\n",
        "# Check available years\n",
        "available_years = df['Date From'].dt.year.unique()\n",
        "print(\"Years available in the data:\", available_years)\n",
        "\n",
        "# Define rolling window parameters\n",
        "window_size = 365 * 10  # 10 years in days\n",
        "prediction_period = 365  # 1 year in days\n",
        "\n",
        "# Get unique companies\n",
        "companies = df['companyname'].unique()\n",
        "\n",
        "# Dictionary to store DataFrames for each company\n",
        "company_results_dfs = {}\n",
        "\n",
        "# Rolling window analysis for each company\n",
        "for company in companies:\n",
        "    company_df = df[df['companyname'] == company].copy()\n",
        "\n",
        "    accuracies = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "    years = []\n",
        "\n",
        "    # Setup time series cross-validation\n",
        "    tscv = TimeSeriesSplit(n_splits=5)  # You can adjust the number of splits as needed\n",
        "\n",
        "    for train_index, test_index in tscv.split(company_df):\n",
        "        train_df, test_df = company_df.iloc[train_index], company_df.iloc[test_index]\n",
        "\n",
        "        if len(test_df) == 0 or len(train_df) == 0:\n",
        "            continue\n",
        "\n",
        "        X_train = np.vstack(train_df['embedding'].values)\n",
        "        y_train = train_df['Future Return Direction'].values\n",
        "\n",
        "        X_test = np.vstack(test_df['embedding'].values)\n",
        "        y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "        # Train logistic regression model\n",
        "        logistic_model = LogisticRegression(max_iter=1000)\n",
        "        logistic_model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = logistic_model.predict(X_test)\n",
        "\n",
        "        # Ensure the length of predictions and test_df matches\n",
        "        if len(y_pred) != len(test_df):\n",
        "            raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_pred)})\")\n",
        "\n",
        "        # Store the predictions and true values\n",
        "        test_df['predicted'] = y_pred\n",
        "\n",
        "        # Update the company-specific DataFrame with predictions\n",
        "        company_df.loc[test_df.index, 'predicted'] = test_df['predicted']\n",
        "\n",
        "        print(f'Company: {company}')\n",
        "        print(f'Training window: {train_df[\"Date From\"].min()} to {train_df[\"Date From\"].max()}')\n",
        "        print(f'Test window: {test_df[\"Date From\"].min()} to {test_df[\"Date From\"].max()}')\n",
        "        print(f'Predicted years: {test_df[\"Date From\"].dt.year.unique()}')\n",
        "\n",
        "    # Evaluate the Model for the company\n",
        "    for year in range(2016, 2024):\n",
        "        year_df = company_df[company_df['Date From'].dt.year == year]\n",
        "\n",
        "        if 'predicted' not in year_df.columns or year_df['predicted'].isnull().all():\n",
        "            print(f\"Missing predictions for year {year} for company {company} due to insufficient data or missing predictions.\")\n",
        "            continue\n",
        "\n",
        "        y_test = year_df['Future Return Direction'].values\n",
        "        y_pred = year_df['predicted'].values\n",
        "\n",
        "        # Remove NaN values in predictions\n",
        "        valid_indices = ~np.isnan(y_pred)\n",
        "        y_test = y_test[valid_indices]\n",
        "        y_pred = y_pred[valid_indices]\n",
        "\n",
        "        if len(y_pred) == 0 or len(y_test) == 0:\n",
        "            print(f\"Insufficient valid predictions for year {year} for company {company}.\")\n",
        "            continue\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "        recall = recall_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "        f1 = f1_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "\n",
        "        accuracies.append(accuracy)\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "        years.append(year)\n",
        "\n",
        "        print(f'Company: {company}, Year: {year}')\n",
        "        print(f'Accuracy: {accuracy:.2f}')\n",
        "        print(f'Precision: {precision:.2f}')\n",
        "        print(f'Recall: {recall:.2f}')\n",
        "        print(f'F1 Score: {f1:.2f}')\n",
        "        print()\n",
        "\n",
        "    # Store the results for the company in a DataFrame\n",
        "    results_df = pd.DataFrame({\n",
        "        'Year': years,\n",
        "        'Accuracy': accuracies,\n",
        "        'Precision': precisions,\n",
        "        'Recall': recalls,\n",
        "        'F1 Score': f1_scores\n",
        "    })\n",
        "\n",
        "    # Save the DataFrame in the dictionary\n",
        "    company_results_dfs[company] = results_df\n",
        "\n",
        "    print(f\"Evaluation results for {company}:\")\n",
        "    print(results_df)\n",
        "\n",
        "    # Plot accuracy over time for the company\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(results_df['Year'], results_df['Accuracy'], marker='o', label='Accuracy')\n",
        "    plt.plot(results_df['Year'], results_df['Precision'], marker='o', label='Precision')\n",
        "    plt.plot(results_df['Year'], results_df['Recall'], marker='o', label='Recall')\n",
        "    plt.plot(results_df['Year'], results_df['F1 Score'], marker='o', label='F1 Score')\n",
        "\n",
        "    plt.title(f'Performance Metrics Over Time for {company}')\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot confusion matrices for each year\n",
        "    n_plots = len(years)\n",
        "    n_cols = 4\n",
        "    n_rows = (n_plots // n_cols) + (n_plots % n_cols > 0)\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 4 * n_rows))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, year in enumerate(years):\n",
        "        year_df = company_df[company_df['Date From'].dt.year == year]\n",
        "        y_test = year_df['Future Return Direction'].values\n",
        "        y_pred = year_df['predicted'].values\n",
        "\n",
        "        # Remove NaN values in predictions\n",
        "        valid_indices = ~np.isnan(y_pred)\n",
        "        y_test = y_test[valid_indices]\n",
        "        y_pred = y_pred[valid_indices]\n",
        "\n",
        "        if len(y_pred) > 0 and len(y_test) > 0:\n",
        "            cm = confusion_matrix(y_test, y_pred)\n",
        "            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logistic_model.classes_)\n",
        "            disp.plot(cmap='Blues', ax=axes[i])\n",
        "            axes[i].set_title(f'{company} {year}')\n",
        "\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Access the stored DataFrames\n",
        "for company, results_df in company_results_dfs.items():\n",
        "    print(f\"\\nResults for {company}:\")\n",
        "    print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MB3lBLe7lh89"
      },
      "outputs": [],
      "source": [
        "# Concatenate all the company results into one DataFrame\n",
        "all_results_df = pd.concat(company_results_dfs.values(), keys=company_results_dfs.keys()).reset_index(level=0).rename(columns={'level_0': 'Company'})\n",
        "\n",
        "# Save the combined DataFrame to a CSV file\n",
        "all_results_df.to_csv('[EVAL] FinBERT_all_company_results.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c9J2rnclh89"
      },
      "source": [
        "## Accuracy all years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pCQEhjKlh89"
      },
      "outputs": [],
      "source": [
        "# Combine insample and outsample data for rolling window\n",
        "df = pd.concat([insample_df, outsample_df])\n",
        "\n",
        "# Convert 'Date From' to datetime\n",
        "df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "\n",
        "# Sort by date\n",
        "df = df.sort_values(by='Date From')\n",
        "\n",
        "# Check available years\n",
        "available_years = df['Date From'].dt.year.unique()\n",
        "print(\"Years available in the data:\", available_years)\n",
        "\n",
        "# Define rolling window parameters\n",
        "window_size = 365 * 10  # 10 years in days\n",
        "prediction_period = 365  # 1 year in days\n",
        "\n",
        "# Get unique companies\n",
        "companies = df['companyname'].unique()\n",
        "\n",
        "# Dictionary to store DataFrames for each company\n",
        "company_results_dfs = {}\n",
        "\n",
        "# Rolling window analysis for each company\n",
        "for company in companies:\n",
        "    company_df = df[df['companyname'] == company].copy()\n",
        "\n",
        "    accuracies = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "    years = []\n",
        "\n",
        "    # Setup time series cross-validation\n",
        "    tscv = TimeSeriesSplit(n_splits=5)  # You can adjust the number of splits as needed\n",
        "\n",
        "    for train_index, test_index in tscv.split(company_df):\n",
        "        train_df, test_df = company_df.iloc[train_index], company_df.iloc[test_index]\n",
        "\n",
        "        if len(test_df) == 0 or len(train_df) == 0:\n",
        "            continue\n",
        "\n",
        "        X_train = np.vstack(train_df['embedding'].values)\n",
        "        y_train = train_df['Future Return Direction'].values\n",
        "\n",
        "        X_test = np.vstack(test_df['embedding'].values)\n",
        "        y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "        # Train logistic regression model\n",
        "        logistic_model = LogisticRegression(max_iter=1000)\n",
        "        logistic_model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = logistic_model.predict(X_test)\n",
        "\n",
        "        # Ensure the length of predictions and test_df matches\n",
        "        if len(y_pred) != len(test_df):\n",
        "            raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_pred)})\")\n",
        "\n",
        "        # Store the predictions and true values\n",
        "        test_df['predicted'] = y_pred\n",
        "\n",
        "        # Update the company-specific DataFrame with predictions\n",
        "        company_df.loc[test_df.index, 'predicted'] = test_df['predicted']\n",
        "\n",
        "    # Evaluate the Model for the company\n",
        "    for year in range(2016, 2024):\n",
        "        year_df = company_df[company_df['Date From'].dt.year == year]\n",
        "\n",
        "        if 'predicted' not in year_df.columns or year_df['predicted'].isnull().all():\n",
        "            continue\n",
        "\n",
        "        y_test = year_df['Future Return Direction'].values\n",
        "        y_pred = year_df['predicted'].values\n",
        "\n",
        "        # Remove NaN values in predictions\n",
        "        valid_indices = ~np.isnan(y_pred)\n",
        "        y_test = y_test[valid_indices]\n",
        "        y_pred = y_pred[valid_indices]\n",
        "\n",
        "        if len(y_pred) == 0 or len(y_test) == 0:\n",
        "            continue\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "        recall = recall_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "        f1 = f1_score(y_test, y_pred, average='binary', pos_label=1)\n",
        "\n",
        "        accuracies.append(accuracy)\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "        years.append(year)\n",
        "\n",
        "    # Store the results for the company in a DataFrame\n",
        "    results_df = pd.DataFrame({\n",
        "        'Year': years,\n",
        "        'Accuracy': accuracies,\n",
        "        'Precision': precisions,\n",
        "        'Recall': recalls,\n",
        "        'F1 Score': f1_scores\n",
        "    })\n",
        "\n",
        "    # Save the DataFrame in the dictionary\n",
        "    company_results_dfs[company] = results_df\n",
        "\n",
        "# Combine results of all companies into a single DataFrame\n",
        "combined_results = pd.concat(company_results_dfs.values(), ignore_index=True)\n",
        "\n",
        "# Calculate average accuracy per year across all companies\n",
        "average_metrics_per_year = combined_results.groupby('Year').mean().reset_index()\n",
        "\n",
        "# Print average metrics for each year\n",
        "print(\"Average Metrics Per Year:\")\n",
        "print(average_metrics_per_year)\n",
        "\n",
        "# Plot average accuracy per year\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "plt.plot(average_metrics_per_year['Year'], average_metrics_per_year['Accuracy'], marker='o', label='Accuracy')\n",
        "plt.plot(average_metrics_per_year['Year'], average_metrics_per_year['Precision'], marker='o', label='Precision')\n",
        "plt.plot(average_metrics_per_year['Year'], average_metrics_per_year['Recall'], marker='o', label='Recall')\n",
        "plt.plot(average_metrics_per_year['Year'], average_metrics_per_year['F1 Score'], marker='o', label='F1 Score')\n",
        "\n",
        "plt.title('Average Performance Metrics Over Time Across All Companies')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcY_eePvlh89"
      },
      "source": [
        "## Standard Deviation all years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ukz0fYjlh89"
      },
      "outputs": [],
      "source": [
        "# Combine insample and outsample data for rolling window\n",
        "df = pd.concat([insample_df, outsample_df])\n",
        "\n",
        "# Convert 'Date From' to datetime\n",
        "df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "\n",
        "# Sort by date\n",
        "df = df.sort_values(by='Date From')\n",
        "\n",
        "# Check available years\n",
        "available_years = df['Date From'].dt.year.unique()\n",
        "print(\"Years available in the data:\", available_years)\n",
        "\n",
        "# Get unique companies\n",
        "companies = df['companyname'].unique()\n",
        "\n",
        "# Dictionary to store DataFrames for each company\n",
        "company_results_dfs = {}\n",
        "\n",
        "# Rolling window analysis for each company\n",
        "for company in companies:\n",
        "    company_df = df[df['companyname'] == company].copy()\n",
        "\n",
        "    accuracies = []\n",
        "    years = []\n",
        "\n",
        "    # Setup time series cross-validation\n",
        "    tscv = TimeSeriesSplit(n_splits=5)  # You can adjust the number of splits as needed\n",
        "\n",
        "    for train_index, test_index in tscv.split(company_df):\n",
        "        train_df, test_df = company_df.iloc[train_index], company_df.iloc[test_index]\n",
        "\n",
        "        if len(test_df) == 0 or len(train_df) == 0:\n",
        "            continue\n",
        "\n",
        "        X_train = np.vstack(train_df['embedding'].values)\n",
        "        y_train = train_df['Future Return Direction'].values\n",
        "\n",
        "        X_test = np.vstack(test_df['embedding'].values)\n",
        "        y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "        # Train logistic regression model\n",
        "        logistic_model = LogisticRegression(max_iter=1000)\n",
        "        logistic_model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = logistic_model.predict(X_test)\n",
        "\n",
        "        # Ensure the length of predictions and test_df matches\n",
        "        if len(y_pred) != len(test_df):\n",
        "            raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_pred)})\")\n",
        "\n",
        "        # Store the predictions and true values\n",
        "        test_df['predicted'] = y_pred\n",
        "\n",
        "        # Update the company-specific DataFrame with predictions\n",
        "        company_df.loc[test_df.index, 'predicted'] = test_df['predicted']\n",
        "\n",
        "    # Evaluate the Model for the company\n",
        "    for year in range(2016, 2024):\n",
        "        year_df = company_df[company_df['Date From'].dt.year == year]\n",
        "\n",
        "        if 'predicted' not in year_df.columns or year_df['predicted'].isnull().all():\n",
        "            continue\n",
        "\n",
        "        y_test = year_df['Future Return Direction'].values\n",
        "        y_pred = year_df['predicted'].values\n",
        "\n",
        "        # Remove NaN values in predictions\n",
        "        valid_indices = ~np.isnan(y_pred)\n",
        "        y_test = y_test[valid_indices]\n",
        "        y_pred = y_pred[valid_indices]\n",
        "\n",
        "        if len(y_pred) == 0 or len(y_test) == 0:\n",
        "            continue\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        accuracies.append(accuracy)\n",
        "        years.append(year)\n",
        "\n",
        "    # Store the results for the company in a DataFrame\n",
        "    results_df = pd.DataFrame({\n",
        "        'Year': years,\n",
        "        'Accuracy': accuracies\n",
        "    })\n",
        "\n",
        "    # Save the DataFrame in the dictionary\n",
        "    company_results_dfs[company] = results_df\n",
        "\n",
        "# Combine results of all companies into a single DataFrame\n",
        "combined_results = pd.concat(company_results_dfs.values(), ignore_index=True)\n",
        "\n",
        "# Calculate average accuracy per year across all companies\n",
        "average_metrics_per_year = combined_results.groupby('Year').agg(\n",
        "    Accuracy_mean=('Accuracy', 'mean'),\n",
        "    Accuracy_std=('Accuracy', 'std')\n",
        ").reset_index()\n",
        "\n",
        "# Print average metrics for each year\n",
        "print(\"Average Metrics Per Year:\")\n",
        "print(average_metrics_per_year)\n",
        "\n",
        "# Get a color sequence from Plotly's default colors\n",
        "colors = px.colors.qualitative.Plotly\n",
        "\n",
        "# Function to make the color more transparent\n",
        "def get_transparent_color(color, alpha=0.2):\n",
        "    # Convert hex to RGB and then to RGBA\n",
        "    hex_color = color.lstrip('#')\n",
        "    rgb_color = tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n",
        "    return f'rgba({rgb_color[0]}, {rgb_color[1]}, {rgb_color[2]}, {alpha})'\n",
        "\n",
        "# Create a figure\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add the mean line\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=average_metrics_per_year['Year'],\n",
        "    y=average_metrics_per_year['Accuracy_mean'],\n",
        "    mode='lines',\n",
        "    name='Accuracy',\n",
        "    line=dict(color=colors[0], width=2)\n",
        "))\n",
        "\n",
        "# Add the standard deviation shaded area\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=pd.concat([average_metrics_per_year['Year'], average_metrics_per_year['Year'][::-1]]),\n",
        "    y=pd.concat([average_metrics_per_year['Accuracy_mean'] + average_metrics_per_year['Accuracy_std'],\n",
        "                 (average_metrics_per_year['Accuracy_mean'] - average_metrics_per_year['Accuracy_std'])[::-1]]),\n",
        "    fill='toself',\n",
        "    fillcolor=get_transparent_color(colors[0], alpha=0.2),  # Use the same color with transparency\n",
        "    line=dict(color='rgba(255,255,255,0)'),\n",
        "    hoverinfo=\"skip\",\n",
        "    showlegend=False,\n",
        "    name='Accuracy std dev'\n",
        "))\n",
        "\n",
        "# Customize layout\n",
        "fig.update_layout(\n",
        "    title='Average Rolling Window Accuracy Over Time Across All Companies',\n",
        "    xaxis_title='Year',\n",
        "    yaxis_title='Accuracy',\n",
        "    template='plotly_white',\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egkCoWgUlh89"
      },
      "source": [
        "## Portfolio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JA53-pEHlh89"
      },
      "outputs": [],
      "source": [
        "def prepare_data(insample_df, outsample_df):\n",
        "    df = pd.concat([insample_df, outsample_df])\n",
        "    df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "    df = df.sort_values(by='Date From')\n",
        "    available_years = df['Date From'].dt.year.unique()\n",
        "    print(\"Years available in the data:\", available_years)\n",
        "    return df\n",
        "\n",
        "def rolling_window_analysis(df):\n",
        "    companies = df['companyname'].unique()\n",
        "    predictions_df = pd.DataFrame()\n",
        "\n",
        "    for company in companies:\n",
        "        company_df = df[df['companyname'] == company].copy()\n",
        "        company_df = company_df.sort_values(by='Date From')\n",
        "\n",
        "        # Define the rolling window parameters\n",
        "        start_year = company_df['Date From'].dt.year.min()\n",
        "        end_year = company_df['Date From'].dt.year.max()\n",
        "        window_size = 10\n",
        "        validation_size = 1\n",
        "\n",
        "        for start in range(start_year, end_year - window_size - validation_size + 1):\n",
        "            train_start = start\n",
        "            train_end = start + window_size\n",
        "            val_start = train_end\n",
        "            val_end = train_end + validation_size\n",
        "\n",
        "            train_df = company_df[(company_df['Date From'].dt.year >= train_start) &\n",
        "                                  (company_df['Date From'].dt.year < train_end)]\n",
        "            test_df = company_df[(company_df['Date From'].dt.year >= val_start) &\n",
        "                                 (company_df['Date From'].dt.year < val_end)]\n",
        "\n",
        "            if len(test_df) == 0 or len(train_df) == 0:\n",
        "                continue\n",
        "\n",
        "            X_train = np.vstack(train_df['embedding'].values)\n",
        "            y_train = train_df['Future Return Direction'].values\n",
        "            X_test = np.vstack(test_df['embedding'].values)\n",
        "            y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "            # Train logistic regression model\n",
        "            logistic_model = LogisticRegression(max_iter=1000)\n",
        "            logistic_model.fit(X_train, y_train)\n",
        "\n",
        "            # Get prediction probabilities\n",
        "            y_prob = logistic_model.predict_proba(X_test)[:, 1]  # Probability of the positive class\n",
        "\n",
        "            if len(y_prob) != len(test_df):\n",
        "                raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_prob)})\")\n",
        "\n",
        "            test_df['predicted_prob'] = y_prob\n",
        "            predictions_df = pd.concat([predictions_df, test_df[['Date From', 'companyname', 'predicted_prob', 'Weekly Compound Return']]], ignore_index=True)\n",
        "\n",
        "    df = df.merge(predictions_df, on=['Date From', 'companyname', 'Weekly Compound Return'], how='left', suffixes=('', '_pred'))\n",
        "    return df\n",
        "\n",
        "def construct_portfolio(df, time_period='Week'):\n",
        "    df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "    if time_period == 'Week':\n",
        "        df['Period'] = df['Date From'].dt.to_period('W').dt.to_timestamp()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid time_period. Use 'Week'.\")\n",
        "\n",
        "    portfolio_returns = []\n",
        "\n",
        "    for period, group in df.groupby('Period'):\n",
        "        if period.year < 2016:\n",
        "            continue\n",
        "\n",
        "        # Sort group by predicted_prob descending\n",
        "        group_sorted = group.sort_values(by='predicted_prob', ascending=False)\n",
        "\n",
        "        # Select top and bottom companies\n",
        "        num_top_companies = 5\n",
        "        num_bottom_companies = 5\n",
        "        top_companies = group_sorted.head(num_top_companies)\n",
        "        bottom_companies = group_sorted.tail(num_bottom_companies)\n",
        "\n",
        "        # Equal-weighted returns\n",
        "        long_return_eq = np.mean(np.log1p(top_companies['Weekly Compound Return']))\n",
        "        short_return_eq = np.mean(np.log1p(bottom_companies['Weekly Compound Return']))\n",
        "        long_short_return_eq = long_return_eq - short_return_eq\n",
        "\n",
        "        # Value-weighted returns\n",
        "        long_return_val = np.sum(np.log1p(top_companies['Weekly Compound Return']) * top_companies['market_cap']) / np.sum(top_companies['market_cap'])\n",
        "        short_return_val = np.sum(np.log1p(bottom_companies['Weekly Compound Return']) * bottom_companies['market_cap']) / np.sum(bottom_companies['market_cap'])\n",
        "        long_short_return_val = long_return_val - short_return_val\n",
        "\n",
        "        portfolio_returns.append({\n",
        "            'Period': period,\n",
        "            'Long Return (Eq)': long_return_eq,\n",
        "            'Short Return (Eq)': short_return_eq,\n",
        "            'Long-Short Return (Eq)': long_short_return_eq,\n",
        "            'Long Return (Val)': long_return_val,\n",
        "            'Short Return (Val)': short_return_val,\n",
        "            'Long-Short Return (Val)': long_short_return_val\n",
        "        })\n",
        "\n",
        "    portfolio_df = pd.DataFrame(portfolio_returns)\n",
        "    portfolio_df['EW L'] = portfolio_df['Long Return (Eq)'].cumsum()\n",
        "    portfolio_df['EW S'] = portfolio_df['Short Return (Eq)'].cumsum()\n",
        "    portfolio_df['EW LS'] = portfolio_df['Long-Short Return (Eq)'].cumsum()\n",
        "    portfolio_df['VW L'] = portfolio_df['Long Return (Val)'].cumsum()\n",
        "    portfolio_df['VW S'] = portfolio_df['Short Return (Val)'].cumsum()\n",
        "    portfolio_df['VW LS'] = portfolio_df['Long-Short Return (Val)'].cumsum()\n",
        "\n",
        "    actual_returns = df[df['Date From'].dt.year >= 2016].groupby('Period')['Weekly Compound Return'].mean()\n",
        "    actual_cumulative_returns = np.log1p(actual_returns).cumsum()\n",
        "    portfolio_df = portfolio_df.merge(actual_cumulative_returns.rename('Market'), on='Period', how='left')\n",
        "\n",
        "    metrics = {}\n",
        "    for portfolio in ['EW L', 'EW S', 'EW LS', 'VW L', 'VW S', 'VW LS']:\n",
        "        returns = portfolio_df[portfolio]\n",
        "\n",
        "        if returns.isnull().all() or returns.eq(0).all():\n",
        "            sharpe_ratio = np.nan\n",
        "            max_drawdown = np.nan\n",
        "            volatility = np.nan\n",
        "        else:\n",
        "            sharpe_ratio = returns.mean() / returns.std() * np.sqrt(52) if returns.std() != 0 else np.nan\n",
        "            cumulative_returns = returns.cumsum()\n",
        "            max_drawdown = (cumulative_returns.cummax() - cumulative_returns).max()\n",
        "            volatility = returns.std() * np.sqrt(52)\n",
        "\n",
        "        metrics[portfolio] = {\n",
        "            'Sharpe Ratio': sharpe_ratio\n",
        "        }\n",
        "\n",
        "        print(f\"Metrics for {portfolio}:\")\n",
        "        print(f\"Sharpe Ratio: {sharpe_ratio}\")\n",
        "        print()\n",
        "\n",
        "    portfolio_df.to_csv('FinBERT_portfolio_returns.csv', index=False)\n",
        "    print(\"Portfolio returns saved to 'FinBERT_portfolio_returns.csv'\")\n",
        "    return portfolio_df\n",
        "\n",
        "def plot_portfolio_returns(portfolio_df, title_suffix=''):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['EW L'], marker='o', markersize=1, label='EW L')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['EW S'], marker='o', markersize=1, label='EW S')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['EW LS'], marker='o', markersize=1, label='EW LS')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['VW L'], marker='o', markersize=1, label='VW L')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['VW S'], marker='o', markersize=1, label='VW S')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['VW LS'], marker='o', markersize=1, label='VW LS')\n",
        "    plt.plot(portfolio_df['Period'], portfolio_df['Market'], marker='o', markersize=1, label='Market')\n",
        "\n",
        "    plt.title(f'Cumulative {title_suffix} Portfolio Returns Over Time')\n",
        "    plt.xlabel('Period')\n",
        "    plt.ylabel('Cumulative Log Return')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage for Weekly\n",
        "df = prepare_data(insample_df, outsample_df)\n",
        "df = rolling_window_analysis(df)\n",
        "\n",
        "# Weekly Portfolio\n",
        "portfolio_df_week = construct_portfolio(df, time_period='Week')\n",
        "portfolio_df_week = portfolio_df_week[portfolio_df_week['Period'].dt.year >= 2016]\n",
        "plot_portfolio_returns(portfolio_df_week, title_suffix='Weekly')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLHHD7zFlh8-"
      },
      "source": [
        "## Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B__4BYiNlh8-"
      },
      "outputs": [],
      "source": [
        "# Suppress SettingWithCopyWarning\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "def prepare_data(insample_df, outsample_df):\n",
        "    df = pd.concat([insample_df, outsample_df])\n",
        "    df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "    df = df.sort_values(by='Date From')\n",
        "    available_years = df['Date From'].dt.year.unique()\n",
        "    print(\"Years available in the data:\", available_years)\n",
        "    return df\n",
        "\n",
        "def rolling_window_analysis(df):\n",
        "    companies = df['companyname'].unique()\n",
        "    predictions_df = pd.DataFrame()\n",
        "\n",
        "    for company in companies:\n",
        "        company_df = df[df['companyname'] == company].copy()\n",
        "        company_df = company_df.sort_values(by='Date From')\n",
        "\n",
        "        start_year = company_df['Date From'].dt.year.min()\n",
        "        end_year = company_df['Date From'].dt.year.max()\n",
        "        window_size = 10\n",
        "        validation_size = 1\n",
        "\n",
        "        for start in range(start_year, end_year - window_size - validation_size + 1):\n",
        "            train_start = start\n",
        "            train_end = start + window_size\n",
        "            val_start = train_end\n",
        "            val_end = train_end + validation_size\n",
        "\n",
        "            train_df = company_df[(company_df['Date From'].dt.year >= train_start) &\n",
        "                                  (company_df['Date From'].dt.year < train_end)]\n",
        "            test_df = company_df[(company_df['Date From'].dt.year >= val_start) &\n",
        "                                 (company_df['Date From'].dt.year < val_end)]\n",
        "\n",
        "            if len(test_df) == 0 or len(train_df) == 0:\n",
        "                continue\n",
        "\n",
        "            X_train = np.vstack(train_df['embedding'].values)\n",
        "            y_train = train_df['Future Return Direction'].values\n",
        "            X_test = np.vstack(test_df['embedding'].values)\n",
        "            y_test = test_df['Future Return Direction'].values\n",
        "\n",
        "            logistic_model = LogisticRegression(max_iter=1000)\n",
        "            logistic_model.fit(X_train, y_train)\n",
        "\n",
        "            y_prob = logistic_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "            if len(y_prob) != len(test_df):\n",
        "                raise ValueError(f\"Mismatch between test data length ({len(test_df)}) and predictions ({len(y_prob)})\")\n",
        "\n",
        "            test_df['predicted_prob'] = y_prob\n",
        "            predictions_df = pd.concat([predictions_df, test_df[['Date From', 'companyname', 'predicted_prob', 'Weekly Compound Return', 'market_cap']]], ignore_index=True)\n",
        "\n",
        "    df = df.merge(predictions_df, on=['Date From', 'companyname', 'Weekly Compound Return', 'market_cap'], how='left', suffixes=('', '_pred'))\n",
        "    return df\n",
        "\n",
        "def calculate_transaction_costs(df):\n",
        "    np.random.seed(None)  # Ensure we're not using a fixed seed\n",
        "\n",
        "    median_market_cap = df['market_cap'].median()\n",
        "    df['is_large_cap'] = df['market_cap'] > median_market_cap\n",
        "\n",
        "    # Convert transaction costs to basis points\n",
        "    df['transaction_cost'] = np.where(df['is_large_cap'],\n",
        "                                      np.random.normal(10.25, 2.05, df.shape[0]),\n",
        "                                      np.random.normal(21, 4.2, df.shape[0]))\n",
        "\n",
        "    # Ensure no negative transaction costs\n",
        "    df['transaction_cost'] = np.maximum(df['transaction_cost'], 0)\n",
        "\n",
        "    print(f\"Average cost for large-cap stocks: {df[df['is_large_cap']]['transaction_cost'].mean():.2f} bps\")\n",
        "    print(f\"Average cost for small-cap stocks: {df[~df['is_large_cap']]['transaction_cost'].mean():.2f} bps\")\n",
        "\n",
        "    print(f\"\\nLarge-cap cost range: {df[df['is_large_cap']]['transaction_cost'].min():.2f} bps to {df[df['is_large_cap']]['transaction_cost'].max():.2f} bps\")\n",
        "    print(f\"Small-cap cost range: {df[~df['is_large_cap']]['transaction_cost'].min():.2f} bps to {df[~df['is_large_cap']]['transaction_cost'].max():.2f} bps\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def construct_portfolio_with_costs(df, time_period='Week'):\n",
        "    df['Date From'] = pd.to_datetime(df['Date From'])\n",
        "    if time_period == 'Week':\n",
        "        df['Period'] = df['Date From'].dt.to_period('W').dt.to_timestamp()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid time_period. Use 'Week'.\")\n",
        "\n",
        "    portfolio_returns = []\n",
        "\n",
        "    for period, group in df.groupby('Period'):\n",
        "        if period.year < 2016:\n",
        "            continue\n",
        "\n",
        "        group_sorted = group.sort_values(by='predicted_prob', ascending=False)\n",
        "\n",
        "        num_top_companies = 5\n",
        "        num_bottom_companies = 5\n",
        "        top_companies = group_sorted.head(num_top_companies)\n",
        "        bottom_companies = group_sorted.tail(num_bottom_companies)\n",
        "\n",
        "        def calculate_return_with_costs(companies, long_position=True):\n",
        "            returns = np.log1p(companies['Weekly Compound Return'])\n",
        "            costs = companies['transaction_cost'] / 10000  # Convert bps to decimal\n",
        "            if long_position:\n",
        "                return returns - costs\n",
        "            else:\n",
        "                return -returns - costs\n",
        "\n",
        "        long_return_eq = np.mean(calculate_return_with_costs(top_companies, long_position=True))\n",
        "        short_return_eq = np.mean(calculate_return_with_costs(bottom_companies, long_position=False))\n",
        "        long_short_return_eq = long_return_eq - short_return_eq\n",
        "\n",
        "        total_market_cap_long = np.sum(top_companies['market_cap'])\n",
        "        total_market_cap_short = np.sum(bottom_companies['market_cap'])\n",
        "\n",
        "        long_return_val = np.sum(calculate_return_with_costs(top_companies, long_position=True) * top_companies['market_cap']) / total_market_cap_long\n",
        "        short_return_val = np.sum(calculate_return_with_costs(bottom_companies, long_position=False) * bottom_companies['market_cap']) / total_market_cap_short\n",
        "        long_short_return_val = long_return_val - short_return_val\n",
        "\n",
        "        portfolio_returns.append({\n",
        "            'Period': period,\n",
        "            'Long Return (Eq)': long_return_eq,\n",
        "            'Short Return (Eq)': short_return_eq,\n",
        "            'Long-Short Return (Eq)': long_short_return_eq,\n",
        "            'Long Return (Val)': long_return_val,\n",
        "            'Short Return (Val)': short_return_val,\n",
        "            'Long-Short Return (Val)': long_short_return_val\n",
        "        })\n",
        "\n",
        "    portfolio_df = pd.DataFrame(portfolio_returns)\n",
        "    portfolio_df['EW L'] = portfolio_df['Long Return (Eq)'].cumsum()\n",
        "    portfolio_df['EW S'] = portfolio_df['Short Return (Eq)'].cumsum()\n",
        "    portfolio_df['EW LS'] = portfolio_df['Long-Short Return (Eq)'].cumsum()\n",
        "    portfolio_df['VW L'] = portfolio_df['Long Return (Val)'].cumsum()\n",
        "    portfolio_df['VW S'] = portfolio_df['Short Return (Val)'].cumsum()\n",
        "    portfolio_df['VW LS'] = portfolio_df['Long-Short Return (Val)'].cumsum()\n",
        "\n",
        "    actual_returns = df[df['Date From'].dt.year >= 2016].groupby('Period')['Weekly Compound Return'].mean()\n",
        "    actual_cumulative_returns = np.log1p(actual_returns).cumsum()\n",
        "    portfolio_df = portfolio_df.merge(actual_cumulative_returns.rename('Market'), on='Period', how='left')\n",
        "\n",
        "    return portfolio_df\n",
        "\n",
        "# Main execution\n",
        "df = prepare_data(insample_df, outsample_df)\n",
        "df = rolling_window_analysis(df)\n",
        "df = calculate_transaction_costs(df)\n",
        "\n",
        "# Weekly Portfolio with transaction costs\n",
        "portfolio_df_week_with_costs = construct_portfolio_with_costs(df, time_period='Week')\n",
        "portfolio_df_week_with_costs = portfolio_df_week_with_costs[portfolio_df_week_with_costs['Period'].dt.year >= 2016]\n",
        "\n",
        "print(\"Portfolio construction with transaction costs completed.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}